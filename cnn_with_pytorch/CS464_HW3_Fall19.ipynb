{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "CS464_HW3_Fall19.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5yghM_BMmkQ",
        "colab_type": "text"
      },
      "source": [
        "<h1><center>CS 464</center></h1>\n",
        "<h1><center>Introduction to Machine Learning</center></h1>\n",
        "<h1><center>Fall 2019</center></h1>\n",
        "<h1><center>Homework 3</center></h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ds6L6MMXMmkR",
        "colab_type": "text"
      },
      "source": [
        "<h3><center>Due: Jan 3, 2020 23:55 (GMT+3)</center></h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCsDGpxqMmkT",
        "colab_type": "text"
      },
      "source": [
        "### Instructions\n",
        "\n",
        "<ul>\n",
        "    <li>\n",
        "    This homework contains both written and programming questions about neural networks. You should implement programming questions on this notebook. Your plots should also be produced in this notebook. Each programming question has its own cell for your answer. You can implement your code directly in these cells, or you can call required functions which are defined in a different location for the given question.\n",
        "    </li>\n",
        "    <li>\n",
        "        For questions that you need to plot, your plot results have to be included in both cell output. For written questions, you may provide them either as comments in code cells or as seperate text cells. \n",
        "    </li>\n",
        "    <li>\n",
        "        It is <b>NOT ALLOWED</b> to use different libraries than given libraries which are defined in the requirements.txt.\n",
        "    </li>\n",
        "    <li>\n",
        "        It is <b>NOT ALLOWED</b> to use a different deep learning framework than PyTorch.\n",
        "    </li>\n",
        "    <li>\n",
        "        While submitting the homework file, please package notebook(\".ipynb\") and model (\".pth\") files as a gzipped TAR file or a ZIP file with the name CS464_HW3_Section#_Firstname_Lastname. Please do not use any Turkish letters for any of your files including code files and model files. Upload your homework to Moodle.\n",
        "    </li>\n",
        "    <li>\n",
        "        This is an individual assignment for each student. That is, you are NOT allowed to share your workwith your classmates.</li>\n",
        "    <li> \n",
        "    If you do not follow the submission routes, deadlines and specifications, it will lead to a significant grade deduction.\n",
        "    </li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cE7TyUOpMmkU",
        "colab_type": "text"
      },
      "source": [
        "## Environment Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDBMnGMFlc50",
        "colab_type": "text"
      },
      "source": [
        "You may use both anaconda or pip to install PyTorch to your own computer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wbzHKKbMmkV",
        "colab_type": "text"
      },
      "source": [
        "### Anaconda Installation\n",
        "\n",
        "<ul>\n",
        "    <li>Download anaconda from https://www.anaconda.com/download</li>\n",
        "    <li>Follow the instructions provided in https://conda.io/docs/user-guide/install/index.html#regular-installation</li>\n",
        "</ul>\n",
        "\n",
        "#### Creation of Virtual Environment\n",
        "\n",
        "<ul>\n",
        "    <li>Create python3.7 virtual environment for your hw3 using follow command from the command line<br>\n",
        "        <i>> conda create -n HW3 python=3.7 anaconda</i></li>\n",
        "    <li>Activate your virtual environment<br>\n",
        "        <i>> source activate HW3</i></li>\n",
        "    <li>To install auxiliary libraries install attached \"requirements.txt\" and run following command in activated \"hw3\" environment<br>\n",
        "        <i>> pip install -r requirements.txt<i></li>\n",
        "     <li>When you create your virtual environment with \"anaconda\" metapackage, jupyter notebook should be installed. Try:<br>\n",
        "         <i>> jupyter notebook</i>\n",
        "</ul>\n",
        "\n",
        "\n",
        "#### Pytorch Installation with Anaconda\n",
        "\n",
        "You should install PyTorch to your virtual environment which is created for the hw3. Therefore, you should activate your homework virtual environment before to start PyTorch installation.\n",
        "<li>> source activate HW3</li>\n",
        "\n",
        "After you have activated the virtual environment, then use one of the following commands to install pytorch for CPU for your system. See https://pytorch.org/ for help.\n",
        "<ul>\n",
        "<li>For MacOS:<br>\n",
        "    <i>> conda install pytorch torchvision -c pytorch</i>\n",
        "</li>\n",
        "<li>For Linux:<br>\n",
        "    <i>> conda install pytorch-cpu torchvision-cpu -c pytorch</i>\n",
        "</li>\n",
        "<li>For Windows:<br>\n",
        "    <i>> conda install pytorch-cpu torchvision-cpu -c pytorch</i><br>\n",
        "</li>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oV6Va_SmV5i",
        "colab_type": "text"
      },
      "source": [
        "###Pip3 Installation\n",
        "<ul>\n",
        "    <li>Download pip3 from https://pip.pypa.io/en/stable/installing/</li>\n",
        "    <li>If you are using Windows, you may need to add Python to your enviroment variables. You may use the following tutorial to install Python and pip.\n",
        "    https://phoenixnap.com/kb/how-to-install-python-3-windows</li>\n",
        "</ul>\n",
        "\n",
        "#### PyTorch Installation with Pip\n",
        "<ul>\n",
        "<li>For MacOS:<br>\n",
        "    <i>> pip3 install torch torchvision</i>\n",
        "</li>\n",
        "<li>For Linux:<br>\n",
        "    <i>> pip3 install torch==1.3.1+cpu torchvision==0.4.2+cpu -f https://download.pytorch.org/whl/torch_stable.html</i>\n",
        "</li>\n",
        "<li>For Windows:<br>\n",
        "    <i>> pip3 install torch==1.3.1+cpu torchvision==0.4.2+cpu -f https://download.pytorch.org/whl/torch_stable.html</i><br>\n",
        "</li>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGRjkX0pMmkY",
        "colab_type": "text"
      },
      "source": [
        "## Question 1 [10 pts]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BcwAjaoMmkZ",
        "colab_type": "text"
      },
      "source": [
        "A computational graph is a directed graph where nodes correspond to variables or operations. \n",
        "\n",
        "Consider a neural network architecture consisting of a hidden layer of a single unit with sigmoid activation, 2 input neurons and 1 output neuron that calculates MSE loss. Draw the computational graph of the given architecture assuming there is no bias term at any layer. You may use any drawing tool you prefer.\n",
        "\n",
        "Now, assume that initial weights are $w_h = 1.5$ for the hidden neuron and $w_o=-0.5$ for the output neuron. Perform the forward pass for the input instance whose features are $x_0=2$ and $x_1=-1$ and label is $y=-0.3$. Then, perform backpropagation to find gradient vector of weights $w_h$ and $w_o$. Indicate all steps of backpropagation on your computational graph.\n",
        "\n",
        "You can check [this](https://cdn-images-1.medium.com/freeze/max/1000/1*GEpvvmhoj0yRTi_kpDS6Eg.png?q=20) link to see an example of computational graph.\n",
        "\n",
        "NOTE: For this question, you can provide your answer in this notebook file using the next text cell or as an additional report in the PDF format.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJPUiAYYV_Q_",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "# Write the url of your image to the blank in \"src\" parameter below\n",
        "# and uncomment that section. You may use any image hosting environment\n",
        "# such as imgbb, imaggmi or Google drive with appropriate link sharing. \n",
        "\n",
        "<img src=\" ...  \" width=\"...\"/>\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmN0va8GMmka",
        "colab_type": "text"
      },
      "source": [
        "## Question 2 [90 pts]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3mspgJlMmkb",
        "colab_type": "text"
      },
      "source": [
        "In this question, you will perform multi-class classification on animals data set (Alessio, 2018). Specifically, you will implement a neural network with two hidden layers to distinguish 10 different animals from each other. The dataset has been preprocessed in such a way that each class has 200 samples and each sample is an image of size 100x100x3.\n",
        "\n",
        "Download the dataset from the following link:\n",
        "<br>\n",
        "https://drive.google.com/file/d/1rc6WbpzbLaYahK4AloPmbsEH2u_OsrKC/view\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgzLrMkW8xxU",
        "colab_type": "text"
      },
      "source": [
        "### 2.1. Multi Layer Perceptron (MLP) [20 pts]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yzlbHxVMmkc",
        "colab_type": "text"
      },
      "source": [
        "#### Data Loader [4 pts]\n",
        "\n",
        "An important part of such a task is to implement your own data loader. In this homework, a partial loader is provided to you. This loader is going to be based on a base class named \"Dataset\", provided in PyTorch library. You need to complete the code below to create your custom \"AnimalDataset\" class which will be able to load your dataset. \n",
        "Implement the functions whose proptotypes are given. Follow the TODO notes below. You have to divide the files into three sets as <b>train (70%)</b>, <b>validation (10%)</b> and **test (20%)** sets.  These non-overlapping splits, which are subsets of AnimalDataset, should be retrieved using the \"get_dataset\" function. Here, you are also supposed to flatten the image into a vector (also to grayscale) to be compatible with MLP. Note that the pixel values also needs to be normalized to [0,1] range.\n",
        "<br>\n",
        "\n",
        "Hint: The dataset is not normalized and your results will heavily depend on your input.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fz7C0YpBV-bW",
        "colab_type": "code",
        "outputId": "83057399-fc75-4e58-a997-e2f81dd2aa2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKA_ylTx6SVM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision import transforms\n",
        "from torch import zeros\n",
        "from torch import randperm\n",
        "from torch import Tensor\n",
        "\n",
        "ROOT = \"/content/drive/My Drive/Colab Notebooks/CS464 - HW3/hw3_dataset.zip (Unzipped Files)/dataset\"\n",
        "IMAGE_SIZE = 100\n",
        "IMAGE_COUNT = 2000\n",
        "TRAIN_COUNT = int(IMAGE_COUNT*0.7)\n",
        "VALID_COUNT = int(IMAGE_COUNT*0.1)\n",
        "TEST_COUNT = int(IMAGE_COUNT*0.2)\n",
        "class AnimalDataset(Dataset):\n",
        "    \n",
        "    # TODO:\n",
        "    # Define constructor for AnimalDataset class\n",
        "    # HINT: You can pass processed data samples and their ground truth values as parameters \n",
        "    def __init__(self, **kwargs):\n",
        "      self.data = kwargs[\"data\"]\n",
        "      self.labels = kwargs[\"labels\"]\n",
        "        \n",
        "    '''This function should return sample count in the dataset'''\n",
        "    def __len__(self):\n",
        "       return self.data.shape[0]\n",
        "\n",
        "    '''This function should return a single sample and its ground truth value from the dataset corresponding to index parameter '''\n",
        "    def __getitem__(self, index):\n",
        "      return Tensor(self.data[index]).cuda(), self.labels[index]\n",
        "        #return _x, _y\n",
        "\n",
        "\n",
        "def get_dataset(root):\n",
        "    # TODO: \n",
        "    # Read dataset files\n",
        "    # Construct training, validation and test sets\n",
        "    # Normalize & flatten datasets\n",
        "    transform_img = transforms.Compose([\n",
        "      # transforms.Resize(int(IMAGE_SIZE*(1.25))),\n",
        "      # transforms.CenterCrop(IMAGE_SIZE),\n",
        "      transforms.Grayscale(num_output_channels=1),\n",
        "      transforms.ToTensor()\n",
        "    ])\n",
        "    images_tuple = ImageFolder(root, transform=transform_img)\n",
        "    images = zeros([IMAGE_COUNT, IMAGE_SIZE*IMAGE_SIZE])\n",
        "    labels = zeros([IMAGE_COUNT])\n",
        "\n",
        "    # print(labels.shape)\n",
        "    for i in range(IMAGE_COUNT):\n",
        "      # print(images_tuple[i][0][0][0])\n",
        "      images[i] = images_tuple[i][0][0].view(-1, IMAGE_SIZE*IMAGE_SIZE)\n",
        "      labels[i] = images_tuple[i][1]\n",
        "\n",
        "    rand_indices = randperm(IMAGE_COUNT).cuda()\n",
        "\n",
        "    train_indices = rand_indices[:TRAIN_COUNT]\n",
        "    valid_indices = rand_indices[TRAIN_COUNT:TRAIN_COUNT+VALID_COUNT]\n",
        "    test_indices = rand_indices[TRAIN_COUNT+VALID_COUNT:]\n",
        "\n",
        "    train_images = images[train_indices,:]\n",
        "\n",
        "    valid_images = images[valid_indices,:]\n",
        "    test_images = images[test_indices,:]\n",
        "\n",
        "\n",
        "    train_labels = labels[train_indices]\n",
        "    valid_labels = labels[valid_indices]\n",
        "    test_labels = labels[test_indices]\n",
        "\n",
        "    train_dataset = AnimalDataset(labels=train_labels, data=train_images)\n",
        "    val_dataset = AnimalDataset(labels=valid_labels, data=valid_images)\n",
        "    test_dataset = AnimalDataset(labels=test_labels, data= test_images)\n",
        "\n",
        "\n",
        "    # dataset = []\n",
        "    # dataset.append(train_dataset)\n",
        "    # dataset.append(valid_dataset)\n",
        "    # dataset.append(test_dataset)\n",
        "    # dataset = dataset.ToTensor()\n",
        "    \n",
        "\n",
        "    return train_dataset, val_dataset, test_dataset\n",
        "    \n",
        "    #return train_dataset, val_dataset, test_dataset#\n",
        "# root = \"/content/drive/My Drive/Colab Notebooks/CS464 - HW3/hw3_dataset.zip (Unzipped Files)/dataset\"\n",
        "# train_dataset, val_dataset, test_dataset = get_dataset(ROOT)\n",
        "# print(len(train_dataset))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMfyonGRMmke",
        "colab_type": "text"
      },
      "source": [
        "#### Neural Network [4 pts]\n",
        "\n",
        "Now, implement your two hidden layer neural network. FNet class will represent your neural network. First hidden layer will contain 1000 neurons and second hidden layer will contain 500 neurons. You will decide the number of input and output neurons. Use ReLU as your hidden layer activation function. You need to pick a proper activation function for the output layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adM6pBIp6cIa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch import nn\n",
        "\n",
        "class FNet(nn.Module):\n",
        "    '''Define your neural network'''\n",
        "    def __init__(self, **kwargs): \n",
        "    # you can add any additional parameters you want \n",
        "    # TODO:\n",
        "    # You should create your neural network here\n",
        "      super(FNet,self).__init__()\n",
        "      self.linear1 = nn.Linear(kwargs[\"D_in\"], kwargs[\"H1\"]).cuda()\n",
        "      nn.ReLU().cuda()\n",
        "      self.linear2 = nn.Linear(kwargs[\"H1\"], kwargs[\"H2\"]).cuda()\n",
        "      nn.ReLU().cuda()\n",
        "      self.linear3 = nn.Linear(kwargs[\"H2\"], kwargs[\"D_out\"]).cuda()\n",
        "     \n",
        "    def forward(self, X):\n",
        "      H1_relu = self.linear1(X).clamp(min=0).cuda()\n",
        "      H2_relu = self.linear2(H1_relu).clamp(min=0).cuda()\n",
        "      y_pred = self.linear3(H2_relu).cuda()\n",
        "      return y_pred \n",
        "    # you can add any additional parameters you want\n",
        "    # TODO:\n",
        "    # Forward propagation implementation should be here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4-GhJe0Mmkf",
        "colab_type": "text"
      },
      "source": [
        "#### Training [7 pts]\n",
        "\n",
        "Complete the code snippet below to train your network. You need to carefully select the appropriate loss function and tune hyper-parameters. Use SGD optimizer for this question. So far, you should have created three dataset splits for train, validation and test. You will need to load these splits at this phase. Make sure that you shuffle the samples in the training split. Save training loss and training accuracy of each iteration (each batch) and also save validation loss and accuracy at each epoch to use them in the next part for plotting. You can use matplotlib library for plotting. Your model is going to run upto the \"max_epoch\" parameter. Pick the best model so far as your final model. You need to save this model as a \".pth\" file. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Hmc6uD6GXBz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "def find_confusion_matrix(preds, values):\n",
        "    _, preds = torch.max(preds.data, 1)\n",
        "    print(preds)\n",
        "    print(values)\n",
        "    confusion_matrix = zeros((10,10)).cuda()\n",
        "    for i in preds:\n",
        "      for j in values:\n",
        "        confusion_matrix[int(i)][int(j)] += 1\n",
        "    return confusion_matrix\n",
        "\n",
        "def metrics_help(preds, values):\n",
        "    _, preds = torch.max(preds.data, 1)\n",
        "    TPs = zeros((10)).cuda()\n",
        "    FPs = zeros((10)).cuda()\n",
        "    TNs = zeros((10)).cuda()\n",
        "    FNs = zeros((10)).cuda()\n",
        "\n",
        "    for k in range(10):\n",
        "      for i in preds:\n",
        "        for j in values:\n",
        "          if i == j:\n",
        "            if i == k:\n",
        "              TPs[k] += 1\n",
        "            else:\n",
        "              TNs[k] += 1\n",
        "          else:\n",
        "            if i == k:\n",
        "              FPs[k] += 1\n",
        "            else:\n",
        "              FNs[k] += 1\n",
        "    return TPs, FPs, TNs, FNs\n",
        "\n",
        "def find_precisions(preds, values):\n",
        "    TPs, FPs, TNs, FNs = metrics_help(preds, values)\n",
        "    return TPs / (TPs + FPs)\n",
        "\n",
        "def find_recalls(preds, values):\n",
        "    TPs, FPs, TNs, FNs = metrics_help(preds, values)\n",
        "    return TPs / (TPs + FNs)\n",
        "\n",
        "def find_f1(preds, values):\n",
        "    precisions = find_precisions(preds, values)\n",
        "    recalls = find_recalls(preds, values)\n",
        "\n",
        "    return 2*precisions*recalls / (precisions + recalls)\n",
        "\n",
        "def find_accuracy(outputs, labels):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels.cuda()).sum().item()\n",
        "    return correct / total\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlkS5jVR6kNb",
        "colab_type": "code",
        "outputId": "f0357ceb-bdd1-4b6c-a154-20a67437f307",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import torch\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader\n",
        "#HINT: note that your training time should not take many days.\n",
        "\n",
        "#TODO:\n",
        "#Pick your hyper parameters\n",
        "max_epoch = 20\n",
        "train_batch = 32\n",
        "test_batch = 32\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = get_dataset(ROOT)\n",
        "trainloader = DataLoader(train_dataset, batch_size=train_batch)\n",
        "testloader = DataLoader(test_dataset, batch_size=test_batch)\n",
        "val_loader = DataLoader(val_dataset)\n",
        "\n",
        "learning_rate = 0.001\n",
        "\n",
        "use_gpu = torch.cuda.is_available()\n",
        "print(\"Use GPU:\", use_gpu)\n",
        "\n",
        "best_path = '/content/drive/My Drive/Colab Notebooks/CS464 - HW3/best_model_mlp.pth'\n",
        "\n",
        "train_accuracy_list = []\n",
        "train_loss_list = []\n",
        "val_accuracy_list = []\n",
        "val_loss_list = []\n",
        "\n",
        "def main(): # you are free to change parameters\n",
        "\n",
        "    # Create train dataset loader\n",
        "    # Create validation dataset loader\n",
        "    # Create test dataset loader\n",
        "    # initialize your GENet neural network\n",
        "    # define your loss function\n",
        "    \n",
        "\n",
        "    model = FNet(D_in=(IMAGE_SIZE*IMAGE_SIZE), H1=1000, H2=500, D_out=10).cuda()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-04) # you can play with momentum and weight_decay parameters as well\n",
        "    \n",
        "\n",
        "    \n",
        "    # start training\n",
        "    # for each epoch calculate validation performance\n",
        "    # save best model according to validation performance\n",
        "    best_acc = 0\n",
        "    \n",
        "    for epoch in range(max_epoch):\n",
        "       train(epoch, model, criterion, optimizer, trainloader, train_accuracy_list, train_loss_list)\n",
        "       acc, loss = test(model, criterion, val_loader)\n",
        "       val_accuracy_list.append(acc)\n",
        "       val_loss_list.append(loss)\n",
        "       if acc > best_acc:\n",
        "          torch.save(model, best_path)\n",
        "          best_acc = acc\n",
        "    print(\"Best Acc:\", best_acc)\n",
        "''' Train your network for a one epoch '''\n",
        "def train(epoch, model, criterion, optimizer, loader, train_accuracy_list, train_loss_list): # you are free to change parameters\n",
        "    model.train()\n",
        "    losses = AverageMeter()\n",
        "    accuracies = AverageMeter()\n",
        "\n",
        "    for batch_idx, (data, labels) in enumerate(loader):\n",
        "        # TODO:\n",
        "        # Implement training code for a one iteration\n",
        "        y_pred = model(data.float().cuda())\n",
        "        loss = criterion(y_pred, labels.long().cuda())\n",
        "        losses.update(loss.item(), data.size(0))\n",
        "        accuracy = find_accuracy(y_pred, labels)\n",
        "        # accuracy = pf1(y_pred, labels)\n",
        "        accuracies.update(accuracy)\n",
        "\n",
        "        train_accuracy_list.append(accuracy)\n",
        "        train_loss_list.append(loss)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # print('Epoch: [{0}][{1}/{2}]\\t'\n",
        "        #       'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "        #       'Data {data_time.val:.4f} ({data_time.avg:.4f})\\t'\n",
        "        #       'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "        #       'Accu {acc.val:.4f} ({acc.avg:.4f})\\t'.format(\n",
        "        #        epoch + 1, batch_idx + 1, len(trainloader), \n",
        "        #        batch_time=batch_time,\n",
        "        #        data_time=data_time, \n",
        "        #        loss=losses,\n",
        "        #        acc=accuracies))\n",
        "        print('Epoch: [{0}][{1}/{2}]\\t'\n",
        "              'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "              'Accu {acc.val:.4f} ({acc.avg:.4f})\\t'.format(\n",
        "               epoch + 1, batch_idx + 1, len(loader), \n",
        "               loss=losses,\n",
        "               acc=accuracies))\n",
        "\n",
        "\n",
        "''' Test&Validate your network '''\n",
        "def test(model, criterion, loader): # you are free to change parameters\n",
        "\n",
        "    model.eval()\n",
        "    losses = AverageMeter()\n",
        "    accuracies = AverageMeter()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, labels) in enumerate(loader):\n",
        "            # TODO:\n",
        "            # Implement test code\n",
        "            # y_pred = model(data.long().conda())\n",
        "            y_pred = model(data.float().cuda())\n",
        "            loss = criterion(y_pred, labels.long().cuda())\n",
        "            losses.update(loss.item(), data.size(0))\n",
        "            accuracy = find_accuracy(y_pred, labels)\n",
        "            # accuracy = pf1(y_pred, labels)\n",
        "            accuracies.update(accuracy)\n",
        "        # print('Time {batch_time.avg:.3f}\\t'\n",
        "        #       'Accu {acc.avg:.4f}\\t'.format(\n",
        "        #        batch_time=batch_time, \n",
        "        #        acc=accuracies))\n",
        "        print('Accu {acc.avg:.4f}\\t'.format(\n",
        "               acc=accuracies))\n",
        "    return accuracies.avg, losses.avg\n",
        "\n",
        "main()"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Use GPU: True\n",
            "Epoch: [1][1/44]\tLoss 2.3043 (2.3043)\tAccu 0.0625 (0.0625)\t\n",
            "Epoch: [1][2/44]\tLoss 2.2984 (2.3014)\tAccu 0.0625 (0.0625)\t\n",
            "Epoch: [1][3/44]\tLoss 2.3095 (2.3041)\tAccu 0.1562 (0.0938)\t\n",
            "Epoch: [1][4/44]\tLoss 2.3122 (2.3061)\tAccu 0.1250 (0.1016)\t\n",
            "Epoch: [1][5/44]\tLoss 2.3084 (2.3066)\tAccu 0.0312 (0.0875)\t\n",
            "Epoch: [1][6/44]\tLoss 2.3111 (2.3073)\tAccu 0.0625 (0.0833)\t\n",
            "Epoch: [1][7/44]\tLoss 2.2953 (2.3056)\tAccu 0.1875 (0.0982)\t\n",
            "Epoch: [1][8/44]\tLoss 2.3106 (2.3062)\tAccu 0.0312 (0.0898)\t\n",
            "Epoch: [1][9/44]\tLoss 2.3177 (2.3075)\tAccu 0.1562 (0.0972)\t\n",
            "Epoch: [1][10/44]\tLoss 2.3040 (2.3072)\tAccu 0.0938 (0.0969)\t\n",
            "Epoch: [1][11/44]\tLoss 2.3061 (2.3071)\tAccu 0.0000 (0.0881)\t\n",
            "Epoch: [1][12/44]\tLoss 2.3080 (2.3071)\tAccu 0.0625 (0.0859)\t\n",
            "Epoch: [1][13/44]\tLoss 2.2899 (2.3058)\tAccu 0.2500 (0.0986)\t\n",
            "Epoch: [1][14/44]\tLoss 2.2882 (2.3046)\tAccu 0.2188 (0.1071)\t\n",
            "Epoch: [1][15/44]\tLoss 2.3029 (2.3044)\tAccu 0.1250 (0.1083)\t\n",
            "Epoch: [1][16/44]\tLoss 2.3002 (2.3042)\tAccu 0.0938 (0.1074)\t\n",
            "Epoch: [1][17/44]\tLoss 2.3254 (2.3054)\tAccu 0.0000 (0.1011)\t\n",
            "Epoch: [1][18/44]\tLoss 2.2946 (2.3048)\tAccu 0.1250 (0.1024)\t\n",
            "Epoch: [1][19/44]\tLoss 2.2871 (2.3039)\tAccu 0.1562 (0.1053)\t\n",
            "Epoch: [1][20/44]\tLoss 2.3071 (2.3040)\tAccu 0.0312 (0.1016)\t\n",
            "Epoch: [1][21/44]\tLoss 2.3145 (2.3045)\tAccu 0.0312 (0.0982)\t\n",
            "Epoch: [1][22/44]\tLoss 2.2879 (2.3038)\tAccu 0.1250 (0.0994)\t\n",
            "Epoch: [1][23/44]\tLoss 2.3131 (2.3042)\tAccu 0.0938 (0.0992)\t\n",
            "Epoch: [1][24/44]\tLoss 2.2946 (2.3038)\tAccu 0.0938 (0.0990)\t\n",
            "Epoch: [1][25/44]\tLoss 2.3323 (2.3049)\tAccu 0.0000 (0.0950)\t\n",
            "Epoch: [1][26/44]\tLoss 2.3067 (2.3050)\tAccu 0.0625 (0.0938)\t\n",
            "Epoch: [1][27/44]\tLoss 2.2956 (2.3047)\tAccu 0.1875 (0.0972)\t\n",
            "Epoch: [1][28/44]\tLoss 2.3059 (2.3047)\tAccu 0.0625 (0.0960)\t\n",
            "Epoch: [1][29/44]\tLoss 2.3035 (2.3047)\tAccu 0.0938 (0.0959)\t\n",
            "Epoch: [1][30/44]\tLoss 2.2974 (2.3044)\tAccu 0.1562 (0.0979)\t\n",
            "Epoch: [1][31/44]\tLoss 2.3022 (2.3043)\tAccu 0.0312 (0.0958)\t\n",
            "Epoch: [1][32/44]\tLoss 2.2902 (2.3039)\tAccu 0.1875 (0.0986)\t\n",
            "Epoch: [1][33/44]\tLoss 2.2849 (2.3033)\tAccu 0.1562 (0.1004)\t\n",
            "Epoch: [1][34/44]\tLoss 2.2987 (2.3032)\tAccu 0.0938 (0.1002)\t\n",
            "Epoch: [1][35/44]\tLoss 2.3118 (2.3034)\tAccu 0.0312 (0.0982)\t\n",
            "Epoch: [1][36/44]\tLoss 2.3024 (2.3034)\tAccu 0.0625 (0.0972)\t\n",
            "Epoch: [1][37/44]\tLoss 2.3069 (2.3035)\tAccu 0.0312 (0.0954)\t\n",
            "Epoch: [1][38/44]\tLoss 2.2851 (2.3030)\tAccu 0.1250 (0.0962)\t\n",
            "Epoch: [1][39/44]\tLoss 2.3234 (2.3035)\tAccu 0.0625 (0.0954)\t\n",
            "Epoch: [1][40/44]\tLoss 2.3184 (2.3039)\tAccu 0.0625 (0.0945)\t\n",
            "Epoch: [1][41/44]\tLoss 2.3032 (2.3039)\tAccu 0.0938 (0.0945)\t\n",
            "Epoch: [1][42/44]\tLoss 2.2991 (2.3038)\tAccu 0.1250 (0.0952)\t\n",
            "Epoch: [1][43/44]\tLoss 2.3286 (2.3044)\tAccu 0.0312 (0.0938)\t\n",
            "Epoch: [1][44/44]\tLoss 2.2749 (2.3039)\tAccu 0.3333 (0.0992)\t\n",
            "Accu 0.1100\t\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type FNet. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: [2][1/44]\tLoss 2.2995 (2.2995)\tAccu 0.1250 (0.1250)\t\n",
            "Epoch: [2][2/44]\tLoss 2.2938 (2.2967)\tAccu 0.1875 (0.1562)\t\n",
            "Epoch: [2][3/44]\tLoss 2.2926 (2.2953)\tAccu 0.1562 (0.1562)\t\n",
            "Epoch: [2][4/44]\tLoss 2.2947 (2.2952)\tAccu 0.1562 (0.1562)\t\n",
            "Epoch: [2][5/44]\tLoss 2.2935 (2.2948)\tAccu 0.0625 (0.1375)\t\n",
            "Epoch: [2][6/44]\tLoss 2.2842 (2.2931)\tAccu 0.1875 (0.1458)\t\n",
            "Epoch: [2][7/44]\tLoss 2.2858 (2.2920)\tAccu 0.1875 (0.1518)\t\n",
            "Epoch: [2][8/44]\tLoss 2.2966 (2.2926)\tAccu 0.2812 (0.1680)\t\n",
            "Epoch: [2][9/44]\tLoss 2.2894 (2.2922)\tAccu 0.0938 (0.1597)\t\n",
            "Epoch: [2][10/44]\tLoss 2.2755 (2.2906)\tAccu 0.2812 (0.1719)\t\n",
            "Epoch: [2][11/44]\tLoss 2.3098 (2.2923)\tAccu 0.0938 (0.1648)\t\n",
            "Epoch: [2][12/44]\tLoss 2.2976 (2.2928)\tAccu 0.0938 (0.1589)\t\n",
            "Epoch: [2][13/44]\tLoss 2.2880 (2.2924)\tAccu 0.1250 (0.1562)\t\n",
            "Epoch: [2][14/44]\tLoss 2.3040 (2.2932)\tAccu 0.0938 (0.1518)\t\n",
            "Epoch: [2][15/44]\tLoss 2.2874 (2.2928)\tAccu 0.1875 (0.1542)\t\n",
            "Epoch: [2][16/44]\tLoss 2.2901 (2.2927)\tAccu 0.1875 (0.1562)\t\n",
            "Epoch: [2][17/44]\tLoss 2.2822 (2.2920)\tAccu 0.1562 (0.1562)\t\n",
            "Epoch: [2][18/44]\tLoss 2.2999 (2.2925)\tAccu 0.1250 (0.1545)\t\n",
            "Epoch: [2][19/44]\tLoss 2.2706 (2.2913)\tAccu 0.1875 (0.1562)\t\n",
            "Epoch: [2][20/44]\tLoss 2.2948 (2.2915)\tAccu 0.0938 (0.1531)\t\n",
            "Epoch: [2][21/44]\tLoss 2.3052 (2.2922)\tAccu 0.0312 (0.1473)\t\n",
            "Epoch: [2][22/44]\tLoss 2.2739 (2.2913)\tAccu 0.0938 (0.1449)\t\n",
            "Epoch: [2][23/44]\tLoss 2.3079 (2.2920)\tAccu 0.0625 (0.1413)\t\n",
            "Epoch: [2][24/44]\tLoss 2.2804 (2.2916)\tAccu 0.1250 (0.1406)\t\n",
            "Epoch: [2][25/44]\tLoss 2.3335 (2.2932)\tAccu 0.0625 (0.1375)\t\n",
            "Epoch: [2][26/44]\tLoss 2.2909 (2.2931)\tAccu 0.1875 (0.1394)\t\n",
            "Epoch: [2][27/44]\tLoss 2.2898 (2.2930)\tAccu 0.1562 (0.1400)\t\n",
            "Epoch: [2][28/44]\tLoss 2.2924 (2.2930)\tAccu 0.1562 (0.1406)\t\n",
            "Epoch: [2][29/44]\tLoss 2.2893 (2.2929)\tAccu 0.1562 (0.1412)\t\n",
            "Epoch: [2][30/44]\tLoss 2.2868 (2.2927)\tAccu 0.1562 (0.1417)\t\n",
            "Epoch: [2][31/44]\tLoss 2.2888 (2.2925)\tAccu 0.1250 (0.1411)\t\n",
            "Epoch: [2][32/44]\tLoss 2.2855 (2.2923)\tAccu 0.1875 (0.1426)\t\n",
            "Epoch: [2][33/44]\tLoss 2.2735 (2.2918)\tAccu 0.1562 (0.1430)\t\n",
            "Epoch: [2][34/44]\tLoss 2.2835 (2.2915)\tAccu 0.0938 (0.1415)\t\n",
            "Epoch: [2][35/44]\tLoss 2.2984 (2.2917)\tAccu 0.0938 (0.1402)\t\n",
            "Epoch: [2][36/44]\tLoss 2.2892 (2.2916)\tAccu 0.1250 (0.1398)\t\n",
            "Epoch: [2][37/44]\tLoss 2.2965 (2.2918)\tAccu 0.0312 (0.1368)\t\n",
            "Epoch: [2][38/44]\tLoss 2.2786 (2.2914)\tAccu 0.1562 (0.1373)\t\n",
            "Epoch: [2][39/44]\tLoss 2.3108 (2.2919)\tAccu 0.0625 (0.1354)\t\n",
            "Epoch: [2][40/44]\tLoss 2.3059 (2.2923)\tAccu 0.1250 (0.1352)\t\n",
            "Epoch: [2][41/44]\tLoss 2.2977 (2.2924)\tAccu 0.0938 (0.1341)\t\n",
            "Epoch: [2][42/44]\tLoss 2.2923 (2.2924)\tAccu 0.0938 (0.1332)\t\n",
            "Epoch: [2][43/44]\tLoss 2.3165 (2.2930)\tAccu 0.0312 (0.1308)\t\n",
            "Epoch: [2][44/44]\tLoss 2.2540 (2.2923)\tAccu 0.2083 (0.1326)\t\n",
            "Accu 0.1100\t\n",
            "Epoch: [3][1/44]\tLoss 2.2889 (2.2889)\tAccu 0.1250 (0.1250)\t\n",
            "Epoch: [3][2/44]\tLoss 2.2756 (2.2823)\tAccu 0.1875 (0.1562)\t\n",
            "Epoch: [3][3/44]\tLoss 2.2814 (2.2820)\tAccu 0.1562 (0.1562)\t\n",
            "Epoch: [3][4/44]\tLoss 2.2836 (2.2824)\tAccu 0.1562 (0.1562)\t\n",
            "Epoch: [3][5/44]\tLoss 2.2897 (2.2839)\tAccu 0.0625 (0.1375)\t\n",
            "Epoch: [3][6/44]\tLoss 2.2757 (2.2825)\tAccu 0.1875 (0.1458)\t\n",
            "Epoch: [3][7/44]\tLoss 2.2773 (2.2818)\tAccu 0.1562 (0.1473)\t\n",
            "Epoch: [3][8/44]\tLoss 2.2819 (2.2818)\tAccu 0.3125 (0.1680)\t\n",
            "Epoch: [3][9/44]\tLoss 2.2834 (2.2820)\tAccu 0.1250 (0.1632)\t\n",
            "Epoch: [3][10/44]\tLoss 2.2611 (2.2799)\tAccu 0.2812 (0.1750)\t\n",
            "Epoch: [3][11/44]\tLoss 2.2943 (2.2812)\tAccu 0.0938 (0.1676)\t\n",
            "Epoch: [3][12/44]\tLoss 2.2858 (2.2816)\tAccu 0.1250 (0.1641)\t\n",
            "Epoch: [3][13/44]\tLoss 2.2796 (2.2814)\tAccu 0.1562 (0.1635)\t\n",
            "Epoch: [3][14/44]\tLoss 2.2940 (2.2823)\tAccu 0.1875 (0.1652)\t\n",
            "Epoch: [3][15/44]\tLoss 2.2774 (2.2820)\tAccu 0.2188 (0.1688)\t\n",
            "Epoch: [3][16/44]\tLoss 2.2778 (2.2817)\tAccu 0.1875 (0.1699)\t\n",
            "Epoch: [3][17/44]\tLoss 2.2781 (2.2815)\tAccu 0.1250 (0.1673)\t\n",
            "Epoch: [3][18/44]\tLoss 2.2895 (2.2820)\tAccu 0.1250 (0.1649)\t\n",
            "Epoch: [3][19/44]\tLoss 2.2594 (2.2808)\tAccu 0.2188 (0.1678)\t\n",
            "Epoch: [3][20/44]\tLoss 2.2816 (2.2808)\tAccu 0.1562 (0.1672)\t\n",
            "Epoch: [3][21/44]\tLoss 2.2987 (2.2817)\tAccu 0.1250 (0.1652)\t\n",
            "Epoch: [3][22/44]\tLoss 2.2611 (2.2807)\tAccu 0.1562 (0.1648)\t\n",
            "Epoch: [3][23/44]\tLoss 2.3005 (2.2816)\tAccu 0.0625 (0.1603)\t\n",
            "Epoch: [3][24/44]\tLoss 2.2738 (2.2813)\tAccu 0.1562 (0.1602)\t\n",
            "Epoch: [3][25/44]\tLoss 2.3315 (2.2833)\tAccu 0.0938 (0.1575)\t\n",
            "Epoch: [3][26/44]\tLoss 2.2796 (2.2831)\tAccu 0.2500 (0.1611)\t\n",
            "Epoch: [3][27/44]\tLoss 2.2849 (2.2832)\tAccu 0.1875 (0.1620)\t\n",
            "Epoch: [3][28/44]\tLoss 2.2821 (2.2832)\tAccu 0.1875 (0.1629)\t\n",
            "Epoch: [3][29/44]\tLoss 2.2756 (2.2829)\tAccu 0.2188 (0.1649)\t\n",
            "Epoch: [3][30/44]\tLoss 2.2788 (2.2828)\tAccu 0.1250 (0.1635)\t\n",
            "Epoch: [3][31/44]\tLoss 2.2805 (2.2827)\tAccu 0.0938 (0.1613)\t\n",
            "Epoch: [3][32/44]\tLoss 2.2736 (2.2824)\tAccu 0.2500 (0.1641)\t\n",
            "Epoch: [3][33/44]\tLoss 2.2646 (2.2819)\tAccu 0.2500 (0.1667)\t\n",
            "Epoch: [3][34/44]\tLoss 2.2739 (2.2816)\tAccu 0.0938 (0.1645)\t\n",
            "Epoch: [3][35/44]\tLoss 2.2896 (2.2818)\tAccu 0.1250 (0.1634)\t\n",
            "Epoch: [3][36/44]\tLoss 2.2776 (2.2817)\tAccu 0.1562 (0.1632)\t\n",
            "Epoch: [3][37/44]\tLoss 2.2860 (2.2818)\tAccu 0.1562 (0.1630)\t\n",
            "Epoch: [3][38/44]\tLoss 2.2712 (2.2816)\tAccu 0.1250 (0.1620)\t\n",
            "Epoch: [3][39/44]\tLoss 2.3031 (2.2821)\tAccu 0.0938 (0.1603)\t\n",
            "Epoch: [3][40/44]\tLoss 2.2974 (2.2825)\tAccu 0.1875 (0.1609)\t\n",
            "Epoch: [3][41/44]\tLoss 2.2919 (2.2827)\tAccu 0.1250 (0.1601)\t\n",
            "Epoch: [3][42/44]\tLoss 2.2846 (2.2828)\tAccu 0.0938 (0.1585)\t\n",
            "Epoch: [3][43/44]\tLoss 2.3088 (2.2834)\tAccu 0.0312 (0.1555)\t\n",
            "Epoch: [3][44/44]\tLoss 2.2302 (2.2825)\tAccu 0.1667 (0.1558)\t\n",
            "Accu 0.1100\t\n",
            "Epoch: [4][1/44]\tLoss 2.2811 (2.2811)\tAccu 0.1250 (0.1250)\t\n",
            "Epoch: [4][2/44]\tLoss 2.2597 (2.2704)\tAccu 0.1875 (0.1562)\t\n",
            "Epoch: [4][3/44]\tLoss 2.2700 (2.2703)\tAccu 0.1562 (0.1562)\t\n",
            "Epoch: [4][4/44]\tLoss 2.2741 (2.2712)\tAccu 0.2188 (0.1719)\t\n",
            "Epoch: [4][5/44]\tLoss 2.2857 (2.2741)\tAccu 0.0938 (0.1562)\t\n",
            "Epoch: [4][6/44]\tLoss 2.2675 (2.2730)\tAccu 0.1562 (0.1562)\t\n",
            "Epoch: [4][7/44]\tLoss 2.2683 (2.2723)\tAccu 0.1562 (0.1562)\t\n",
            "Epoch: [4][8/44]\tLoss 2.2665 (2.2716)\tAccu 0.3125 (0.1758)\t\n",
            "Epoch: [4][9/44]\tLoss 2.2740 (2.2719)\tAccu 0.1250 (0.1701)\t\n",
            "Epoch: [4][10/44]\tLoss 2.2512 (2.2698)\tAccu 0.2500 (0.1781)\t\n",
            "Epoch: [4][11/44]\tLoss 2.2823 (2.2709)\tAccu 0.0938 (0.1705)\t\n",
            "Epoch: [4][12/44]\tLoss 2.2725 (2.2711)\tAccu 0.1562 (0.1693)\t\n",
            "Epoch: [4][13/44]\tLoss 2.2733 (2.2712)\tAccu 0.1875 (0.1707)\t\n",
            "Epoch: [4][14/44]\tLoss 2.2865 (2.2723)\tAccu 0.1875 (0.1719)\t\n",
            "Epoch: [4][15/44]\tLoss 2.2671 (2.2720)\tAccu 0.2500 (0.1771)\t\n",
            "Epoch: [4][16/44]\tLoss 2.2679 (2.2717)\tAccu 0.2812 (0.1836)\t\n",
            "Epoch: [4][17/44]\tLoss 2.2734 (2.2718)\tAccu 0.1562 (0.1820)\t\n",
            "Epoch: [4][18/44]\tLoss 2.2826 (2.2724)\tAccu 0.1562 (0.1806)\t\n",
            "Epoch: [4][19/44]\tLoss 2.2486 (2.2712)\tAccu 0.2500 (0.1842)\t\n",
            "Epoch: [4][20/44]\tLoss 2.2697 (2.2711)\tAccu 0.1875 (0.1844)\t\n",
            "Epoch: [4][21/44]\tLoss 2.2950 (2.2722)\tAccu 0.1250 (0.1815)\t\n",
            "Epoch: [4][22/44]\tLoss 2.2477 (2.2711)\tAccu 0.1562 (0.1804)\t\n",
            "Epoch: [4][23/44]\tLoss 2.2950 (2.2722)\tAccu 0.0938 (0.1766)\t\n",
            "Epoch: [4][24/44]\tLoss 2.2655 (2.2719)\tAccu 0.1875 (0.1771)\t\n",
            "Epoch: [4][25/44]\tLoss 2.3292 (2.2742)\tAccu 0.0938 (0.1737)\t\n",
            "Epoch: [4][26/44]\tLoss 2.2682 (2.2739)\tAccu 0.2812 (0.1779)\t\n",
            "Epoch: [4][27/44]\tLoss 2.2786 (2.2741)\tAccu 0.1875 (0.1782)\t\n",
            "Epoch: [4][28/44]\tLoss 2.2738 (2.2741)\tAccu 0.2188 (0.1797)\t\n",
            "Epoch: [4][29/44]\tLoss 2.2631 (2.2737)\tAccu 0.2500 (0.1821)\t\n",
            "Epoch: [4][30/44]\tLoss 2.2716 (2.2737)\tAccu 0.1562 (0.1812)\t\n",
            "Epoch: [4][31/44]\tLoss 2.2683 (2.2735)\tAccu 0.1250 (0.1794)\t\n",
            "Epoch: [4][32/44]\tLoss 2.2647 (2.2732)\tAccu 0.2188 (0.1807)\t\n",
            "Epoch: [4][33/44]\tLoss 2.2554 (2.2727)\tAccu 0.2188 (0.1818)\t\n",
            "Epoch: [4][34/44]\tLoss 2.2644 (2.2724)\tAccu 0.0625 (0.1783)\t\n",
            "Epoch: [4][35/44]\tLoss 2.2809 (2.2727)\tAccu 0.0938 (0.1759)\t\n",
            "Epoch: [4][36/44]\tLoss 2.2666 (2.2725)\tAccu 0.1562 (0.1753)\t\n",
            "Epoch: [4][37/44]\tLoss 2.2766 (2.2726)\tAccu 0.1875 (0.1757)\t\n",
            "Epoch: [4][38/44]\tLoss 2.2662 (2.2724)\tAccu 0.1250 (0.1743)\t\n",
            "Epoch: [4][39/44]\tLoss 2.2944 (2.2730)\tAccu 0.1562 (0.1739)\t\n",
            "Epoch: [4][40/44]\tLoss 2.2885 (2.2734)\tAccu 0.1562 (0.1734)\t\n",
            "Epoch: [4][41/44]\tLoss 2.2851 (2.2737)\tAccu 0.1250 (0.1723)\t\n",
            "Epoch: [4][42/44]\tLoss 2.2770 (2.2738)\tAccu 0.1250 (0.1711)\t\n",
            "Epoch: [4][43/44]\tLoss 2.2972 (2.2743)\tAccu 0.0312 (0.1679)\t\n",
            "Epoch: [4][44/44]\tLoss 2.2086 (2.2732)\tAccu 0.1667 (0.1679)\t\n",
            "Accu 0.1050\t\n",
            "Epoch: [5][1/44]\tLoss 2.2711 (2.2711)\tAccu 0.1250 (0.1250)\t\n",
            "Epoch: [5][2/44]\tLoss 2.2454 (2.2583)\tAccu 0.1875 (0.1562)\t\n",
            "Epoch: [5][3/44]\tLoss 2.2595 (2.2587)\tAccu 0.1562 (0.1562)\t\n",
            "Epoch: [5][4/44]\tLoss 2.2640 (2.2600)\tAccu 0.1875 (0.1641)\t\n",
            "Epoch: [5][5/44]\tLoss 2.2830 (2.2646)\tAccu 0.0938 (0.1500)\t\n",
            "Epoch: [5][6/44]\tLoss 2.2581 (2.2635)\tAccu 0.1562 (0.1510)\t\n",
            "Epoch: [5][7/44]\tLoss 2.2584 (2.2628)\tAccu 0.1562 (0.1518)\t\n",
            "Epoch: [5][8/44]\tLoss 2.2494 (2.2611)\tAccu 0.3125 (0.1719)\t\n",
            "Epoch: [5][9/44]\tLoss 2.2646 (2.2615)\tAccu 0.1250 (0.1667)\t\n",
            "Epoch: [5][10/44]\tLoss 2.2389 (2.2592)\tAccu 0.2812 (0.1781)\t\n",
            "Epoch: [5][11/44]\tLoss 2.2658 (2.2598)\tAccu 0.1562 (0.1761)\t\n",
            "Epoch: [5][12/44]\tLoss 2.2590 (2.2598)\tAccu 0.1875 (0.1771)\t\n",
            "Epoch: [5][13/44]\tLoss 2.2663 (2.2603)\tAccu 0.1875 (0.1779)\t\n",
            "Epoch: [5][14/44]\tLoss 2.2757 (2.2614)\tAccu 0.2500 (0.1830)\t\n",
            "Epoch: [5][15/44]\tLoss 2.2590 (2.2612)\tAccu 0.2188 (0.1854)\t\n",
            "Epoch: [5][16/44]\tLoss 2.2570 (2.2609)\tAccu 0.2500 (0.1895)\t\n",
            "Epoch: [5][17/44]\tLoss 2.2690 (2.2614)\tAccu 0.1562 (0.1875)\t\n",
            "Epoch: [5][18/44]\tLoss 2.2744 (2.2621)\tAccu 0.1875 (0.1875)\t\n",
            "Epoch: [5][19/44]\tLoss 2.2385 (2.2609)\tAccu 0.2500 (0.1908)\t\n",
            "Epoch: [5][20/44]\tLoss 2.2585 (2.2608)\tAccu 0.2188 (0.1922)\t\n",
            "Epoch: [5][21/44]\tLoss 2.2918 (2.2623)\tAccu 0.1250 (0.1890)\t\n",
            "Epoch: [5][22/44]\tLoss 2.2330 (2.2609)\tAccu 0.1875 (0.1889)\t\n",
            "Epoch: [5][23/44]\tLoss 2.2905 (2.2622)\tAccu 0.0938 (0.1848)\t\n",
            "Epoch: [5][24/44]\tLoss 2.2574 (2.2620)\tAccu 0.2812 (0.1888)\t\n",
            "Epoch: [5][25/44]\tLoss 2.3274 (2.2646)\tAccu 0.0938 (0.1850)\t\n",
            "Epoch: [5][26/44]\tLoss 2.2560 (2.2643)\tAccu 0.3125 (0.1899)\t\n",
            "Epoch: [5][27/44]\tLoss 2.2712 (2.2646)\tAccu 0.2188 (0.1910)\t\n",
            "Epoch: [5][28/44]\tLoss 2.2637 (2.2645)\tAccu 0.2188 (0.1920)\t\n",
            "Epoch: [5][29/44]\tLoss 2.2504 (2.2640)\tAccu 0.2500 (0.1940)\t\n",
            "Epoch: [5][30/44]\tLoss 2.2636 (2.2640)\tAccu 0.1562 (0.1927)\t\n",
            "Epoch: [5][31/44]\tLoss 2.2569 (2.2638)\tAccu 0.1562 (0.1915)\t\n",
            "Epoch: [5][32/44]\tLoss 2.2546 (2.2635)\tAccu 0.2188 (0.1924)\t\n",
            "Epoch: [5][33/44]\tLoss 2.2456 (2.2630)\tAccu 0.2188 (0.1932)\t\n",
            "Epoch: [5][34/44]\tLoss 2.2515 (2.2626)\tAccu 0.1562 (0.1921)\t\n",
            "Epoch: [5][35/44]\tLoss 2.2708 (2.2629)\tAccu 0.0938 (0.1893)\t\n",
            "Epoch: [5][36/44]\tLoss 2.2538 (2.2626)\tAccu 0.1562 (0.1884)\t\n",
            "Epoch: [5][37/44]\tLoss 2.2663 (2.2627)\tAccu 0.1875 (0.1883)\t\n",
            "Epoch: [5][38/44]\tLoss 2.2579 (2.2626)\tAccu 0.1250 (0.1867)\t\n",
            "Epoch: [5][39/44]\tLoss 2.2854 (2.2632)\tAccu 0.1250 (0.1851)\t\n",
            "Epoch: [5][40/44]\tLoss 2.2792 (2.2636)\tAccu 0.1562 (0.1844)\t\n",
            "Epoch: [5][41/44]\tLoss 2.2798 (2.2640)\tAccu 0.1250 (0.1829)\t\n",
            "Epoch: [5][42/44]\tLoss 2.2701 (2.2641)\tAccu 0.1250 (0.1815)\t\n",
            "Epoch: [5][43/44]\tLoss 2.2884 (2.2647)\tAccu 0.0625 (0.1788)\t\n",
            "Epoch: [5][44/44]\tLoss 2.1845 (2.2633)\tAccu 0.1667 (0.1785)\t\n",
            "Accu 0.1050\t\n",
            "Epoch: [6][1/44]\tLoss 2.2628 (2.2628)\tAccu 0.0938 (0.0938)\t\n",
            "Epoch: [6][2/44]\tLoss 2.2309 (2.2468)\tAccu 0.2188 (0.1562)\t\n",
            "Epoch: [6][3/44]\tLoss 2.2479 (2.2472)\tAccu 0.1875 (0.1667)\t\n",
            "Epoch: [6][4/44]\tLoss 2.2511 (2.2482)\tAccu 0.2188 (0.1797)\t\n",
            "Epoch: [6][5/44]\tLoss 2.2790 (2.2544)\tAccu 0.1250 (0.1688)\t\n",
            "Epoch: [6][6/44]\tLoss 2.2464 (2.2530)\tAccu 0.2188 (0.1771)\t\n",
            "Epoch: [6][7/44]\tLoss 2.2515 (2.2528)\tAccu 0.1875 (0.1786)\t\n",
            "Epoch: [6][8/44]\tLoss 2.2329 (2.2503)\tAccu 0.3438 (0.1992)\t\n",
            "Epoch: [6][9/44]\tLoss 2.2525 (2.2506)\tAccu 0.1250 (0.1910)\t\n",
            "Epoch: [6][10/44]\tLoss 2.2288 (2.2484)\tAccu 0.2500 (0.1969)\t\n",
            "Epoch: [6][11/44]\tLoss 2.2528 (2.2488)\tAccu 0.1562 (0.1932)\t\n",
            "Epoch: [6][12/44]\tLoss 2.2435 (2.2484)\tAccu 0.1875 (0.1927)\t\n",
            "Epoch: [6][13/44]\tLoss 2.2590 (2.2492)\tAccu 0.1875 (0.1923)\t\n",
            "Epoch: [6][14/44]\tLoss 2.2671 (2.2505)\tAccu 0.2500 (0.1964)\t\n",
            "Epoch: [6][15/44]\tLoss 2.2489 (2.2503)\tAccu 0.2188 (0.1979)\t\n",
            "Epoch: [6][16/44]\tLoss 2.2493 (2.2503)\tAccu 0.2188 (0.1992)\t\n",
            "Epoch: [6][17/44]\tLoss 2.2627 (2.2510)\tAccu 0.1875 (0.1985)\t\n",
            "Epoch: [6][18/44]\tLoss 2.2658 (2.2518)\tAccu 0.1562 (0.1962)\t\n",
            "Epoch: [6][19/44]\tLoss 2.2283 (2.2506)\tAccu 0.2500 (0.1990)\t\n",
            "Epoch: [6][20/44]\tLoss 2.2442 (2.2503)\tAccu 0.2188 (0.2000)\t\n",
            "Epoch: [6][21/44]\tLoss 2.2872 (2.2520)\tAccu 0.1250 (0.1964)\t\n",
            "Epoch: [6][22/44]\tLoss 2.2194 (2.2505)\tAccu 0.2188 (0.1974)\t\n",
            "Epoch: [6][23/44]\tLoss 2.2837 (2.2520)\tAccu 0.1250 (0.1943)\t\n",
            "Epoch: [6][24/44]\tLoss 2.2512 (2.2520)\tAccu 0.2812 (0.1979)\t\n",
            "Epoch: [6][25/44]\tLoss 2.3233 (2.2548)\tAccu 0.0938 (0.1938)\t\n",
            "Epoch: [6][26/44]\tLoss 2.2430 (2.2544)\tAccu 0.2812 (0.1971)\t\n",
            "Epoch: [6][27/44]\tLoss 2.2656 (2.2548)\tAccu 0.2188 (0.1979)\t\n",
            "Epoch: [6][28/44]\tLoss 2.2529 (2.2547)\tAccu 0.2188 (0.1987)\t\n",
            "Epoch: [6][29/44]\tLoss 2.2359 (2.2541)\tAccu 0.2500 (0.2004)\t\n",
            "Epoch: [6][30/44]\tLoss 2.2551 (2.2541)\tAccu 0.1562 (0.1990)\t\n",
            "Epoch: [6][31/44]\tLoss 2.2447 (2.2538)\tAccu 0.0938 (0.1956)\t\n",
            "Epoch: [6][32/44]\tLoss 2.2420 (2.2534)\tAccu 0.2812 (0.1982)\t\n",
            "Epoch: [6][33/44]\tLoss 2.2378 (2.2529)\tAccu 0.2188 (0.1989)\t\n",
            "Epoch: [6][34/44]\tLoss 2.2420 (2.2526)\tAccu 0.1562 (0.1976)\t\n",
            "Epoch: [6][35/44]\tLoss 2.2628 (2.2529)\tAccu 0.1250 (0.1955)\t\n",
            "Epoch: [6][36/44]\tLoss 2.2392 (2.2525)\tAccu 0.1875 (0.1953)\t\n",
            "Epoch: [6][37/44]\tLoss 2.2560 (2.2526)\tAccu 0.1562 (0.1943)\t\n",
            "Epoch: [6][38/44]\tLoss 2.2492 (2.2525)\tAccu 0.1562 (0.1933)\t\n",
            "Epoch: [6][39/44]\tLoss 2.2775 (2.2532)\tAccu 0.0938 (0.1907)\t\n",
            "Epoch: [6][40/44]\tLoss 2.2717 (2.2536)\tAccu 0.1562 (0.1898)\t\n",
            "Epoch: [6][41/44]\tLoss 2.2725 (2.2541)\tAccu 0.1250 (0.1883)\t\n",
            "Epoch: [6][42/44]\tLoss 2.2621 (2.2543)\tAccu 0.1562 (0.1875)\t\n",
            "Epoch: [6][43/44]\tLoss 2.2780 (2.2548)\tAccu 0.0938 (0.1853)\t\n",
            "Epoch: [6][44/44]\tLoss 2.1603 (2.2532)\tAccu 0.1667 (0.1849)\t\n",
            "Accu 0.1250\t\n",
            "Epoch: [7][1/44]\tLoss 2.2526 (2.2526)\tAccu 0.1562 (0.1562)\t\n",
            "Epoch: [7][2/44]\tLoss 2.2183 (2.2355)\tAccu 0.2188 (0.1875)\t\n",
            "Epoch: [7][3/44]\tLoss 2.2373 (2.2361)\tAccu 0.1562 (0.1771)\t\n",
            "Epoch: [7][4/44]\tLoss 2.2399 (2.2370)\tAccu 0.1562 (0.1719)\t\n",
            "Epoch: [7][5/44]\tLoss 2.2711 (2.2438)\tAccu 0.1875 (0.1750)\t\n",
            "Epoch: [7][6/44]\tLoss 2.2330 (2.2420)\tAccu 0.1875 (0.1771)\t\n",
            "Epoch: [7][7/44]\tLoss 2.2421 (2.2420)\tAccu 0.1875 (0.1786)\t\n",
            "Epoch: [7][8/44]\tLoss 2.2144 (2.2386)\tAccu 0.3438 (0.1992)\t\n",
            "Epoch: [7][9/44]\tLoss 2.2398 (2.2387)\tAccu 0.1250 (0.1910)\t\n",
            "Epoch: [7][10/44]\tLoss 2.2157 (2.2364)\tAccu 0.2812 (0.2000)\t\n",
            "Epoch: [7][11/44]\tLoss 2.2355 (2.2363)\tAccu 0.1875 (0.1989)\t\n",
            "Epoch: [7][12/44]\tLoss 2.2258 (2.2355)\tAccu 0.2188 (0.2005)\t\n",
            "Epoch: [7][13/44]\tLoss 2.2513 (2.2367)\tAccu 0.1875 (0.1995)\t\n",
            "Epoch: [7][14/44]\tLoss 2.2556 (2.2380)\tAccu 0.2188 (0.2009)\t\n",
            "Epoch: [7][15/44]\tLoss 2.2399 (2.2381)\tAccu 0.1875 (0.2000)\t\n",
            "Epoch: [7][16/44]\tLoss 2.2387 (2.2382)\tAccu 0.2500 (0.2031)\t\n",
            "Epoch: [7][17/44]\tLoss 2.2601 (2.2395)\tAccu 0.2188 (0.2040)\t\n",
            "Epoch: [7][18/44]\tLoss 2.2570 (2.2404)\tAccu 0.1875 (0.2031)\t\n",
            "Epoch: [7][19/44]\tLoss 2.2148 (2.2391)\tAccu 0.2500 (0.2056)\t\n",
            "Epoch: [7][20/44]\tLoss 2.2310 (2.2387)\tAccu 0.2500 (0.2078)\t\n",
            "Epoch: [7][21/44]\tLoss 2.2837 (2.2408)\tAccu 0.0938 (0.2024)\t\n",
            "Epoch: [7][22/44]\tLoss 2.2041 (2.2392)\tAccu 0.2188 (0.2031)\t\n",
            "Epoch: [7][23/44]\tLoss 2.2778 (2.2408)\tAccu 0.1562 (0.2011)\t\n",
            "Epoch: [7][24/44]\tLoss 2.2427 (2.2409)\tAccu 0.2812 (0.2044)\t\n",
            "Epoch: [7][25/44]\tLoss 2.3228 (2.2442)\tAccu 0.1250 (0.2013)\t\n",
            "Epoch: [7][26/44]\tLoss 2.2286 (2.2436)\tAccu 0.3125 (0.2055)\t\n",
            "Epoch: [7][27/44]\tLoss 2.2612 (2.2442)\tAccu 0.2188 (0.2060)\t\n",
            "Epoch: [7][28/44]\tLoss 2.2403 (2.2441)\tAccu 0.2500 (0.2076)\t\n",
            "Epoch: [7][29/44]\tLoss 2.2221 (2.2433)\tAccu 0.2188 (0.2080)\t\n",
            "Epoch: [7][30/44]\tLoss 2.2450 (2.2434)\tAccu 0.1562 (0.2062)\t\n",
            "Epoch: [7][31/44]\tLoss 2.2348 (2.2431)\tAccu 0.0938 (0.2026)\t\n",
            "Epoch: [7][32/44]\tLoss 2.2292 (2.2427)\tAccu 0.2812 (0.2051)\t\n",
            "Epoch: [7][33/44]\tLoss 2.2295 (2.2423)\tAccu 0.2188 (0.2055)\t\n",
            "Epoch: [7][34/44]\tLoss 2.2337 (2.2420)\tAccu 0.1250 (0.2031)\t\n",
            "Epoch: [7][35/44]\tLoss 2.2535 (2.2424)\tAccu 0.1250 (0.2009)\t\n",
            "Epoch: [7][36/44]\tLoss 2.2269 (2.2419)\tAccu 0.1875 (0.2005)\t\n",
            "Epoch: [7][37/44]\tLoss 2.2456 (2.2420)\tAccu 0.2188 (0.2010)\t\n",
            "Epoch: [7][38/44]\tLoss 2.2407 (2.2420)\tAccu 0.1562 (0.1998)\t\n",
            "Epoch: [7][39/44]\tLoss 2.2679 (2.2427)\tAccu 0.0938 (0.1971)\t\n",
            "Epoch: [7][40/44]\tLoss 2.2608 (2.2431)\tAccu 0.1562 (0.1961)\t\n",
            "Epoch: [7][41/44]\tLoss 2.2643 (2.2436)\tAccu 0.1250 (0.1944)\t\n",
            "Epoch: [7][42/44]\tLoss 2.2550 (2.2439)\tAccu 0.1562 (0.1935)\t\n",
            "Epoch: [7][43/44]\tLoss 2.2645 (2.2444)\tAccu 0.0938 (0.1911)\t\n",
            "Epoch: [7][44/44]\tLoss 2.1367 (2.2425)\tAccu 0.1667 (0.1906)\t\n",
            "Accu 0.1350\t\n",
            "Epoch: [8][1/44]\tLoss 2.2426 (2.2426)\tAccu 0.1562 (0.1562)\t\n",
            "Epoch: [8][2/44]\tLoss 2.2041 (2.2234)\tAccu 0.2188 (0.1875)\t\n",
            "Epoch: [8][3/44]\tLoss 2.2245 (2.2238)\tAccu 0.1250 (0.1667)\t\n",
            "Epoch: [8][4/44]\tLoss 2.2255 (2.2242)\tAccu 0.1562 (0.1641)\t\n",
            "Epoch: [8][5/44]\tLoss 2.2636 (2.2321)\tAccu 0.1875 (0.1688)\t\n",
            "Epoch: [8][6/44]\tLoss 2.2220 (2.2304)\tAccu 0.2188 (0.1771)\t\n",
            "Epoch: [8][7/44]\tLoss 2.2343 (2.2310)\tAccu 0.2812 (0.1920)\t\n",
            "Epoch: [8][8/44]\tLoss 2.1962 (2.2266)\tAccu 0.3750 (0.2148)\t\n",
            "Epoch: [8][9/44]\tLoss 2.2274 (2.2267)\tAccu 0.0938 (0.2014)\t\n",
            "Epoch: [8][10/44]\tLoss 2.2048 (2.2245)\tAccu 0.2812 (0.2094)\t\n",
            "Epoch: [8][11/44]\tLoss 2.2176 (2.2239)\tAccu 0.2188 (0.2102)\t\n",
            "Epoch: [8][12/44]\tLoss 2.2072 (2.2225)\tAccu 0.2500 (0.2135)\t\n",
            "Epoch: [8][13/44]\tLoss 2.2452 (2.2242)\tAccu 0.1875 (0.2115)\t\n",
            "Epoch: [8][14/44]\tLoss 2.2478 (2.2259)\tAccu 0.2188 (0.2121)\t\n",
            "Epoch: [8][15/44]\tLoss 2.2305 (2.2262)\tAccu 0.1875 (0.2104)\t\n",
            "Epoch: [8][16/44]\tLoss 2.2280 (2.2263)\tAccu 0.2500 (0.2129)\t\n",
            "Epoch: [8][17/44]\tLoss 2.2571 (2.2282)\tAccu 0.2500 (0.2151)\t\n",
            "Epoch: [8][18/44]\tLoss 2.2478 (2.2292)\tAccu 0.2188 (0.2153)\t\n",
            "Epoch: [8][19/44]\tLoss 2.2012 (2.2278)\tAccu 0.2500 (0.2171)\t\n",
            "Epoch: [8][20/44]\tLoss 2.2177 (2.2273)\tAccu 0.2500 (0.2188)\t\n",
            "Epoch: [8][21/44]\tLoss 2.2802 (2.2298)\tAccu 0.0938 (0.2128)\t\n",
            "Epoch: [8][22/44]\tLoss 2.1906 (2.2280)\tAccu 0.2188 (0.2131)\t\n",
            "Epoch: [8][23/44]\tLoss 2.2695 (2.2298)\tAccu 0.1562 (0.2106)\t\n",
            "Epoch: [8][24/44]\tLoss 2.2346 (2.2300)\tAccu 0.2812 (0.2135)\t\n",
            "Epoch: [8][25/44]\tLoss 2.3190 (2.2336)\tAccu 0.1562 (0.2112)\t\n",
            "Epoch: [8][26/44]\tLoss 2.2130 (2.2328)\tAccu 0.2812 (0.2139)\t\n",
            "Epoch: [8][27/44]\tLoss 2.2552 (2.2336)\tAccu 0.2188 (0.2141)\t\n",
            "Epoch: [8][28/44]\tLoss 2.2280 (2.2334)\tAccu 0.2188 (0.2143)\t\n",
            "Epoch: [8][29/44]\tLoss 2.2077 (2.2325)\tAccu 0.2188 (0.2144)\t\n",
            "Epoch: [8][30/44]\tLoss 2.2347 (2.2326)\tAccu 0.1562 (0.2125)\t\n",
            "Epoch: [8][31/44]\tLoss 2.2232 (2.2323)\tAccu 0.0938 (0.2087)\t\n",
            "Epoch: [8][32/44]\tLoss 2.2171 (2.2318)\tAccu 0.2500 (0.2100)\t\n",
            "Epoch: [8][33/44]\tLoss 2.2218 (2.2315)\tAccu 0.2188 (0.2102)\t\n",
            "Epoch: [8][34/44]\tLoss 2.2236 (2.2313)\tAccu 0.1250 (0.2077)\t\n",
            "Epoch: [8][35/44]\tLoss 2.2444 (2.2317)\tAccu 0.1250 (0.2054)\t\n",
            "Epoch: [8][36/44]\tLoss 2.2140 (2.2312)\tAccu 0.2500 (0.2066)\t\n",
            "Epoch: [8][37/44]\tLoss 2.2349 (2.2313)\tAccu 0.2188 (0.2069)\t\n",
            "Epoch: [8][38/44]\tLoss 2.2320 (2.2313)\tAccu 0.1562 (0.2056)\t\n",
            "Epoch: [8][39/44]\tLoss 2.2580 (2.2320)\tAccu 0.1875 (0.2051)\t\n",
            "Epoch: [8][40/44]\tLoss 2.2517 (2.2325)\tAccu 0.1875 (0.2047)\t\n",
            "Epoch: [8][41/44]\tLoss 2.2564 (2.2330)\tAccu 0.1562 (0.2035)\t\n",
            "Epoch: [8][42/44]\tLoss 2.2454 (2.2333)\tAccu 0.1875 (0.2031)\t\n",
            "Epoch: [8][43/44]\tLoss 2.2543 (2.2338)\tAccu 0.0625 (0.1999)\t\n",
            "Epoch: [8][44/44]\tLoss 2.1145 (2.2318)\tAccu 0.1667 (0.1991)\t\n",
            "Accu 0.1450\t\n",
            "Epoch: [9][1/44]\tLoss 2.2318 (2.2318)\tAccu 0.1875 (0.1875)\t\n",
            "Epoch: [9][2/44]\tLoss 2.1908 (2.2113)\tAccu 0.2500 (0.2188)\t\n",
            "Epoch: [9][3/44]\tLoss 2.2122 (2.2116)\tAccu 0.1250 (0.1875)\t\n",
            "Epoch: [9][4/44]\tLoss 2.2115 (2.2116)\tAccu 0.2188 (0.1953)\t\n",
            "Epoch: [9][5/44]\tLoss 2.2558 (2.2204)\tAccu 0.2188 (0.2000)\t\n",
            "Epoch: [9][6/44]\tLoss 2.2074 (2.2182)\tAccu 0.2188 (0.2031)\t\n",
            "Epoch: [9][7/44]\tLoss 2.2261 (2.2194)\tAccu 0.1875 (0.2009)\t\n",
            "Epoch: [9][8/44]\tLoss 2.1769 (2.2141)\tAccu 0.3750 (0.2227)\t\n",
            "Epoch: [9][9/44]\tLoss 2.2156 (2.2142)\tAccu 0.0938 (0.2083)\t\n",
            "Epoch: [9][10/44]\tLoss 2.1945 (2.2123)\tAccu 0.2812 (0.2156)\t\n",
            "Epoch: [9][11/44]\tLoss 2.1984 (2.2110)\tAccu 0.2188 (0.2159)\t\n",
            "Epoch: [9][12/44]\tLoss 2.1864 (2.2089)\tAccu 0.2812 (0.2214)\t\n",
            "Epoch: [9][13/44]\tLoss 2.2389 (2.2113)\tAccu 0.1875 (0.2188)\t\n",
            "Epoch: [9][14/44]\tLoss 2.2372 (2.2131)\tAccu 0.2188 (0.2188)\t\n",
            "Epoch: [9][15/44]\tLoss 2.2215 (2.2137)\tAccu 0.1875 (0.2167)\t\n",
            "Epoch: [9][16/44]\tLoss 2.2175 (2.2139)\tAccu 0.2500 (0.2188)\t\n",
            "Epoch: [9][17/44]\tLoss 2.2531 (2.2162)\tAccu 0.2500 (0.2206)\t\n",
            "Epoch: [9][18/44]\tLoss 2.2387 (2.2175)\tAccu 0.2188 (0.2205)\t\n",
            "Epoch: [9][19/44]\tLoss 2.1881 (2.2159)\tAccu 0.2812 (0.2237)\t\n",
            "Epoch: [9][20/44]\tLoss 2.2032 (2.2153)\tAccu 0.2812 (0.2266)\t\n",
            "Epoch: [9][21/44]\tLoss 2.2777 (2.2183)\tAccu 0.1250 (0.2217)\t\n",
            "Epoch: [9][22/44]\tLoss 2.1735 (2.2162)\tAccu 0.2500 (0.2230)\t\n",
            "Epoch: [9][23/44]\tLoss 2.2644 (2.2183)\tAccu 0.1562 (0.2201)\t\n",
            "Epoch: [9][24/44]\tLoss 2.2267 (2.2187)\tAccu 0.2812 (0.2227)\t\n",
            "Epoch: [9][25/44]\tLoss 2.3168 (2.2226)\tAccu 0.1562 (0.2200)\t\n",
            "Epoch: [9][26/44]\tLoss 2.1978 (2.2216)\tAccu 0.2812 (0.2224)\t\n",
            "Epoch: [9][27/44]\tLoss 2.2485 (2.2226)\tAccu 0.2188 (0.2222)\t\n",
            "Epoch: [9][28/44]\tLoss 2.2144 (2.2223)\tAccu 0.2188 (0.2221)\t\n",
            "Epoch: [9][29/44]\tLoss 2.1953 (2.2214)\tAccu 0.2188 (0.2220)\t\n",
            "Epoch: [9][30/44]\tLoss 2.2242 (2.2215)\tAccu 0.1875 (0.2208)\t\n",
            "Epoch: [9][31/44]\tLoss 2.2118 (2.2212)\tAccu 0.0938 (0.2167)\t\n",
            "Epoch: [9][32/44]\tLoss 2.2031 (2.2206)\tAccu 0.2500 (0.2178)\t\n",
            "Epoch: [9][33/44]\tLoss 2.2139 (2.2204)\tAccu 0.2188 (0.2178)\t\n",
            "Epoch: [9][34/44]\tLoss 2.2153 (2.2203)\tAccu 0.1562 (0.2160)\t\n",
            "Epoch: [9][35/44]\tLoss 2.2352 (2.2207)\tAccu 0.1250 (0.2134)\t\n",
            "Epoch: [9][36/44]\tLoss 2.2023 (2.2202)\tAccu 0.2812 (0.2153)\t\n",
            "Epoch: [9][37/44]\tLoss 2.2242 (2.2203)\tAccu 0.2500 (0.2162)\t\n",
            "Epoch: [9][38/44]\tLoss 2.2237 (2.2204)\tAccu 0.1562 (0.2146)\t\n",
            "Epoch: [9][39/44]\tLoss 2.2511 (2.2212)\tAccu 0.1875 (0.2139)\t\n",
            "Epoch: [9][40/44]\tLoss 2.2444 (2.2218)\tAccu 0.1875 (0.2133)\t\n",
            "Epoch: [9][41/44]\tLoss 2.2501 (2.2224)\tAccu 0.1562 (0.2119)\t\n",
            "Epoch: [9][42/44]\tLoss 2.2367 (2.2228)\tAccu 0.1875 (0.2113)\t\n",
            "Epoch: [9][43/44]\tLoss 2.2416 (2.2232)\tAccu 0.0625 (0.2078)\t\n",
            "Epoch: [9][44/44]\tLoss 2.0900 (2.2209)\tAccu 0.1667 (0.2069)\t\n",
            "Accu 0.1550\t\n",
            "Epoch: [10][1/44]\tLoss 2.2233 (2.2233)\tAccu 0.1875 (0.1875)\t\n",
            "Epoch: [10][2/44]\tLoss 2.1794 (2.2013)\tAccu 0.2500 (0.2188)\t\n",
            "Epoch: [10][3/44]\tLoss 2.2001 (2.2009)\tAccu 0.1250 (0.1875)\t\n",
            "Epoch: [10][4/44]\tLoss 2.1972 (2.2000)\tAccu 0.2500 (0.2031)\t\n",
            "Epoch: [10][5/44]\tLoss 2.2454 (2.2091)\tAccu 0.2500 (0.2125)\t\n",
            "Epoch: [10][6/44]\tLoss 2.1945 (2.2067)\tAccu 0.1875 (0.2083)\t\n",
            "Epoch: [10][7/44]\tLoss 2.2178 (2.2082)\tAccu 0.1562 (0.2009)\t\n",
            "Epoch: [10][8/44]\tLoss 2.1592 (2.2021)\tAccu 0.3750 (0.2227)\t\n",
            "Epoch: [10][9/44]\tLoss 2.2031 (2.2022)\tAccu 0.1250 (0.2118)\t\n",
            "Epoch: [10][10/44]\tLoss 2.1846 (2.2005)\tAccu 0.2812 (0.2188)\t\n",
            "Epoch: [10][11/44]\tLoss 2.1796 (2.1986)\tAccu 0.2500 (0.2216)\t\n",
            "Epoch: [10][12/44]\tLoss 2.1679 (2.1960)\tAccu 0.2500 (0.2240)\t\n",
            "Epoch: [10][13/44]\tLoss 2.2311 (2.1987)\tAccu 0.1875 (0.2212)\t\n",
            "Epoch: [10][14/44]\tLoss 2.2276 (2.2008)\tAccu 0.2188 (0.2210)\t\n",
            "Epoch: [10][15/44]\tLoss 2.2123 (2.2015)\tAccu 0.1875 (0.2188)\t\n",
            "Epoch: [10][16/44]\tLoss 2.2072 (2.2019)\tAccu 0.2500 (0.2207)\t\n",
            "Epoch: [10][17/44]\tLoss 2.2501 (2.2047)\tAccu 0.2188 (0.2206)\t\n",
            "Epoch: [10][18/44]\tLoss 2.2290 (2.2061)\tAccu 0.2500 (0.2222)\t\n",
            "Epoch: [10][19/44]\tLoss 2.1758 (2.2045)\tAccu 0.2812 (0.2253)\t\n",
            "Epoch: [10][20/44]\tLoss 2.1889 (2.2037)\tAccu 0.2812 (0.2281)\t\n",
            "Epoch: [10][21/44]\tLoss 2.2740 (2.2070)\tAccu 0.1250 (0.2232)\t\n",
            "Epoch: [10][22/44]\tLoss 2.1561 (2.2047)\tAccu 0.2500 (0.2244)\t\n",
            "Epoch: [10][23/44]\tLoss 2.2578 (2.2070)\tAccu 0.1562 (0.2215)\t\n",
            "Epoch: [10][24/44]\tLoss 2.2189 (2.2075)\tAccu 0.2812 (0.2240)\t\n",
            "Epoch: [10][25/44]\tLoss 2.3137 (2.2118)\tAccu 0.1562 (0.2213)\t\n",
            "Epoch: [10][26/44]\tLoss 2.1834 (2.2107)\tAccu 0.2812 (0.2236)\t\n",
            "Epoch: [10][27/44]\tLoss 2.2430 (2.2119)\tAccu 0.2188 (0.2234)\t\n",
            "Epoch: [10][28/44]\tLoss 2.2006 (2.2115)\tAccu 0.1875 (0.2221)\t\n",
            "Epoch: [10][29/44]\tLoss 2.1839 (2.2105)\tAccu 0.2188 (0.2220)\t\n",
            "Epoch: [10][30/44]\tLoss 2.2136 (2.2106)\tAccu 0.1875 (0.2208)\t\n",
            "Epoch: [10][31/44]\tLoss 2.2006 (2.2103)\tAccu 0.0938 (0.2167)\t\n",
            "Epoch: [10][32/44]\tLoss 2.1891 (2.2096)\tAccu 0.2188 (0.2168)\t\n",
            "Epoch: [10][33/44]\tLoss 2.2049 (2.2095)\tAccu 0.2188 (0.2169)\t\n",
            "Epoch: [10][34/44]\tLoss 2.2056 (2.2094)\tAccu 0.1562 (0.2151)\t\n",
            "Epoch: [10][35/44]\tLoss 2.2270 (2.2099)\tAccu 0.1250 (0.2125)\t\n",
            "Epoch: [10][36/44]\tLoss 2.1882 (2.2093)\tAccu 0.2812 (0.2144)\t\n",
            "Epoch: [10][37/44]\tLoss 2.2150 (2.2094)\tAccu 0.2812 (0.2162)\t\n",
            "Epoch: [10][38/44]\tLoss 2.2155 (2.2096)\tAccu 0.1562 (0.2146)\t\n",
            "Epoch: [10][39/44]\tLoss 2.2426 (2.2104)\tAccu 0.1875 (0.2139)\t\n",
            "Epoch: [10][40/44]\tLoss 2.2366 (2.2111)\tAccu 0.1875 (0.2133)\t\n",
            "Epoch: [10][41/44]\tLoss 2.2410 (2.2118)\tAccu 0.1562 (0.2119)\t\n",
            "Epoch: [10][42/44]\tLoss 2.2266 (2.2122)\tAccu 0.1875 (0.2113)\t\n",
            "Epoch: [10][43/44]\tLoss 2.2289 (2.2126)\tAccu 0.1250 (0.2093)\t\n",
            "Epoch: [10][44/44]\tLoss 2.0677 (2.2101)\tAccu 0.2083 (0.2093)\t\n",
            "Accu 0.1500\t\n",
            "Epoch: [11][1/44]\tLoss 2.2124 (2.2124)\tAccu 0.2188 (0.2188)\t\n",
            "Epoch: [11][2/44]\tLoss 2.1664 (2.1894)\tAccu 0.2500 (0.2344)\t\n",
            "Epoch: [11][3/44]\tLoss 2.1883 (2.1891)\tAccu 0.1250 (0.1979)\t\n",
            "Epoch: [11][4/44]\tLoss 2.1835 (2.1877)\tAccu 0.2500 (0.2109)\t\n",
            "Epoch: [11][5/44]\tLoss 2.2352 (2.1972)\tAccu 0.2500 (0.2188)\t\n",
            "Epoch: [11][6/44]\tLoss 2.1810 (2.1945)\tAccu 0.2812 (0.2292)\t\n",
            "Epoch: [11][7/44]\tLoss 2.2100 (2.1967)\tAccu 0.1250 (0.2143)\t\n",
            "Epoch: [11][8/44]\tLoss 2.1428 (2.1900)\tAccu 0.3750 (0.2344)\t\n",
            "Epoch: [11][9/44]\tLoss 2.1905 (2.1900)\tAccu 0.1250 (0.2222)\t\n",
            "Epoch: [11][10/44]\tLoss 2.1777 (2.1888)\tAccu 0.3125 (0.2313)\t\n",
            "Epoch: [11][11/44]\tLoss 2.1597 (2.1861)\tAccu 0.2500 (0.2330)\t\n",
            "Epoch: [11][12/44]\tLoss 2.1483 (2.1830)\tAccu 0.2500 (0.2344)\t\n",
            "Epoch: [11][13/44]\tLoss 2.2255 (2.1863)\tAccu 0.1562 (0.2284)\t\n",
            "Epoch: [11][14/44]\tLoss 2.2168 (2.1884)\tAccu 0.2500 (0.2299)\t\n",
            "Epoch: [11][15/44]\tLoss 2.2030 (2.1894)\tAccu 0.2188 (0.2292)\t\n",
            "Epoch: [11][16/44]\tLoss 2.1976 (2.1899)\tAccu 0.2812 (0.2324)\t\n",
            "Epoch: [11][17/44]\tLoss 2.2454 (2.1932)\tAccu 0.2188 (0.2316)\t\n",
            "Epoch: [11][18/44]\tLoss 2.2187 (2.1946)\tAccu 0.2812 (0.2344)\t\n",
            "Epoch: [11][19/44]\tLoss 2.1625 (2.1929)\tAccu 0.2812 (0.2368)\t\n",
            "Epoch: [11][20/44]\tLoss 2.1731 (2.1919)\tAccu 0.2500 (0.2375)\t\n",
            "Epoch: [11][21/44]\tLoss 2.2683 (2.1956)\tAccu 0.1562 (0.2336)\t\n",
            "Epoch: [11][22/44]\tLoss 2.1373 (2.1929)\tAccu 0.2812 (0.2358)\t\n",
            "Epoch: [11][23/44]\tLoss 2.2506 (2.1954)\tAccu 0.1875 (0.2337)\t\n",
            "Epoch: [11][24/44]\tLoss 2.2107 (2.1961)\tAccu 0.2812 (0.2357)\t\n",
            "Epoch: [11][25/44]\tLoss 2.3089 (2.2006)\tAccu 0.1562 (0.2325)\t\n",
            "Epoch: [11][26/44]\tLoss 2.1685 (2.1993)\tAccu 0.2812 (0.2344)\t\n",
            "Epoch: [11][27/44]\tLoss 2.2384 (2.2008)\tAccu 0.1875 (0.2326)\t\n",
            "Epoch: [11][28/44]\tLoss 2.1860 (2.2003)\tAccu 0.1875 (0.2310)\t\n",
            "Epoch: [11][29/44]\tLoss 2.1733 (2.1993)\tAccu 0.2188 (0.2306)\t\n",
            "Epoch: [11][30/44]\tLoss 2.2020 (2.1994)\tAccu 0.1875 (0.2292)\t\n",
            "Epoch: [11][31/44]\tLoss 2.1903 (2.1991)\tAccu 0.1250 (0.2258)\t\n",
            "Epoch: [11][32/44]\tLoss 2.1729 (2.1983)\tAccu 0.2188 (0.2256)\t\n",
            "Epoch: [11][33/44]\tLoss 2.1970 (2.1983)\tAccu 0.2188 (0.2254)\t\n",
            "Epoch: [11][34/44]\tLoss 2.1975 (2.1982)\tAccu 0.1562 (0.2233)\t\n",
            "Epoch: [11][35/44]\tLoss 2.2185 (2.1988)\tAccu 0.1250 (0.2205)\t\n",
            "Epoch: [11][36/44]\tLoss 2.1759 (2.1982)\tAccu 0.2812 (0.2222)\t\n",
            "Epoch: [11][37/44]\tLoss 2.2060 (2.1984)\tAccu 0.2812 (0.2238)\t\n",
            "Epoch: [11][38/44]\tLoss 2.2062 (2.1986)\tAccu 0.1562 (0.2220)\t\n",
            "Epoch: [11][39/44]\tLoss 2.2370 (2.1996)\tAccu 0.2188 (0.2220)\t\n",
            "Epoch: [11][40/44]\tLoss 2.2304 (2.2004)\tAccu 0.1562 (0.2203)\t\n",
            "Epoch: [11][41/44]\tLoss 2.2310 (2.2011)\tAccu 0.1562 (0.2188)\t\n",
            "Epoch: [11][42/44]\tLoss 2.2152 (2.2014)\tAccu 0.1875 (0.2180)\t\n",
            "Epoch: [11][43/44]\tLoss 2.2152 (2.2018)\tAccu 0.1562 (0.2166)\t\n",
            "Epoch: [11][44/44]\tLoss 2.0458 (2.1991)\tAccu 0.2917 (0.2183)\t\n",
            "Accu 0.1500\t\n",
            "Epoch: [12][1/44]\tLoss 2.2027 (2.2027)\tAccu 0.2500 (0.2500)\t\n",
            "Epoch: [12][2/44]\tLoss 2.1552 (2.1790)\tAccu 0.2500 (0.2500)\t\n",
            "Epoch: [12][3/44]\tLoss 2.1768 (2.1782)\tAccu 0.1250 (0.2083)\t\n",
            "Epoch: [12][4/44]\tLoss 2.1678 (2.1756)\tAccu 0.2188 (0.2109)\t\n",
            "Epoch: [12][5/44]\tLoss 2.2237 (2.1852)\tAccu 0.2500 (0.2188)\t\n",
            "Epoch: [12][6/44]\tLoss 2.1669 (2.1822)\tAccu 0.2812 (0.2292)\t\n",
            "Epoch: [12][7/44]\tLoss 2.2024 (2.1851)\tAccu 0.1562 (0.2188)\t\n",
            "Epoch: [12][8/44]\tLoss 2.1281 (2.1780)\tAccu 0.3750 (0.2383)\t\n",
            "Epoch: [12][9/44]\tLoss 2.1770 (2.1779)\tAccu 0.1250 (0.2257)\t\n",
            "Epoch: [12][10/44]\tLoss 2.1700 (2.1771)\tAccu 0.3125 (0.2344)\t\n",
            "Epoch: [12][11/44]\tLoss 2.1412 (2.1738)\tAccu 0.3125 (0.2415)\t\n",
            "Epoch: [12][12/44]\tLoss 2.1289 (2.1701)\tAccu 0.2500 (0.2422)\t\n",
            "Epoch: [12][13/44]\tLoss 2.2199 (2.1739)\tAccu 0.1562 (0.2356)\t\n",
            "Epoch: [12][14/44]\tLoss 2.2092 (2.1764)\tAccu 0.2500 (0.2366)\t\n",
            "Epoch: [12][15/44]\tLoss 2.1937 (2.1776)\tAccu 0.2188 (0.2354)\t\n",
            "Epoch: [12][16/44]\tLoss 2.1872 (2.1782)\tAccu 0.2812 (0.2383)\t\n",
            "Epoch: [12][17/44]\tLoss 2.2398 (2.1818)\tAccu 0.2188 (0.2371)\t\n",
            "Epoch: [12][18/44]\tLoss 2.2093 (2.1833)\tAccu 0.2812 (0.2396)\t\n",
            "Epoch: [12][19/44]\tLoss 2.1490 (2.1815)\tAccu 0.2812 (0.2418)\t\n",
            "Epoch: [12][20/44]\tLoss 2.1567 (2.1803)\tAccu 0.2500 (0.2422)\t\n",
            "Epoch: [12][21/44]\tLoss 2.2628 (2.1842)\tAccu 0.1562 (0.2381)\t\n",
            "Epoch: [12][22/44]\tLoss 2.1204 (2.1813)\tAccu 0.2812 (0.2401)\t\n",
            "Epoch: [12][23/44]\tLoss 2.2408 (2.1839)\tAccu 0.1875 (0.2378)\t\n",
            "Epoch: [12][24/44]\tLoss 2.2033 (2.1847)\tAccu 0.2500 (0.2383)\t\n",
            "Epoch: [12][25/44]\tLoss 2.3036 (2.1895)\tAccu 0.1562 (0.2350)\t\n",
            "Epoch: [12][26/44]\tLoss 2.1542 (2.1881)\tAccu 0.2812 (0.2368)\t\n",
            "Epoch: [12][27/44]\tLoss 2.2335 (2.1898)\tAccu 0.1562 (0.2338)\t\n",
            "Epoch: [12][28/44]\tLoss 2.1720 (2.1892)\tAccu 0.2500 (0.2344)\t\n",
            "Epoch: [12][29/44]\tLoss 2.1626 (2.1882)\tAccu 0.2188 (0.2338)\t\n",
            "Epoch: [12][30/44]\tLoss 2.1906 (2.1883)\tAccu 0.2188 (0.2333)\t\n",
            "Epoch: [12][31/44]\tLoss 2.1796 (2.1880)\tAccu 0.1250 (0.2298)\t\n",
            "Epoch: [12][32/44]\tLoss 2.1583 (2.1871)\tAccu 0.2500 (0.2305)\t\n",
            "Epoch: [12][33/44]\tLoss 2.1887 (2.1872)\tAccu 0.2188 (0.2301)\t\n",
            "Epoch: [12][34/44]\tLoss 2.1877 (2.1872)\tAccu 0.1562 (0.2279)\t\n",
            "Epoch: [12][35/44]\tLoss 2.2100 (2.1878)\tAccu 0.0938 (0.2241)\t\n",
            "Epoch: [12][36/44]\tLoss 2.1629 (2.1871)\tAccu 0.2812 (0.2257)\t\n",
            "Epoch: [12][37/44]\tLoss 2.1979 (2.1874)\tAccu 0.2812 (0.2272)\t\n",
            "Epoch: [12][38/44]\tLoss 2.1971 (2.1877)\tAccu 0.1875 (0.2262)\t\n",
            "Epoch: [12][39/44]\tLoss 2.2290 (2.1887)\tAccu 0.1875 (0.2252)\t\n",
            "Epoch: [12][40/44]\tLoss 2.2229 (2.1896)\tAccu 0.1562 (0.2234)\t\n",
            "Epoch: [12][41/44]\tLoss 2.2194 (2.1903)\tAccu 0.1562 (0.2218)\t\n",
            "Epoch: [12][42/44]\tLoss 2.2042 (2.1907)\tAccu 0.1875 (0.2210)\t\n",
            "Epoch: [12][43/44]\tLoss 2.2000 (2.1909)\tAccu 0.1875 (0.2202)\t\n",
            "Epoch: [12][44/44]\tLoss 2.0242 (2.1880)\tAccu 0.2917 (0.2218)\t\n",
            "Accu 0.1550\t\n",
            "Epoch: [13][1/44]\tLoss 2.1919 (2.1919)\tAccu 0.2188 (0.2188)\t\n",
            "Epoch: [13][2/44]\tLoss 2.1431 (2.1675)\tAccu 0.2500 (0.2344)\t\n",
            "Epoch: [13][3/44]\tLoss 2.1672 (2.1674)\tAccu 0.1250 (0.1979)\t\n",
            "Epoch: [13][4/44]\tLoss 2.1523 (2.1636)\tAccu 0.2500 (0.2109)\t\n",
            "Epoch: [13][5/44]\tLoss 2.2134 (2.1736)\tAccu 0.2188 (0.2125)\t\n",
            "Epoch: [13][6/44]\tLoss 2.1538 (2.1703)\tAccu 0.2812 (0.2240)\t\n",
            "Epoch: [13][7/44]\tLoss 2.1959 (2.1739)\tAccu 0.1562 (0.2143)\t\n",
            "Epoch: [13][8/44]\tLoss 2.1146 (2.1665)\tAccu 0.3438 (0.2305)\t\n",
            "Epoch: [13][9/44]\tLoss 2.1678 (2.1667)\tAccu 0.1250 (0.2188)\t\n",
            "Epoch: [13][10/44]\tLoss 2.1635 (2.1663)\tAccu 0.3125 (0.2281)\t\n",
            "Epoch: [13][11/44]\tLoss 2.1243 (2.1625)\tAccu 0.3438 (0.2386)\t\n",
            "Epoch: [13][12/44]\tLoss 2.1102 (2.1582)\tAccu 0.2500 (0.2396)\t\n",
            "Epoch: [13][13/44]\tLoss 2.2138 (2.1624)\tAccu 0.1562 (0.2332)\t\n",
            "Epoch: [13][14/44]\tLoss 2.2011 (2.1652)\tAccu 0.2500 (0.2344)\t\n",
            "Epoch: [13][15/44]\tLoss 2.1861 (2.1666)\tAccu 0.2188 (0.2333)\t\n",
            "Epoch: [13][16/44]\tLoss 2.1766 (2.1672)\tAccu 0.2812 (0.2363)\t\n",
            "Epoch: [13][17/44]\tLoss 2.2335 (2.1711)\tAccu 0.2188 (0.2353)\t\n",
            "Epoch: [13][18/44]\tLoss 2.1988 (2.1727)\tAccu 0.3125 (0.2396)\t\n",
            "Epoch: [13][19/44]\tLoss 2.1370 (2.1708)\tAccu 0.2812 (0.2418)\t\n",
            "Epoch: [13][20/44]\tLoss 2.1408 (2.1693)\tAccu 0.2500 (0.2422)\t\n",
            "Epoch: [13][21/44]\tLoss 2.2565 (2.1734)\tAccu 0.1875 (0.2396)\t\n",
            "Epoch: [13][22/44]\tLoss 2.1018 (2.1702)\tAccu 0.3438 (0.2443)\t\n",
            "Epoch: [13][23/44]\tLoss 2.2322 (2.1729)\tAccu 0.1875 (0.2418)\t\n",
            "Epoch: [13][24/44]\tLoss 2.1954 (2.1738)\tAccu 0.2500 (0.2422)\t\n",
            "Epoch: [13][25/44]\tLoss 2.2971 (2.1787)\tAccu 0.1562 (0.2387)\t\n",
            "Epoch: [13][26/44]\tLoss 2.1408 (2.1773)\tAccu 0.2812 (0.2404)\t\n",
            "Epoch: [13][27/44]\tLoss 2.2277 (2.1791)\tAccu 0.1562 (0.2373)\t\n",
            "Epoch: [13][28/44]\tLoss 2.1562 (2.1783)\tAccu 0.2812 (0.2388)\t\n",
            "Epoch: [13][29/44]\tLoss 2.1549 (2.1775)\tAccu 0.2188 (0.2381)\t\n",
            "Epoch: [13][30/44]\tLoss 2.1777 (2.1775)\tAccu 0.2188 (0.2375)\t\n",
            "Epoch: [13][31/44]\tLoss 2.1687 (2.1772)\tAccu 0.1250 (0.2339)\t\n",
            "Epoch: [13][32/44]\tLoss 2.1428 (2.1762)\tAccu 0.2500 (0.2344)\t\n",
            "Epoch: [13][33/44]\tLoss 2.1789 (2.1762)\tAccu 0.2188 (0.2339)\t\n",
            "Epoch: [13][34/44]\tLoss 2.1765 (2.1763)\tAccu 0.1875 (0.2325)\t\n",
            "Epoch: [13][35/44]\tLoss 2.2045 (2.1771)\tAccu 0.0938 (0.2286)\t\n",
            "Epoch: [13][36/44]\tLoss 2.1503 (2.1763)\tAccu 0.3125 (0.2309)\t\n",
            "Epoch: [13][37/44]\tLoss 2.1895 (2.1767)\tAccu 0.2500 (0.2314)\t\n",
            "Epoch: [13][38/44]\tLoss 2.1895 (2.1770)\tAccu 0.1875 (0.2303)\t\n",
            "Epoch: [13][39/44]\tLoss 2.2220 (2.1782)\tAccu 0.2188 (0.2300)\t\n",
            "Epoch: [13][40/44]\tLoss 2.2167 (2.1791)\tAccu 0.1875 (0.2289)\t\n",
            "Epoch: [13][41/44]\tLoss 2.2083 (2.1798)\tAccu 0.1875 (0.2279)\t\n",
            "Epoch: [13][42/44]\tLoss 2.1910 (2.1801)\tAccu 0.1875 (0.2269)\t\n",
            "Epoch: [13][43/44]\tLoss 2.1859 (2.1802)\tAccu 0.2188 (0.2267)\t\n",
            "Epoch: [13][44/44]\tLoss 2.0030 (2.1772)\tAccu 0.3333 (0.2292)\t\n",
            "Accu 0.1650\t\n",
            "Epoch: [14][1/44]\tLoss 2.1816 (2.1816)\tAccu 0.2188 (0.2188)\t\n",
            "Epoch: [14][2/44]\tLoss 2.1323 (2.1570)\tAccu 0.2812 (0.2500)\t\n",
            "Epoch: [14][3/44]\tLoss 2.1564 (2.1568)\tAccu 0.1250 (0.2083)\t\n",
            "Epoch: [14][4/44]\tLoss 2.1368 (2.1518)\tAccu 0.2500 (0.2188)\t\n",
            "Epoch: [14][5/44]\tLoss 2.2013 (2.1617)\tAccu 0.2188 (0.2188)\t\n",
            "Epoch: [14][6/44]\tLoss 2.1409 (2.1582)\tAccu 0.2812 (0.2292)\t\n",
            "Epoch: [14][7/44]\tLoss 2.1872 (2.1623)\tAccu 0.1875 (0.2232)\t\n",
            "Epoch: [14][8/44]\tLoss 2.1020 (2.1548)\tAccu 0.3438 (0.2383)\t\n",
            "Epoch: [14][9/44]\tLoss 2.1560 (2.1549)\tAccu 0.1250 (0.2257)\t\n",
            "Epoch: [14][10/44]\tLoss 2.1549 (2.1549)\tAccu 0.3125 (0.2344)\t\n",
            "Epoch: [14][11/44]\tLoss 2.1053 (2.1504)\tAccu 0.3438 (0.2443)\t\n",
            "Epoch: [14][12/44]\tLoss 2.0926 (2.1456)\tAccu 0.2500 (0.2448)\t\n",
            "Epoch: [14][13/44]\tLoss 2.2070 (2.1503)\tAccu 0.1562 (0.2380)\t\n",
            "Epoch: [14][14/44]\tLoss 2.1930 (2.1534)\tAccu 0.2500 (0.2388)\t\n",
            "Epoch: [14][15/44]\tLoss 2.1778 (2.1550)\tAccu 0.2188 (0.2375)\t\n",
            "Epoch: [14][16/44]\tLoss 2.1664 (2.1557)\tAccu 0.2812 (0.2402)\t\n",
            "Epoch: [14][17/44]\tLoss 2.2263 (2.1599)\tAccu 0.2188 (0.2390)\t\n",
            "Epoch: [14][18/44]\tLoss 2.1884 (2.1615)\tAccu 0.3438 (0.2448)\t\n",
            "Epoch: [14][19/44]\tLoss 2.1239 (2.1595)\tAccu 0.2812 (0.2467)\t\n",
            "Epoch: [14][20/44]\tLoss 2.1258 (2.1578)\tAccu 0.2500 (0.2469)\t\n",
            "Epoch: [14][21/44]\tLoss 2.2504 (2.1622)\tAccu 0.1562 (0.2426)\t\n",
            "Epoch: [14][22/44]\tLoss 2.0834 (2.1586)\tAccu 0.3438 (0.2472)\t\n",
            "Epoch: [14][23/44]\tLoss 2.2218 (2.1614)\tAccu 0.1250 (0.2418)\t\n",
            "Epoch: [14][24/44]\tLoss 2.1873 (2.1624)\tAccu 0.2188 (0.2409)\t\n",
            "Epoch: [14][25/44]\tLoss 2.2914 (2.1676)\tAccu 0.1562 (0.2375)\t\n",
            "Epoch: [14][26/44]\tLoss 2.1282 (2.1661)\tAccu 0.2812 (0.2392)\t\n",
            "Epoch: [14][27/44]\tLoss 2.2218 (2.1682)\tAccu 0.1562 (0.2361)\t\n",
            "Epoch: [14][28/44]\tLoss 2.1414 (2.1672)\tAccu 0.2500 (0.2366)\t\n",
            "Epoch: [14][29/44]\tLoss 2.1463 (2.1665)\tAccu 0.2188 (0.2360)\t\n",
            "Epoch: [14][30/44]\tLoss 2.1655 (2.1664)\tAccu 0.2188 (0.2354)\t\n",
            "Epoch: [14][31/44]\tLoss 2.1579 (2.1662)\tAccu 0.1250 (0.2319)\t\n",
            "Epoch: [14][32/44]\tLoss 2.1262 (2.1649)\tAccu 0.2500 (0.2324)\t\n",
            "Epoch: [14][33/44]\tLoss 2.1687 (2.1650)\tAccu 0.2500 (0.2330)\t\n",
            "Epoch: [14][34/44]\tLoss 2.1655 (2.1650)\tAccu 0.1875 (0.2316)\t\n",
            "Epoch: [14][35/44]\tLoss 2.1970 (2.1660)\tAccu 0.0938 (0.2277)\t\n",
            "Epoch: [14][36/44]\tLoss 2.1377 (2.1652)\tAccu 0.3125 (0.2300)\t\n",
            "Epoch: [14][37/44]\tLoss 2.1808 (2.1656)\tAccu 0.2500 (0.2306)\t\n",
            "Epoch: [14][38/44]\tLoss 2.1801 (2.1660)\tAccu 0.1875 (0.2294)\t\n",
            "Epoch: [14][39/44]\tLoss 2.2142 (2.1672)\tAccu 0.2500 (0.2300)\t\n",
            "Epoch: [14][40/44]\tLoss 2.2120 (2.1683)\tAccu 0.1875 (0.2289)\t\n",
            "Epoch: [14][41/44]\tLoss 2.1967 (2.1690)\tAccu 0.1875 (0.2279)\t\n",
            "Epoch: [14][42/44]\tLoss 2.1797 (2.1693)\tAccu 0.2500 (0.2284)\t\n",
            "Epoch: [14][43/44]\tLoss 2.1702 (2.1693)\tAccu 0.2500 (0.2289)\t\n",
            "Epoch: [14][44/44]\tLoss 1.9833 (2.1661)\tAccu 0.3750 (0.2322)\t\n",
            "Accu 0.1600\t\n",
            "Epoch: [15][1/44]\tLoss 2.1706 (2.1706)\tAccu 0.2188 (0.2188)\t\n",
            "Epoch: [15][2/44]\tLoss 2.1207 (2.1456)\tAccu 0.2812 (0.2500)\t\n",
            "Epoch: [15][3/44]\tLoss 2.1471 (2.1461)\tAccu 0.1562 (0.2188)\t\n",
            "Epoch: [15][4/44]\tLoss 2.1207 (2.1398)\tAccu 0.3125 (0.2422)\t\n",
            "Epoch: [15][5/44]\tLoss 2.1918 (2.1502)\tAccu 0.2188 (0.2375)\t\n",
            "Epoch: [15][6/44]\tLoss 2.1284 (2.1466)\tAccu 0.4062 (0.2656)\t\n",
            "Epoch: [15][7/44]\tLoss 2.1817 (2.1516)\tAccu 0.1875 (0.2545)\t\n",
            "Epoch: [15][8/44]\tLoss 2.0918 (2.1441)\tAccu 0.3438 (0.2656)\t\n",
            "Epoch: [15][9/44]\tLoss 2.1457 (2.1443)\tAccu 0.1250 (0.2500)\t\n",
            "Epoch: [15][10/44]\tLoss 2.1480 (2.1447)\tAccu 0.3125 (0.2562)\t\n",
            "Epoch: [15][11/44]\tLoss 2.0886 (2.1396)\tAccu 0.3438 (0.2642)\t\n",
            "Epoch: [15][12/44]\tLoss 2.0723 (2.1340)\tAccu 0.2812 (0.2656)\t\n",
            "Epoch: [15][13/44]\tLoss 2.2019 (2.1392)\tAccu 0.1562 (0.2572)\t\n",
            "Epoch: [15][14/44]\tLoss 2.1849 (2.1425)\tAccu 0.2500 (0.2567)\t\n",
            "Epoch: [15][15/44]\tLoss 2.1711 (2.1444)\tAccu 0.2188 (0.2542)\t\n",
            "Epoch: [15][16/44]\tLoss 2.1548 (2.1450)\tAccu 0.2812 (0.2559)\t\n",
            "Epoch: [15][17/44]\tLoss 2.2179 (2.1493)\tAccu 0.2188 (0.2537)\t\n",
            "Epoch: [15][18/44]\tLoss 2.1781 (2.1509)\tAccu 0.3125 (0.2569)\t\n",
            "Epoch: [15][19/44]\tLoss 2.1101 (2.1488)\tAccu 0.2812 (0.2582)\t\n",
            "Epoch: [15][20/44]\tLoss 2.1105 (2.1468)\tAccu 0.2500 (0.2578)\t\n",
            "Epoch: [15][21/44]\tLoss 2.2444 (2.1515)\tAccu 0.1562 (0.2530)\t\n",
            "Epoch: [15][22/44]\tLoss 2.0637 (2.1475)\tAccu 0.3438 (0.2571)\t\n",
            "Epoch: [15][23/44]\tLoss 2.2137 (2.1504)\tAccu 0.1250 (0.2514)\t\n",
            "Epoch: [15][24/44]\tLoss 2.1796 (2.1516)\tAccu 0.2188 (0.2500)\t\n",
            "Epoch: [15][25/44]\tLoss 2.2862 (2.1570)\tAccu 0.1562 (0.2462)\t\n",
            "Epoch: [15][26/44]\tLoss 2.1170 (2.1554)\tAccu 0.2812 (0.2476)\t\n",
            "Epoch: [15][27/44]\tLoss 2.2183 (2.1578)\tAccu 0.1250 (0.2431)\t\n",
            "Epoch: [15][28/44]\tLoss 2.1259 (2.1566)\tAccu 0.2500 (0.2433)\t\n",
            "Epoch: [15][29/44]\tLoss 2.1391 (2.1560)\tAccu 0.2188 (0.2425)\t\n",
            "Epoch: [15][30/44]\tLoss 2.1525 (2.1559)\tAccu 0.2188 (0.2417)\t\n",
            "Epoch: [15][31/44]\tLoss 2.1478 (2.1556)\tAccu 0.1250 (0.2379)\t\n",
            "Epoch: [15][32/44]\tLoss 2.1090 (2.1542)\tAccu 0.2500 (0.2383)\t\n",
            "Epoch: [15][33/44]\tLoss 2.1560 (2.1542)\tAccu 0.2500 (0.2386)\t\n",
            "Epoch: [15][34/44]\tLoss 2.1536 (2.1542)\tAccu 0.2188 (0.2381)\t\n",
            "Epoch: [15][35/44]\tLoss 2.1888 (2.1552)\tAccu 0.0938 (0.2339)\t\n",
            "Epoch: [15][36/44]\tLoss 2.1244 (2.1544)\tAccu 0.3125 (0.2361)\t\n",
            "Epoch: [15][37/44]\tLoss 2.1717 (2.1548)\tAccu 0.2500 (0.2365)\t\n",
            "Epoch: [15][38/44]\tLoss 2.1725 (2.1553)\tAccu 0.1562 (0.2344)\t\n",
            "Epoch: [15][39/44]\tLoss 2.2056 (2.1566)\tAccu 0.2500 (0.2348)\t\n",
            "Epoch: [15][40/44]\tLoss 2.2058 (2.1578)\tAccu 0.2188 (0.2344)\t\n",
            "Epoch: [15][41/44]\tLoss 2.1849 (2.1585)\tAccu 0.1875 (0.2332)\t\n",
            "Epoch: [15][42/44]\tLoss 2.1670 (2.1587)\tAccu 0.2500 (0.2336)\t\n",
            "Epoch: [15][43/44]\tLoss 2.1560 (2.1586)\tAccu 0.2500 (0.2340)\t\n",
            "Epoch: [15][44/44]\tLoss 1.9619 (2.1552)\tAccu 0.4167 (0.2382)\t\n",
            "Accu 0.1700\t\n",
            "Epoch: [16][1/44]\tLoss 2.1590 (2.1590)\tAccu 0.2500 (0.2500)\t\n",
            "Epoch: [16][2/44]\tLoss 2.1095 (2.1342)\tAccu 0.2812 (0.2656)\t\n",
            "Epoch: [16][3/44]\tLoss 2.1382 (2.1356)\tAccu 0.1875 (0.2396)\t\n",
            "Epoch: [16][4/44]\tLoss 2.1039 (2.1276)\tAccu 0.3438 (0.2656)\t\n",
            "Epoch: [16][5/44]\tLoss 2.1801 (2.1381)\tAccu 0.2188 (0.2562)\t\n",
            "Epoch: [16][6/44]\tLoss 2.1159 (2.1344)\tAccu 0.3750 (0.2760)\t\n",
            "Epoch: [16][7/44]\tLoss 2.1748 (2.1402)\tAccu 0.1875 (0.2634)\t\n",
            "Epoch: [16][8/44]\tLoss 2.0810 (2.1328)\tAccu 0.3438 (0.2734)\t\n",
            "Epoch: [16][9/44]\tLoss 2.1343 (2.1330)\tAccu 0.1875 (0.2639)\t\n",
            "Epoch: [16][10/44]\tLoss 2.1424 (2.1339)\tAccu 0.3125 (0.2687)\t\n",
            "Epoch: [16][11/44]\tLoss 2.0710 (2.1282)\tAccu 0.3750 (0.2784)\t\n",
            "Epoch: [16][12/44]\tLoss 2.0541 (2.1220)\tAccu 0.2812 (0.2786)\t\n",
            "Epoch: [16][13/44]\tLoss 2.1946 (2.1276)\tAccu 0.1875 (0.2716)\t\n",
            "Epoch: [16][14/44]\tLoss 2.1764 (2.1311)\tAccu 0.2500 (0.2701)\t\n",
            "Epoch: [16][15/44]\tLoss 2.1618 (2.1331)\tAccu 0.2188 (0.2667)\t\n",
            "Epoch: [16][16/44]\tLoss 2.1441 (2.1338)\tAccu 0.2812 (0.2676)\t\n",
            "Epoch: [16][17/44]\tLoss 2.2103 (2.1383)\tAccu 0.2188 (0.2647)\t\n",
            "Epoch: [16][18/44]\tLoss 2.1660 (2.1399)\tAccu 0.3125 (0.2674)\t\n",
            "Epoch: [16][19/44]\tLoss 2.0947 (2.1375)\tAccu 0.2812 (0.2681)\t\n",
            "Epoch: [16][20/44]\tLoss 2.0958 (2.1354)\tAccu 0.2500 (0.2672)\t\n",
            "Epoch: [16][21/44]\tLoss 2.2372 (2.1402)\tAccu 0.1875 (0.2634)\t\n",
            "Epoch: [16][22/44]\tLoss 2.0449 (2.1359)\tAccu 0.3438 (0.2670)\t\n",
            "Epoch: [16][23/44]\tLoss 2.2023 (2.1388)\tAccu 0.1250 (0.2609)\t\n",
            "Epoch: [16][24/44]\tLoss 2.1704 (2.1401)\tAccu 0.2500 (0.2604)\t\n",
            "Epoch: [16][25/44]\tLoss 2.2774 (2.1456)\tAccu 0.1562 (0.2562)\t\n",
            "Epoch: [16][26/44]\tLoss 2.1053 (2.1441)\tAccu 0.2812 (0.2572)\t\n",
            "Epoch: [16][27/44]\tLoss 2.2094 (2.1465)\tAccu 0.1562 (0.2535)\t\n",
            "Epoch: [16][28/44]\tLoss 2.1115 (2.1452)\tAccu 0.2500 (0.2533)\t\n",
            "Epoch: [16][29/44]\tLoss 2.1305 (2.1447)\tAccu 0.2500 (0.2532)\t\n",
            "Epoch: [16][30/44]\tLoss 2.1391 (2.1445)\tAccu 0.2500 (0.2531)\t\n",
            "Epoch: [16][31/44]\tLoss 2.1361 (2.1443)\tAccu 0.1250 (0.2490)\t\n",
            "Epoch: [16][32/44]\tLoss 2.0928 (2.1426)\tAccu 0.2500 (0.2490)\t\n",
            "Epoch: [16][33/44]\tLoss 2.1436 (2.1427)\tAccu 0.2812 (0.2500)\t\n",
            "Epoch: [16][34/44]\tLoss 2.1412 (2.1426)\tAccu 0.2500 (0.2500)\t\n",
            "Epoch: [16][35/44]\tLoss 2.1809 (2.1437)\tAccu 0.0938 (0.2455)\t\n",
            "Epoch: [16][36/44]\tLoss 2.1122 (2.1429)\tAccu 0.3750 (0.2491)\t\n",
            "Epoch: [16][37/44]\tLoss 2.1627 (2.1434)\tAccu 0.2500 (0.2492)\t\n",
            "Epoch: [16][38/44]\tLoss 2.1624 (2.1439)\tAccu 0.1875 (0.2475)\t\n",
            "Epoch: [16][39/44]\tLoss 2.1942 (2.1452)\tAccu 0.2500 (0.2476)\t\n",
            "Epoch: [16][40/44]\tLoss 2.1987 (2.1465)\tAccu 0.2188 (0.2469)\t\n",
            "Epoch: [16][41/44]\tLoss 2.1716 (2.1471)\tAccu 0.1875 (0.2454)\t\n",
            "Epoch: [16][42/44]\tLoss 2.1538 (2.1473)\tAccu 0.2500 (0.2455)\t\n",
            "Epoch: [16][43/44]\tLoss 2.1419 (2.1472)\tAccu 0.2188 (0.2449)\t\n",
            "Epoch: [16][44/44]\tLoss 1.9428 (2.1437)\tAccu 0.4167 (0.2488)\t\n",
            "Accu 0.1750\t\n",
            "Epoch: [17][1/44]\tLoss 2.1467 (2.1467)\tAccu 0.2500 (0.2500)\t\n",
            "Epoch: [17][2/44]\tLoss 2.0970 (2.1218)\tAccu 0.2812 (0.2656)\t\n",
            "Epoch: [17][3/44]\tLoss 2.1285 (2.1241)\tAccu 0.1875 (0.2396)\t\n",
            "Epoch: [17][4/44]\tLoss 2.0871 (2.1148)\tAccu 0.3438 (0.2656)\t\n",
            "Epoch: [17][5/44]\tLoss 2.1680 (2.1255)\tAccu 0.2188 (0.2562)\t\n",
            "Epoch: [17][6/44]\tLoss 2.1025 (2.1216)\tAccu 0.4062 (0.2812)\t\n",
            "Epoch: [17][7/44]\tLoss 2.1669 (2.1281)\tAccu 0.1875 (0.2679)\t\n",
            "Epoch: [17][8/44]\tLoss 2.0712 (2.1210)\tAccu 0.3438 (0.2773)\t\n",
            "Epoch: [17][9/44]\tLoss 2.1231 (2.1212)\tAccu 0.2188 (0.2708)\t\n",
            "Epoch: [17][10/44]\tLoss 2.1358 (2.1227)\tAccu 0.2812 (0.2719)\t\n",
            "Epoch: [17][11/44]\tLoss 2.0538 (2.1164)\tAccu 0.4375 (0.2869)\t\n",
            "Epoch: [17][12/44]\tLoss 2.0362 (2.1097)\tAccu 0.3125 (0.2891)\t\n",
            "Epoch: [17][13/44]\tLoss 2.1859 (2.1156)\tAccu 0.1875 (0.2812)\t\n",
            "Epoch: [17][14/44]\tLoss 2.1682 (2.1194)\tAccu 0.2500 (0.2790)\t\n",
            "Epoch: [17][15/44]\tLoss 2.1543 (2.1217)\tAccu 0.2188 (0.2750)\t\n",
            "Epoch: [17][16/44]\tLoss 2.1329 (2.1224)\tAccu 0.2812 (0.2754)\t\n",
            "Epoch: [17][17/44]\tLoss 2.1990 (2.1269)\tAccu 0.1875 (0.2702)\t\n",
            "Epoch: [17][18/44]\tLoss 2.1562 (2.1285)\tAccu 0.3125 (0.2726)\t\n",
            "Epoch: [17][19/44]\tLoss 2.0806 (2.1260)\tAccu 0.2812 (0.2730)\t\n",
            "Epoch: [17][20/44]\tLoss 2.0808 (2.1237)\tAccu 0.2812 (0.2734)\t\n",
            "Epoch: [17][21/44]\tLoss 2.2299 (2.1288)\tAccu 0.2188 (0.2708)\t\n",
            "Epoch: [17][22/44]\tLoss 2.0267 (2.1242)\tAccu 0.3438 (0.2741)\t\n",
            "Epoch: [17][23/44]\tLoss 2.1917 (2.1271)\tAccu 0.1250 (0.2677)\t\n",
            "Epoch: [17][24/44]\tLoss 2.1615 (2.1285)\tAccu 0.2500 (0.2669)\t\n",
            "Epoch: [17][25/44]\tLoss 2.2691 (2.1342)\tAccu 0.1562 (0.2625)\t\n",
            "Epoch: [17][26/44]\tLoss 2.0939 (2.1326)\tAccu 0.3125 (0.2644)\t\n",
            "Epoch: [17][27/44]\tLoss 2.2022 (2.1352)\tAccu 0.1562 (0.2604)\t\n",
            "Epoch: [17][28/44]\tLoss 2.0957 (2.1338)\tAccu 0.2812 (0.2612)\t\n",
            "Epoch: [17][29/44]\tLoss 2.1228 (2.1334)\tAccu 0.2500 (0.2608)\t\n",
            "Epoch: [17][30/44]\tLoss 2.1266 (2.1332)\tAccu 0.2500 (0.2604)\t\n",
            "Epoch: [17][31/44]\tLoss 2.1255 (2.1329)\tAccu 0.1562 (0.2571)\t\n",
            "Epoch: [17][32/44]\tLoss 2.0762 (2.1311)\tAccu 0.3438 (0.2598)\t\n",
            "Epoch: [17][33/44]\tLoss 2.1293 (2.1311)\tAccu 0.2812 (0.2604)\t\n",
            "Epoch: [17][34/44]\tLoss 2.1270 (2.1310)\tAccu 0.2500 (0.2601)\t\n",
            "Epoch: [17][35/44]\tLoss 2.1710 (2.1321)\tAccu 0.1250 (0.2562)\t\n",
            "Epoch: [17][36/44]\tLoss 2.0995 (2.1312)\tAccu 0.3750 (0.2595)\t\n",
            "Epoch: [17][37/44]\tLoss 2.1535 (2.1318)\tAccu 0.1875 (0.2576)\t\n",
            "Epoch: [17][38/44]\tLoss 2.1540 (2.1324)\tAccu 0.1875 (0.2558)\t\n",
            "Epoch: [17][39/44]\tLoss 2.1841 (2.1337)\tAccu 0.2812 (0.2564)\t\n",
            "Epoch: [17][40/44]\tLoss 2.1923 (2.1352)\tAccu 0.1875 (0.2547)\t\n",
            "Epoch: [17][41/44]\tLoss 2.1597 (2.1358)\tAccu 0.1875 (0.2530)\t\n",
            "Epoch: [17][42/44]\tLoss 2.1402 (2.1359)\tAccu 0.2500 (0.2530)\t\n",
            "Epoch: [17][43/44]\tLoss 2.1261 (2.1357)\tAccu 0.2188 (0.2522)\t\n",
            "Epoch: [17][44/44]\tLoss 1.9200 (2.1320)\tAccu 0.4167 (0.2559)\t\n",
            "Accu 0.1900\t\n",
            "Epoch: [18][1/44]\tLoss 2.1357 (2.1357)\tAccu 0.2188 (0.2188)\t\n",
            "Epoch: [18][2/44]\tLoss 2.0853 (2.1105)\tAccu 0.3125 (0.2656)\t\n",
            "Epoch: [18][3/44]\tLoss 2.1211 (2.1140)\tAccu 0.1875 (0.2396)\t\n",
            "Epoch: [18][4/44]\tLoss 2.0692 (2.1028)\tAccu 0.3438 (0.2656)\t\n",
            "Epoch: [18][5/44]\tLoss 2.1570 (2.1137)\tAccu 0.2188 (0.2562)\t\n",
            "Epoch: [18][6/44]\tLoss 2.0894 (2.1096)\tAccu 0.4062 (0.2812)\t\n",
            "Epoch: [18][7/44]\tLoss 2.1605 (2.1169)\tAccu 0.1875 (0.2679)\t\n",
            "Epoch: [18][8/44]\tLoss 2.0633 (2.1102)\tAccu 0.3438 (0.2773)\t\n",
            "Epoch: [18][9/44]\tLoss 2.1105 (2.1102)\tAccu 0.2188 (0.2708)\t\n",
            "Epoch: [18][10/44]\tLoss 2.1297 (2.1122)\tAccu 0.2812 (0.2719)\t\n",
            "Epoch: [18][11/44]\tLoss 2.0367 (2.1053)\tAccu 0.4688 (0.2898)\t\n",
            "Epoch: [18][12/44]\tLoss 2.0195 (2.0982)\tAccu 0.3438 (0.2943)\t\n",
            "Epoch: [18][13/44]\tLoss 2.1757 (2.1041)\tAccu 0.1875 (0.2861)\t\n",
            "Epoch: [18][14/44]\tLoss 2.1598 (2.1081)\tAccu 0.2500 (0.2835)\t\n",
            "Epoch: [18][15/44]\tLoss 2.1435 (2.1105)\tAccu 0.2188 (0.2792)\t\n",
            "Epoch: [18][16/44]\tLoss 2.1220 (2.1112)\tAccu 0.2812 (0.2793)\t\n",
            "Epoch: [18][17/44]\tLoss 2.1885 (2.1157)\tAccu 0.1875 (0.2739)\t\n",
            "Epoch: [18][18/44]\tLoss 2.1438 (2.1173)\tAccu 0.3125 (0.2760)\t\n",
            "Epoch: [18][19/44]\tLoss 2.0648 (2.1145)\tAccu 0.3125 (0.2780)\t\n",
            "Epoch: [18][20/44]\tLoss 2.0654 (2.1121)\tAccu 0.3125 (0.2797)\t\n",
            "Epoch: [18][21/44]\tLoss 2.2220 (2.1173)\tAccu 0.2500 (0.2783)\t\n",
            "Epoch: [18][22/44]\tLoss 2.0079 (2.1123)\tAccu 0.3438 (0.2812)\t\n",
            "Epoch: [18][23/44]\tLoss 2.1793 (2.1152)\tAccu 0.1562 (0.2758)\t\n",
            "Epoch: [18][24/44]\tLoss 2.1529 (2.1168)\tAccu 0.2500 (0.2747)\t\n",
            "Epoch: [18][25/44]\tLoss 2.2619 (2.1226)\tAccu 0.1562 (0.2700)\t\n",
            "Epoch: [18][26/44]\tLoss 2.0830 (2.1211)\tAccu 0.3438 (0.2728)\t\n",
            "Epoch: [18][27/44]\tLoss 2.1948 (2.1238)\tAccu 0.1562 (0.2685)\t\n",
            "Epoch: [18][28/44]\tLoss 2.0788 (2.1222)\tAccu 0.2812 (0.2690)\t\n",
            "Epoch: [18][29/44]\tLoss 2.1135 (2.1219)\tAccu 0.2500 (0.2683)\t\n",
            "Epoch: [18][30/44]\tLoss 2.1127 (2.1216)\tAccu 0.2500 (0.2677)\t\n",
            "Epoch: [18][31/44]\tLoss 2.1145 (2.1214)\tAccu 0.1562 (0.2641)\t\n",
            "Epoch: [18][32/44]\tLoss 2.0571 (2.1194)\tAccu 0.3125 (0.2656)\t\n",
            "Epoch: [18][33/44]\tLoss 2.1163 (2.1193)\tAccu 0.3438 (0.2680)\t\n",
            "Epoch: [18][34/44]\tLoss 2.1142 (2.1191)\tAccu 0.2500 (0.2675)\t\n",
            "Epoch: [18][35/44]\tLoss 2.1631 (2.1204)\tAccu 0.1250 (0.2634)\t\n",
            "Epoch: [18][36/44]\tLoss 2.0872 (2.1195)\tAccu 0.3750 (0.2665)\t\n",
            "Epoch: [18][37/44]\tLoss 2.1437 (2.1201)\tAccu 0.2188 (0.2652)\t\n",
            "Epoch: [18][38/44]\tLoss 2.1436 (2.1207)\tAccu 0.2188 (0.2640)\t\n",
            "Epoch: [18][39/44]\tLoss 2.1732 (2.1221)\tAccu 0.2812 (0.2644)\t\n",
            "Epoch: [18][40/44]\tLoss 2.1876 (2.1237)\tAccu 0.1875 (0.2625)\t\n",
            "Epoch: [18][41/44]\tLoss 2.1454 (2.1242)\tAccu 0.1875 (0.2607)\t\n",
            "Epoch: [18][42/44]\tLoss 2.1275 (2.1243)\tAccu 0.2500 (0.2604)\t\n",
            "Epoch: [18][43/44]\tLoss 2.1124 (2.1240)\tAccu 0.2188 (0.2594)\t\n",
            "Epoch: [18][44/44]\tLoss 1.9006 (2.1202)\tAccu 0.4167 (0.2630)\t\n",
            "Accu 0.2050\t\n",
            "Epoch: [19][1/44]\tLoss 2.1234 (2.1234)\tAccu 0.2188 (0.2188)\t\n",
            "Epoch: [19][2/44]\tLoss 2.0725 (2.0980)\tAccu 0.3125 (0.2656)\t\n",
            "Epoch: [19][3/44]\tLoss 2.1110 (2.1023)\tAccu 0.1875 (0.2396)\t\n",
            "Epoch: [19][4/44]\tLoss 2.0520 (2.0897)\tAccu 0.3438 (0.2656)\t\n",
            "Epoch: [19][5/44]\tLoss 2.1448 (2.1008)\tAccu 0.2812 (0.2687)\t\n",
            "Epoch: [19][6/44]\tLoss 2.0757 (2.0966)\tAccu 0.3750 (0.2865)\t\n",
            "Epoch: [19][7/44]\tLoss 2.1537 (2.1047)\tAccu 0.1875 (0.2723)\t\n",
            "Epoch: [19][8/44]\tLoss 2.0554 (2.0986)\tAccu 0.3438 (0.2812)\t\n",
            "Epoch: [19][9/44]\tLoss 2.0977 (2.0985)\tAccu 0.2188 (0.2743)\t\n",
            "Epoch: [19][10/44]\tLoss 2.1229 (2.1009)\tAccu 0.2812 (0.2750)\t\n",
            "Epoch: [19][11/44]\tLoss 2.0192 (2.0935)\tAccu 0.4375 (0.2898)\t\n",
            "Epoch: [19][12/44]\tLoss 2.0016 (2.0858)\tAccu 0.3750 (0.2969)\t\n",
            "Epoch: [19][13/44]\tLoss 2.1669 (2.0921)\tAccu 0.1875 (0.2885)\t\n",
            "Epoch: [19][14/44]\tLoss 2.1506 (2.0962)\tAccu 0.2500 (0.2857)\t\n",
            "Epoch: [19][15/44]\tLoss 2.1331 (2.0987)\tAccu 0.2188 (0.2812)\t\n",
            "Epoch: [19][16/44]\tLoss 2.1094 (2.0994)\tAccu 0.3125 (0.2832)\t\n",
            "Epoch: [19][17/44]\tLoss 2.1773 (2.1040)\tAccu 0.2188 (0.2794)\t\n",
            "Epoch: [19][18/44]\tLoss 2.1324 (2.1055)\tAccu 0.3125 (0.2812)\t\n",
            "Epoch: [19][19/44]\tLoss 2.0495 (2.1026)\tAccu 0.3125 (0.2829)\t\n",
            "Epoch: [19][20/44]\tLoss 2.0490 (2.0999)\tAccu 0.3438 (0.2859)\t\n",
            "Epoch: [19][21/44]\tLoss 2.2140 (2.1053)\tAccu 0.2500 (0.2842)\t\n",
            "Epoch: [19][22/44]\tLoss 1.9890 (2.1000)\tAccu 0.3750 (0.2884)\t\n",
            "Epoch: [19][23/44]\tLoss 2.1654 (2.1029)\tAccu 0.1562 (0.2826)\t\n",
            "Epoch: [19][24/44]\tLoss 2.1415 (2.1045)\tAccu 0.2500 (0.2812)\t\n",
            "Epoch: [19][25/44]\tLoss 2.2543 (2.1105)\tAccu 0.1562 (0.2762)\t\n",
            "Epoch: [19][26/44]\tLoss 2.0714 (2.1090)\tAccu 0.3125 (0.2776)\t\n",
            "Epoch: [19][27/44]\tLoss 2.1864 (2.1119)\tAccu 0.1562 (0.2731)\t\n",
            "Epoch: [19][28/44]\tLoss 2.0624 (2.1101)\tAccu 0.3125 (0.2746)\t\n",
            "Epoch: [19][29/44]\tLoss 2.1039 (2.1099)\tAccu 0.2500 (0.2737)\t\n",
            "Epoch: [19][30/44]\tLoss 2.0985 (2.1095)\tAccu 0.2812 (0.2740)\t\n",
            "Epoch: [19][31/44]\tLoss 2.1039 (2.1093)\tAccu 0.1562 (0.2702)\t\n",
            "Epoch: [19][32/44]\tLoss 2.0384 (2.1071)\tAccu 0.3125 (0.2715)\t\n",
            "Epoch: [19][33/44]\tLoss 2.0986 (2.1068)\tAccu 0.3438 (0.2737)\t\n",
            "Epoch: [19][34/44]\tLoss 2.0972 (2.1066)\tAccu 0.2500 (0.2730)\t\n",
            "Epoch: [19][35/44]\tLoss 2.1540 (2.1079)\tAccu 0.1250 (0.2687)\t\n",
            "Epoch: [19][36/44]\tLoss 2.0742 (2.1070)\tAccu 0.4062 (0.2726)\t\n",
            "Epoch: [19][37/44]\tLoss 2.1340 (2.1077)\tAccu 0.2188 (0.2711)\t\n",
            "Epoch: [19][38/44]\tLoss 2.1344 (2.1084)\tAccu 0.2188 (0.2697)\t\n",
            "Epoch: [19][39/44]\tLoss 2.1594 (2.1097)\tAccu 0.2812 (0.2700)\t\n",
            "Epoch: [19][40/44]\tLoss 2.1802 (2.1115)\tAccu 0.1875 (0.2680)\t\n",
            "Epoch: [19][41/44]\tLoss 2.1329 (2.1120)\tAccu 0.1875 (0.2660)\t\n",
            "Epoch: [19][42/44]\tLoss 2.1135 (2.1120)\tAccu 0.2500 (0.2656)\t\n",
            "Epoch: [19][43/44]\tLoss 2.0989 (2.1117)\tAccu 0.2188 (0.2645)\t\n",
            "Epoch: [19][44/44]\tLoss 1.8806 (2.1078)\tAccu 0.4583 (0.2689)\t\n",
            "Accu 0.2100\t\n",
            "Epoch: [20][1/44]\tLoss 2.1101 (2.1101)\tAccu 0.2500 (0.2500)\t\n",
            "Epoch: [20][2/44]\tLoss 2.0603 (2.0852)\tAccu 0.3438 (0.2969)\t\n",
            "Epoch: [20][3/44]\tLoss 2.0989 (2.0898)\tAccu 0.1875 (0.2604)\t\n",
            "Epoch: [20][4/44]\tLoss 2.0332 (2.0756)\tAccu 0.3438 (0.2812)\t\n",
            "Epoch: [20][5/44]\tLoss 2.1330 (2.0871)\tAccu 0.2812 (0.2812)\t\n",
            "Epoch: [20][6/44]\tLoss 2.0613 (2.0828)\tAccu 0.4062 (0.3021)\t\n",
            "Epoch: [20][7/44]\tLoss 2.1484 (2.0922)\tAccu 0.1875 (0.2857)\t\n",
            "Epoch: [20][8/44]\tLoss 2.0492 (2.0868)\tAccu 0.3438 (0.2930)\t\n",
            "Epoch: [20][9/44]\tLoss 2.0845 (2.0865)\tAccu 0.2500 (0.2882)\t\n",
            "Epoch: [20][10/44]\tLoss 2.1158 (2.0895)\tAccu 0.2812 (0.2875)\t\n",
            "Epoch: [20][11/44]\tLoss 2.0026 (2.0816)\tAccu 0.4375 (0.3011)\t\n",
            "Epoch: [20][12/44]\tLoss 1.9821 (2.0733)\tAccu 0.3750 (0.3073)\t\n",
            "Epoch: [20][13/44]\tLoss 2.1570 (2.0797)\tAccu 0.1875 (0.2981)\t\n",
            "Epoch: [20][14/44]\tLoss 2.1409 (2.0841)\tAccu 0.2500 (0.2946)\t\n",
            "Epoch: [20][15/44]\tLoss 2.1214 (2.0866)\tAccu 0.2188 (0.2896)\t\n",
            "Epoch: [20][16/44]\tLoss 2.0973 (2.0872)\tAccu 0.3125 (0.2910)\t\n",
            "Epoch: [20][17/44]\tLoss 2.1651 (2.0918)\tAccu 0.2188 (0.2868)\t\n",
            "Epoch: [20][18/44]\tLoss 2.1207 (2.0934)\tAccu 0.3125 (0.2882)\t\n",
            "Epoch: [20][19/44]\tLoss 2.0336 (2.0903)\tAccu 0.3438 (0.2911)\t\n",
            "Epoch: [20][20/44]\tLoss 2.0340 (2.0875)\tAccu 0.3750 (0.2953)\t\n",
            "Epoch: [20][21/44]\tLoss 2.2056 (2.0931)\tAccu 0.2500 (0.2932)\t\n",
            "Epoch: [20][22/44]\tLoss 1.9687 (2.0874)\tAccu 0.3750 (0.2969)\t\n",
            "Epoch: [20][23/44]\tLoss 2.1515 (2.0902)\tAccu 0.1562 (0.2908)\t\n",
            "Epoch: [20][24/44]\tLoss 2.1307 (2.0919)\tAccu 0.2188 (0.2878)\t\n",
            "Epoch: [20][25/44]\tLoss 2.2453 (2.0980)\tAccu 0.1562 (0.2825)\t\n",
            "Epoch: [20][26/44]\tLoss 2.0575 (2.0965)\tAccu 0.3125 (0.2837)\t\n",
            "Epoch: [20][27/44]\tLoss 2.1775 (2.0995)\tAccu 0.1562 (0.2789)\t\n",
            "Epoch: [20][28/44]\tLoss 2.0434 (2.0975)\tAccu 0.3125 (0.2801)\t\n",
            "Epoch: [20][29/44]\tLoss 2.0934 (2.0973)\tAccu 0.2500 (0.2791)\t\n",
            "Epoch: [20][30/44]\tLoss 2.0833 (2.0969)\tAccu 0.2812 (0.2792)\t\n",
            "Epoch: [20][31/44]\tLoss 2.0928 (2.0967)\tAccu 0.2188 (0.2772)\t\n",
            "Epoch: [20][32/44]\tLoss 2.0186 (2.0943)\tAccu 0.3125 (0.2783)\t\n",
            "Epoch: [20][33/44]\tLoss 2.0830 (2.0940)\tAccu 0.3125 (0.2794)\t\n",
            "Epoch: [20][34/44]\tLoss 2.0825 (2.0936)\tAccu 0.2812 (0.2794)\t\n",
            "Epoch: [20][35/44]\tLoss 2.1456 (2.0951)\tAccu 0.1250 (0.2750)\t\n",
            "Epoch: [20][36/44]\tLoss 2.0609 (2.0942)\tAccu 0.4062 (0.2786)\t\n",
            "Epoch: [20][37/44]\tLoss 2.1232 (2.0949)\tAccu 0.2188 (0.2770)\t\n",
            "Epoch: [20][38/44]\tLoss 2.1230 (2.0957)\tAccu 0.2188 (0.2755)\t\n",
            "Epoch: [20][39/44]\tLoss 2.1466 (2.0970)\tAccu 0.2812 (0.2756)\t\n",
            "Epoch: [20][40/44]\tLoss 2.1734 (2.0989)\tAccu 0.2188 (0.2742)\t\n",
            "Epoch: [20][41/44]\tLoss 2.1199 (2.0994)\tAccu 0.1875 (0.2721)\t\n",
            "Epoch: [20][42/44]\tLoss 2.1013 (2.0995)\tAccu 0.2500 (0.2716)\t\n",
            "Epoch: [20][43/44]\tLoss 2.0821 (2.0990)\tAccu 0.2188 (0.2703)\t\n",
            "Epoch: [20][44/44]\tLoss 1.8610 (2.0950)\tAccu 0.4583 (0.2746)\t\n",
            "Accu 0.2000\t\n",
            "Best Acc: 0.21\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWJZLiIiMmkj",
        "colab_type": "text"
      },
      "source": [
        "#### Plotting Your Results [3 pts]\n",
        "\n",
        "You need to provide two distinct plots, one demonstrating training loss and training accuracy in y axis and iteration (batch) in the x axis and the other demonstrating validation loss and validation accuracy in the y axis and epoch in the x axis. Please note that we need these plots to see if your model behaves as expected. Therefore, you may lose additional points if you do not provide these plots.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDfGUr10Mmkj",
        "colab_type": "code",
        "outputId": "92d736f9-dcbf-4643-879c-95efd0164874",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# write your code in this cell to plot your results\n",
        "def train_plot(train_accuracy_list, train_loss_list):\n",
        "  x = [(i+1) for i in range(len(train_accuracy_list))]\n",
        "  plt.plot(x, train_accuracy_list, 'ro', label='Accuracy', color=\"blue\")\n",
        "  plt.plot(x, train_loss_list, 'ro', label='Loss', color=\"red\")\n",
        "  plt.title('Train Plot')\n",
        "  plt.xlabel('Iteration', color='#1C2833')\n",
        "  plt.ylabel('Accuracy and Loss', color='#1C2833')\n",
        "  plt.legend(loc='upper left')\n",
        "  plt.grid()\n",
        "  plt.show()\n",
        "\n",
        "def validation_plot(val_accuracy_list, val_loss_list):\n",
        "  x = [(i+1) for i in range(len(val_accuracy_list))]\n",
        "  plt.plot(x, val_accuracy_list, 'ro', label='Accuracy', color=\"blue\")\n",
        "  plt.plot(x, val_loss_list, 'ro', label='Loss', color=\"red\")\n",
        "  plt.title('Validation Plot')\n",
        "  plt.xlabel('Epoch', color='#1C2833')\n",
        "  plt.ylabel('Accuracy and Loss', color='#1C2833')\n",
        "  plt.legend(loc='upper left')\n",
        "  plt.grid()\n",
        "  plt.show()\n",
        "\n",
        "train_plot(train_accuracy_list, train_loss_list)\n",
        "validation_plot(val_accuracy_list, val_loss_list)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/36fJ/YAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7v8am0hMmkm",
        "colab_type": "text"
      },
      "source": [
        "#### Testing [2 pts]\n",
        "\n",
        "Test your final model on your test set. Calculate confusion matrix, F1 score, precision and recall values and report these findings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aq6YqUadMmkn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# write your code in this cell to test your best model with the test dataset\n",
        "def test_final(criterion, testloader, test_batch, best_path):\n",
        "    model = torch.load(best_path)\n",
        "    model.eval()\n",
        "    preds = zeros((TEST_COUNT, 10)).cuda()\n",
        "    total_labels = zeros((TEST_COUNT)).cuda()\n",
        "    count = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, labels) in enumerate(testloader):\n",
        "          y_pred = model(data.float().cuda())\n",
        "          print(y_pred.shape)\n",
        "          print(labels.shape)\n",
        "          for i in range(y_pred.shape[0]):\n",
        "            preds[i+count] = y_pred[i]\n",
        "            total_labels[i+count] = labels[i]\n",
        "          count += test_batch\n",
        "    print(preds.shape)\n",
        "    print(find_confusion_matrix(preds, total_labels))\n",
        "    print(find_precisions(preds, total_labels))\n",
        "    print(find_recalls(preds, total_labels))\n",
        "    print(find_f1(preds, total_labels))\n",
        "    print(find_accuracy(preds, total_labels))\n",
        "\n",
        "test_final(criterion, testloader, test_batch, best_path)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIpXteK9ndHF",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-J6f1QPMmk3",
        "colab_type": "text"
      },
      "source": [
        "### 2.2. Convolutional Neural Network (CNN) [40 pts]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssXrFEtjMmk4",
        "colab_type": "text"
      },
      "source": [
        "#### Data Loader [2 pts]\n",
        "\n",
        "In this part, you will train a CNN for the same problem. Again, the pixel values need to be normalized to [0,1] range. Please do **not** change images to grayscale this time. First, implement the data loader (AnimalsDataset). You have to divide the files into three sets which are <b>train (70%)</b>, <b>validation (20%)</b> and **test (10%)**.  These non-overlapping splits, which are subsets of AnimalDataset, should be retrieved using the \"get_dataset\" function. Note that this time you do **not** need to flatten the image. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLpZ2tCnMmk4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ROOT = \"/content/drive/My Drive/Colab Notebooks/CS464 - HW3/hw3_dataset.zip (Unzipped Files)/dataset\"\n",
        "IMAGE_SIZE = 100\n",
        "IMAGE_COUNT = 2000\n",
        "TRAIN_COUNT = int(IMAGE_COUNT*0.7)\n",
        "VALID_COUNT = int(IMAGE_COUNT*0.2)\n",
        "TEST_COUNT = int(IMAGE_COUNT*0.1)\n",
        "class AnimalsDataset(Dataset):\n",
        "    \n",
        "    # TODO:\n",
        "    # Define constructor for AnimalDataset class\n",
        "    # HINT: You can pass processed data samples and their ground truth values as parameters \n",
        "    def __init__(self, **kwargs):\n",
        "      self.data = kwargs[\"data\"]\n",
        "      self.labels = kwargs[\"labels\"]\n",
        "        \n",
        "    '''This function should return sample count in the dataset'''\n",
        "    def __len__(self):\n",
        "       return self.data.shape[0]\n",
        "\n",
        "    '''This function should return a single sample and its ground truth value from the dataset corresponding to index parameter '''\n",
        "    def __getitem__(self, index):\n",
        "      return Tensor(self.data[index]).cuda(), self.labels[index]\n",
        "        #return _x, _y\n",
        "\n",
        "        \n",
        "def get_dataset(root):\n",
        "    # TODO: \n",
        "    # Read dataset files\n",
        "    # Construct training, validation and test sets\n",
        "    # Normalize datasets\n",
        "    transform_img = transforms.Compose([\n",
        "      # transforms.Resize(int(IMAGE_SIZE*(1.25))),\n",
        "      # transforms.CenterCrop(IMAGE_SIZE),\n",
        "      transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    images_tuple = ImageFolder(root, transform=transform_img)\n",
        "    print(images_tuple[0][0].shape)\n",
        "\n",
        "    images = zeros([IMAGE_COUNT, 3, IMAGE_SIZE, IMAGE_SIZE])\n",
        "\n",
        "    print(images.shape)\n",
        "    labels = zeros([IMAGE_COUNT])\n",
        "\n",
        "    # print(labels.shape)\n",
        "    for i in range(IMAGE_COUNT):\n",
        "      # print(images_tuple[i][0][0][0])\n",
        "      images[i] = images_tuple[i][0][0]\n",
        "      labels[i] = images_tuple[i][1]\n",
        "\n",
        "    rand_indices = randperm(IMAGE_COUNT).cuda()\n",
        "\n",
        "    train_indices = rand_indices[:TRAIN_COUNT]\n",
        "    valid_indices = rand_indices[TRAIN_COUNT:TRAIN_COUNT+VALID_COUNT]\n",
        "    test_indices = rand_indices[TRAIN_COUNT+VALID_COUNT:]\n",
        "    train_images = images[train_indices,:]\n",
        "\n",
        "    valid_images = images[valid_indices,:]\n",
        "    test_images = images[test_indices,:]\n",
        "\n",
        "    train_labels = labels[train_indices]\n",
        "    valid_labels = labels[valid_indices]\n",
        "    test_labels = labels[test_indices]\n",
        "    train_dataset = AnimalsDataset(labels=train_labels, data=train_images)\n",
        "    val_dataset = AnimalsDataset(labels=valid_labels, data=valid_images)\n",
        "    test_dataset = AnimalsDataset(labels=test_labels, data= test_images)\n",
        "    \n",
        "\n",
        "    return train_dataset, val_dataset, test_dataset\n",
        "    #return train_dataset, val_dataset, test_dataset#"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-RsQD3EMmk8",
        "colab_type": "text"
      },
      "source": [
        "#### Convolutional Neural Network [10 pts]\n",
        "\n",
        "Now implement your CNN. ConvNet class will represent your convolutional neural network. Implement 3 layers of convolution: \n",
        "<ul>\n",
        "    <li>(1) 32 filters with size of 3 x 3 x 3 with stride 1 and no padding, (2) ReLU </li>\n",
        "    <li>(3) 64 filters with size of 3 x 3 x 3 with stride 1 and no padding, (4) ReLU and (5) MaxPool 2 x 2 </li>\n",
        "    <li>(6) 128 filters with size of 3 x 3 x 3 with stride 1 and zero padding, (7) ReLU and (8) MaxPool 2 x 2 </li> \n",
        "</ul>\n",
        "\n",
        "As a classifier layer, you need to add only one linear layer at the end of the network. You need to choose the appropriate input and output neuron sizes and the activation function for the dense layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGiD0Y_oMmk9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch import nn\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "    '''Define your neural network'''\n",
        "    def __init__(self, **kwargs): # you can add any additional parameters you want \n",
        "    # TODO:\n",
        "    # You should create your neural network here\n",
        "      super(ConvNet, self).__init__()\n",
        "      self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1)\n",
        "      self.relu1 = nn.ReLU()\n",
        "      self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1)\n",
        "      self.relu2 = nn.ReLU()\n",
        "      self.max_pool1 = nn.MaxPool2d(kernel_size=(2,2))\n",
        "      self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=0)\n",
        "      self.relu3 = nn.ReLU()\n",
        "      self.max_pool2 = nn.MaxPool2d(kernel_size=(2,2))\n",
        "      self.linear = nn.Linear(23*23*128,10)\n",
        "     \n",
        "    def forward(self, X): # you can add any additional parameters you want\n",
        "    # TODO:\n",
        "    # Forward propagation implementation should be here\n",
        "      X = self.conv1(X)\n",
        "      X = self.relu1(X)   \n",
        "      X = self.conv2(X)\n",
        "      X = self.relu2(X)\n",
        "      X = self.max_pool1(X)     \n",
        "      X = self.conv3(X)\n",
        "      X = self.relu3(X)\n",
        "      X = self.max_pool2(X)\n",
        "\n",
        "      X = X.view(-1,23*23*128)\n",
        "      X = self.linear(X)\n",
        "      \n",
        "      return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioU02PmPMmlA",
        "colab_type": "text"
      },
      "source": [
        "#### Training and Testing [20 pts]\n",
        "\n",
        "Now, train your network. You need to select the appropriate loss function and  your hyper-parameters. Make sure to shuffle the samples in the training split. Plot the training loss and accuracy for each iteration. Also plot the validation loss and accuracy for each epoch as another figure. Your model is going to run upto the \"max_epoch\" parameter. Pick the best model as your final model. You need to save this model as a \".pth\" file. Report the test performance change (In terms of accuracy, F1 score, precision and recall) between MLP and CNN and explain the reason for this change explicitly, if there is any."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swWSqCgnMmlD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4f0b8a6e-efe8-4168-8090-1f8b61828017"
      },
      "source": [
        "import torch\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader\n",
        "#HINT: note that your training time should not take many days.\n",
        "\n",
        "#TODO:\n",
        "#Pick your hyper parameters\n",
        "max_epoch = 50\n",
        "train_batch = 32\n",
        "test_batch = 32\n",
        "\n",
        "\n",
        "learning_rate = 0.001\n",
        "\n",
        "use_gpu = torch.cuda.is_available()\n",
        "print(\"Use GPU:\", use_gpu)\n",
        "\n",
        "\n",
        "def main(): # you are free to change parameters\n",
        "\n",
        "    # Create train dataset loader\n",
        "    # Create validation dataset loader\n",
        "    # Create test dataset loader\n",
        "    # initialize your GENet neural network\n",
        "    # define your loss function\n",
        "    train_dataset, val_dataset, test_dataset = get_dataset(ROOT)\n",
        "    trainloader = DataLoader(train_dataset, batch_size=train_batch)\n",
        "    testloader = DataLoader(test_dataset, batch_size=test_batch)\n",
        "    val_loader = DataLoader(val_dataset)\n",
        "\n",
        "    model = ConvNet().cuda()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-04) # you can play with momentum and weight_decay parameters as well\n",
        "    criterion = nn.CrossEntropyLoss().cuda()\n",
        "    # start training\n",
        "    # for each epoch calculate validation performance\n",
        "    # save best model according to validation performance\n",
        "    best_acc = 0\n",
        "    best_path = '/content/drive/My Drive/Colab Notebooks/CS464 - HW3/best_model_cnn.pth'\n",
        "    for epoch in range(max_epoch):\n",
        "       train(epoch, model, criterion, optimizer, trainloader)\n",
        "       acc = test(model, val_loader)\n",
        "       if acc > best_acc:\n",
        "          torch.save(model, best_path)\n",
        "          best_acc = acc\n",
        "    print(\"Best Acc:\", best_acc)\n",
        "''' Train your network for a one epoch '''\n",
        "def train(epoch, model, criterion, optimizer, loader): # you are free to change parameters\n",
        "    model.train()\n",
        "    losses = AverageMeter()\n",
        "    accuracies = AverageMeter()\n",
        "\n",
        "    for batch_idx, (data, labels) in enumerate(loader):\n",
        "        # TODO:\n",
        "        # Implement training code for a one iteration\n",
        "        y_pred = model(data.float().cuda())\n",
        "        loss = criterion(y_pred, labels.long().cuda())\n",
        "        losses.update(loss.item(), data.size(0))\n",
        "        accuracy = find_accuracy(y_pred, labels)\n",
        "        # accuracy = pf1(y_pred, labels)\n",
        "        accuracies.update(accuracy)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # print('Epoch: [{0}][{1}/{2}]\\t'\n",
        "        #       'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "        #       'Data {data_time.val:.4f} ({data_time.avg:.4f})\\t'\n",
        "        #       'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "        #       'Accu {acc.val:.4f} ({acc.avg:.4f})\\t'.format(\n",
        "        #        epoch + 1, batch_idx + 1, len(trainloader), \n",
        "        #        batch_time=batch_time,\n",
        "        #        data_time=data_time, \n",
        "        #        loss=losses,\n",
        "        #        acc=accuracies))\n",
        "        print('Epoch: [{0}][{1}/{2}]\\t'\n",
        "              'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "              'Accu {acc.val:.4f} ({acc.avg:.4f})\\t'.format(\n",
        "               epoch + 1, batch_idx + 1, len(loader), \n",
        "               loss=losses,\n",
        "               acc=accuracies))\n",
        "\n",
        "\n",
        "''' Test&Validate your network '''\n",
        "def test(model, loader): # you are free to change parameters\n",
        "\n",
        "    model.eval()\n",
        "    # losses = AverageMeter()\n",
        "    accuracies = AverageMeter()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, labels) in enumerate(loader):\n",
        "            # TODO:\n",
        "            # Implement test code\n",
        "            # y_pred = model(data.long().conda())\n",
        "            y_pred = model(data.float().cuda())\n",
        "            # loss = criterion(y_pred, labels.long().cuda())\n",
        "            # losses.update(loss.item(), data.size(0))\n",
        "            accuracy = find_accuracy(y_pred, labels)\n",
        "            # accuracy = pf1(y_pred, labels)\n",
        "            accuracies.update(accuracy)\n",
        "        # print('Time {batch_time.avg:.3f}\\t'\n",
        "        #       'Accu {acc.avg:.4f}\\t'.format(\n",
        "        #        batch_time=batch_time, \n",
        "        #        acc=accuracies))\n",
        "        print('Accu {acc.avg:.4f}\\t'.format(\n",
        "               acc=accuracies))\n",
        "    return accuracies.avg\n",
        "\n",
        "main()"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Use GPU: True\n",
            "torch.Size([3, 100, 100])\n",
            "torch.Size([2000, 3, 100, 100])\n",
            "Epoch: [1][1/44]\tLoss 2.3025 (2.3025)\tAccu 0.1250 (0.1250)\t\n",
            "Epoch: [1][2/44]\tLoss 2.2948 (2.2987)\tAccu 0.1562 (0.1406)\t\n",
            "Epoch: [1][3/44]\tLoss 2.3048 (2.3007)\tAccu 0.0938 (0.1250)\t\n",
            "Epoch: [1][4/44]\tLoss 2.3109 (2.3033)\tAccu 0.0312 (0.1016)\t\n",
            "Epoch: [1][5/44]\tLoss 2.2989 (2.3024)\tAccu 0.1250 (0.1062)\t\n",
            "Epoch: [1][6/44]\tLoss 2.3096 (2.3036)\tAccu 0.0312 (0.0938)\t\n",
            "Epoch: [1][7/44]\tLoss 2.3099 (2.3045)\tAccu 0.0938 (0.0938)\t\n",
            "Epoch: [1][8/44]\tLoss 2.2892 (2.3026)\tAccu 0.0938 (0.0938)\t\n",
            "Epoch: [1][9/44]\tLoss 2.3124 (2.3037)\tAccu 0.0938 (0.0938)\t\n",
            "Epoch: [1][10/44]\tLoss 2.3077 (2.3041)\tAccu 0.0625 (0.0906)\t\n",
            "Epoch: [1][11/44]\tLoss 2.2934 (2.3031)\tAccu 0.0938 (0.0909)\t\n",
            "Epoch: [1][12/44]\tLoss 2.2999 (2.3029)\tAccu 0.1562 (0.0964)\t\n",
            "Epoch: [1][13/44]\tLoss 2.2955 (2.3023)\tAccu 0.1250 (0.0986)\t\n",
            "Epoch: [1][14/44]\tLoss 2.2985 (2.3020)\tAccu 0.0938 (0.0982)\t\n",
            "Epoch: [1][15/44]\tLoss 2.2823 (2.3007)\tAccu 0.2188 (0.1062)\t\n",
            "Epoch: [1][16/44]\tLoss 2.2922 (2.3002)\tAccu 0.1562 (0.1094)\t\n",
            "Epoch: [1][17/44]\tLoss 2.3135 (2.3010)\tAccu 0.0625 (0.1066)\t\n",
            "Epoch: [1][18/44]\tLoss 2.3026 (2.3011)\tAccu 0.0312 (0.1024)\t\n",
            "Epoch: [1][19/44]\tLoss 2.3188 (2.3020)\tAccu 0.0625 (0.1003)\t\n",
            "Epoch: [1][20/44]\tLoss 2.2937 (2.3016)\tAccu 0.1875 (0.1047)\t\n",
            "Epoch: [1][21/44]\tLoss 2.3302 (2.3029)\tAccu 0.1250 (0.1057)\t\n",
            "Epoch: [1][22/44]\tLoss 2.3115 (2.3033)\tAccu 0.0938 (0.1051)\t\n",
            "Epoch: [1][23/44]\tLoss 2.2890 (2.3027)\tAccu 0.1562 (0.1073)\t\n",
            "Epoch: [1][24/44]\tLoss 2.3200 (2.3034)\tAccu 0.1562 (0.1094)\t\n",
            "Epoch: [1][25/44]\tLoss 2.3218 (2.3042)\tAccu 0.1250 (0.1100)\t\n",
            "Epoch: [1][26/44]\tLoss 2.3391 (2.3055)\tAccu 0.0312 (0.1070)\t\n",
            "Epoch: [1][27/44]\tLoss 2.3178 (2.3060)\tAccu 0.0625 (0.1053)\t\n",
            "Epoch: [1][28/44]\tLoss 2.3214 (2.3065)\tAccu 0.1562 (0.1071)\t\n",
            "Epoch: [1][29/44]\tLoss 2.3488 (2.3080)\tAccu 0.0312 (0.1045)\t\n",
            "Epoch: [1][30/44]\tLoss 2.3342 (2.3088)\tAccu 0.0312 (0.1021)\t\n",
            "Epoch: [1][31/44]\tLoss 2.3061 (2.3088)\tAccu 0.0938 (0.1018)\t\n",
            "Epoch: [1][32/44]\tLoss 2.3114 (2.3088)\tAccu 0.0938 (0.1016)\t\n",
            "Epoch: [1][33/44]\tLoss 2.2994 (2.3086)\tAccu 0.1250 (0.1023)\t\n",
            "Epoch: [1][34/44]\tLoss 2.2787 (2.3077)\tAccu 0.1250 (0.1029)\t\n",
            "Epoch: [1][35/44]\tLoss 2.2966 (2.3074)\tAccu 0.0938 (0.1027)\t\n",
            "Epoch: [1][36/44]\tLoss 2.3157 (2.3076)\tAccu 0.0312 (0.1007)\t\n",
            "Epoch: [1][37/44]\tLoss 2.2986 (2.3073)\tAccu 0.0625 (0.0997)\t\n",
            "Epoch: [1][38/44]\tLoss 2.3114 (2.3075)\tAccu 0.1562 (0.1012)\t\n",
            "Epoch: [1][39/44]\tLoss 2.2964 (2.3072)\tAccu 0.0938 (0.1010)\t\n",
            "Epoch: [1][40/44]\tLoss 2.3073 (2.3072)\tAccu 0.1562 (0.1023)\t\n",
            "Epoch: [1][41/44]\tLoss 2.2931 (2.3068)\tAccu 0.0938 (0.1021)\t\n",
            "Epoch: [1][42/44]\tLoss 2.3020 (2.3067)\tAccu 0.1250 (0.1027)\t\n",
            "Epoch: [1][43/44]\tLoss 2.3236 (2.3071)\tAccu 0.0938 (0.1025)\t\n",
            "Epoch: [1][44/44]\tLoss 2.2186 (2.3056)\tAccu 0.2083 (0.1049)\t\n",
            "Accu 0.0900\t\n",
            "Epoch: [2][1/44]\tLoss 2.3242 (2.3242)\tAccu 0.0312 (0.0312)\t\n",
            "Epoch: [2][2/44]\tLoss 2.3036 (2.3139)\tAccu 0.1250 (0.0781)\t\n",
            "Epoch: [2][3/44]\tLoss 2.3065 (2.3114)\tAccu 0.0938 (0.0833)\t\n",
            "Epoch: [2][4/44]\tLoss 2.3233 (2.3144)\tAccu 0.0938 (0.0859)\t\n",
            "Epoch: [2][5/44]\tLoss 2.2918 (2.3099)\tAccu 0.1250 (0.0938)\t\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ConvNet. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Conv2d. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type MaxPool2d. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: [2][6/44]\tLoss 2.3629 (2.3187)\tAccu 0.0312 (0.0833)\t\n",
            "Epoch: [2][7/44]\tLoss 2.3102 (2.3175)\tAccu 0.0625 (0.0804)\t\n",
            "Epoch: [2][8/44]\tLoss 2.3137 (2.3170)\tAccu 0.2500 (0.1016)\t\n",
            "Epoch: [2][9/44]\tLoss 2.3082 (2.3161)\tAccu 0.0625 (0.0972)\t\n",
            "Epoch: [2][10/44]\tLoss 2.3104 (2.3155)\tAccu 0.0938 (0.0969)\t\n",
            "Epoch: [2][11/44]\tLoss 2.3280 (2.3166)\tAccu 0.0312 (0.0909)\t\n",
            "Epoch: [2][12/44]\tLoss 2.3021 (2.3154)\tAccu 0.0938 (0.0911)\t\n",
            "Epoch: [2][13/44]\tLoss 2.3091 (2.3149)\tAccu 0.0625 (0.0889)\t\n",
            "Epoch: [2][14/44]\tLoss 2.3176 (2.3151)\tAccu 0.0312 (0.0848)\t\n",
            "Epoch: [2][15/44]\tLoss 2.3056 (2.3145)\tAccu 0.1562 (0.0896)\t\n",
            "Epoch: [2][16/44]\tLoss 2.2971 (2.3134)\tAccu 0.0938 (0.0898)\t\n",
            "Epoch: [2][17/44]\tLoss 2.2819 (2.3115)\tAccu 0.1250 (0.0919)\t\n",
            "Epoch: [2][18/44]\tLoss 2.3072 (2.3113)\tAccu 0.0938 (0.0920)\t\n",
            "Epoch: [2][19/44]\tLoss 2.3005 (2.3107)\tAccu 0.1875 (0.0970)\t\n",
            "Epoch: [2][20/44]\tLoss 2.3000 (2.3102)\tAccu 0.1562 (0.1000)\t\n",
            "Epoch: [2][21/44]\tLoss 2.3024 (2.3098)\tAccu 0.0938 (0.0997)\t\n",
            "Epoch: [2][22/44]\tLoss 2.2886 (2.3089)\tAccu 0.1250 (0.1009)\t\n",
            "Epoch: [2][23/44]\tLoss 2.2880 (2.3080)\tAccu 0.0312 (0.0978)\t\n",
            "Epoch: [2][24/44]\tLoss 2.2975 (2.3075)\tAccu 0.1562 (0.1003)\t\n",
            "Epoch: [2][25/44]\tLoss 2.3113 (2.3077)\tAccu 0.0938 (0.1000)\t\n",
            "Epoch: [2][26/44]\tLoss 2.3212 (2.3082)\tAccu 0.0625 (0.0986)\t\n",
            "Epoch: [2][27/44]\tLoss 2.3040 (2.3080)\tAccu 0.1250 (0.0995)\t\n",
            "Epoch: [2][28/44]\tLoss 2.3180 (2.3084)\tAccu 0.1562 (0.1016)\t\n",
            "Epoch: [2][29/44]\tLoss 2.3324 (2.3092)\tAccu 0.0312 (0.0991)\t\n",
            "Epoch: [2][30/44]\tLoss 2.3160 (2.3094)\tAccu 0.0312 (0.0969)\t\n",
            "Epoch: [2][31/44]\tLoss 2.2987 (2.3091)\tAccu 0.0938 (0.0968)\t\n",
            "Epoch: [2][32/44]\tLoss 2.3017 (2.3089)\tAccu 0.0938 (0.0967)\t\n",
            "Epoch: [2][33/44]\tLoss 2.2901 (2.3083)\tAccu 0.1250 (0.0975)\t\n",
            "Epoch: [2][34/44]\tLoss 2.2939 (2.3079)\tAccu 0.0938 (0.0974)\t\n",
            "Epoch: [2][35/44]\tLoss 2.2947 (2.3075)\tAccu 0.0312 (0.0955)\t\n",
            "Epoch: [2][36/44]\tLoss 2.2941 (2.3071)\tAccu 0.2188 (0.0990)\t\n",
            "Epoch: [2][37/44]\tLoss 2.2860 (2.3066)\tAccu 0.1250 (0.0997)\t\n",
            "Epoch: [2][38/44]\tLoss 2.2908 (2.3061)\tAccu 0.0625 (0.0987)\t\n",
            "Epoch: [2][39/44]\tLoss 2.2904 (2.3057)\tAccu 0.1562 (0.1002)\t\n",
            "Epoch: [2][40/44]\tLoss 2.3084 (2.3058)\tAccu 0.1562 (0.1016)\t\n",
            "Epoch: [2][41/44]\tLoss 2.2908 (2.3054)\tAccu 0.1250 (0.1021)\t\n",
            "Epoch: [2][42/44]\tLoss 2.2938 (2.3052)\tAccu 0.1562 (0.1034)\t\n",
            "Epoch: [2][43/44]\tLoss 2.2949 (2.3049)\tAccu 0.0938 (0.1032)\t\n",
            "Epoch: [2][44/44]\tLoss 2.2607 (2.3042)\tAccu 0.1667 (0.1046)\t\n",
            "Accu 0.1050\t\n",
            "Epoch: [3][1/44]\tLoss 2.2986 (2.2986)\tAccu 0.0312 (0.0312)\t\n",
            "Epoch: [3][2/44]\tLoss 2.2971 (2.2978)\tAccu 0.1250 (0.0781)\t\n",
            "Epoch: [3][3/44]\tLoss 2.2919 (2.2959)\tAccu 0.0625 (0.0729)\t\n",
            "Epoch: [3][4/44]\tLoss 2.2856 (2.2933)\tAccu 0.0938 (0.0781)\t\n",
            "Epoch: [3][5/44]\tLoss 2.2803 (2.2907)\tAccu 0.1250 (0.0875)\t\n",
            "Epoch: [3][6/44]\tLoss 2.3057 (2.2932)\tAccu 0.0312 (0.0781)\t\n",
            "Epoch: [3][7/44]\tLoss 2.2925 (2.2931)\tAccu 0.1250 (0.0848)\t\n",
            "Epoch: [3][8/44]\tLoss 2.2932 (2.2931)\tAccu 0.0938 (0.0859)\t\n",
            "Epoch: [3][9/44]\tLoss 2.2979 (2.2936)\tAccu 0.1250 (0.0903)\t\n",
            "Epoch: [3][10/44]\tLoss 2.2929 (2.2936)\tAccu 0.0312 (0.0844)\t\n",
            "Epoch: [3][11/44]\tLoss 2.3059 (2.2947)\tAccu 0.0625 (0.0824)\t\n",
            "Epoch: [3][12/44]\tLoss 2.2828 (2.2937)\tAccu 0.1250 (0.0859)\t\n",
            "Epoch: [3][13/44]\tLoss 2.3000 (2.2942)\tAccu 0.0938 (0.0865)\t\n",
            "Epoch: [3][14/44]\tLoss 2.3007 (2.2946)\tAccu 0.0625 (0.0848)\t\n",
            "Epoch: [3][15/44]\tLoss 2.3019 (2.2951)\tAccu 0.1562 (0.0896)\t\n",
            "Epoch: [3][16/44]\tLoss 2.2817 (2.2943)\tAccu 0.2188 (0.0977)\t\n",
            "Epoch: [3][17/44]\tLoss 2.2736 (2.2931)\tAccu 0.2812 (0.1085)\t\n",
            "Epoch: [3][18/44]\tLoss 2.3060 (2.2938)\tAccu 0.1562 (0.1111)\t\n",
            "Epoch: [3][19/44]\tLoss 2.2913 (2.2937)\tAccu 0.0938 (0.1102)\t\n",
            "Epoch: [3][20/44]\tLoss 2.2877 (2.2934)\tAccu 0.2188 (0.1156)\t\n",
            "Epoch: [3][21/44]\tLoss 2.2903 (2.2932)\tAccu 0.0938 (0.1146)\t\n",
            "Epoch: [3][22/44]\tLoss 2.2773 (2.2925)\tAccu 0.1562 (0.1165)\t\n",
            "Epoch: [3][23/44]\tLoss 2.2864 (2.2922)\tAccu 0.2188 (0.1209)\t\n",
            "Epoch: [3][24/44]\tLoss 2.2835 (2.2919)\tAccu 0.1562 (0.1224)\t\n",
            "Epoch: [3][25/44]\tLoss 2.2926 (2.2919)\tAccu 0.1562 (0.1237)\t\n",
            "Epoch: [3][26/44]\tLoss 2.2990 (2.2922)\tAccu 0.0625 (0.1214)\t\n",
            "Epoch: [3][27/44]\tLoss 2.2936 (2.2922)\tAccu 0.1562 (0.1227)\t\n",
            "Epoch: [3][28/44]\tLoss 2.3013 (2.2925)\tAccu 0.1250 (0.1228)\t\n",
            "Epoch: [3][29/44]\tLoss 2.3191 (2.2935)\tAccu 0.0625 (0.1207)\t\n",
            "Epoch: [3][30/44]\tLoss 2.3016 (2.2937)\tAccu 0.0625 (0.1187)\t\n",
            "Epoch: [3][31/44]\tLoss 2.2922 (2.2937)\tAccu 0.1562 (0.1200)\t\n",
            "Epoch: [3][32/44]\tLoss 2.2796 (2.2932)\tAccu 0.1875 (0.1221)\t\n",
            "Epoch: [3][33/44]\tLoss 2.2752 (2.2927)\tAccu 0.2500 (0.1259)\t\n",
            "Epoch: [3][34/44]\tLoss 2.2647 (2.2919)\tAccu 0.2188 (0.1287)\t\n",
            "Epoch: [3][35/44]\tLoss 2.2660 (2.2911)\tAccu 0.1562 (0.1295)\t\n",
            "Epoch: [3][36/44]\tLoss 2.2948 (2.2912)\tAccu 0.1875 (0.1311)\t\n",
            "Epoch: [3][37/44]\tLoss 2.2623 (2.2904)\tAccu 0.1875 (0.1326)\t\n",
            "Epoch: [3][38/44]\tLoss 2.2714 (2.2899)\tAccu 0.1562 (0.1332)\t\n",
            "Epoch: [3][39/44]\tLoss 2.2697 (2.2894)\tAccu 0.1250 (0.1330)\t\n",
            "Epoch: [3][40/44]\tLoss 2.3013 (2.2897)\tAccu 0.1875 (0.1344)\t\n",
            "Epoch: [3][41/44]\tLoss 2.2772 (2.2894)\tAccu 0.1250 (0.1341)\t\n",
            "Epoch: [3][42/44]\tLoss 2.2809 (2.2892)\tAccu 0.1562 (0.1347)\t\n",
            "Epoch: [3][43/44]\tLoss 2.2911 (2.2893)\tAccu 0.1562 (0.1352)\t\n",
            "Epoch: [3][44/44]\tLoss 2.2259 (2.2882)\tAccu 0.2083 (0.1368)\t\n",
            "Accu 0.1075\t\n",
            "Epoch: [4][1/44]\tLoss 2.2979 (2.2979)\tAccu 0.0938 (0.0938)\t\n",
            "Epoch: [4][2/44]\tLoss 2.2901 (2.2940)\tAccu 0.1875 (0.1406)\t\n",
            "Epoch: [4][3/44]\tLoss 2.2839 (2.2906)\tAccu 0.1562 (0.1458)\t\n",
            "Epoch: [4][4/44]\tLoss 2.2732 (2.2863)\tAccu 0.1562 (0.1484)\t\n",
            "Epoch: [4][5/44]\tLoss 2.2691 (2.2828)\tAccu 0.0938 (0.1375)\t\n",
            "Epoch: [4][6/44]\tLoss 2.2920 (2.2844)\tAccu 0.1562 (0.1406)\t\n",
            "Epoch: [4][7/44]\tLoss 2.2867 (2.2847)\tAccu 0.1562 (0.1429)\t\n",
            "Epoch: [4][8/44]\tLoss 2.2819 (2.2843)\tAccu 0.1250 (0.1406)\t\n",
            "Epoch: [4][9/44]\tLoss 2.2903 (2.2850)\tAccu 0.1250 (0.1389)\t\n",
            "Epoch: [4][10/44]\tLoss 2.2771 (2.2842)\tAccu 0.0938 (0.1344)\t\n",
            "Epoch: [4][11/44]\tLoss 2.2964 (2.2853)\tAccu 0.0312 (0.1250)\t\n",
            "Epoch: [4][12/44]\tLoss 2.2680 (2.2839)\tAccu 0.1250 (0.1250)\t\n",
            "Epoch: [4][13/44]\tLoss 2.2901 (2.2844)\tAccu 0.0938 (0.1226)\t\n",
            "Epoch: [4][14/44]\tLoss 2.2809 (2.2841)\tAccu 0.0625 (0.1183)\t\n",
            "Epoch: [4][15/44]\tLoss 2.2959 (2.2849)\tAccu 0.1562 (0.1208)\t\n",
            "Epoch: [4][16/44]\tLoss 2.2668 (2.2838)\tAccu 0.1875 (0.1250)\t\n",
            "Epoch: [4][17/44]\tLoss 2.2550 (2.2821)\tAccu 0.2812 (0.1342)\t\n",
            "Epoch: [4][18/44]\tLoss 2.2957 (2.2828)\tAccu 0.2188 (0.1389)\t\n",
            "Epoch: [4][19/44]\tLoss 2.2807 (2.2827)\tAccu 0.2188 (0.1431)\t\n",
            "Epoch: [4][20/44]\tLoss 2.2744 (2.2823)\tAccu 0.2188 (0.1469)\t\n",
            "Epoch: [4][21/44]\tLoss 2.2912 (2.2827)\tAccu 0.1250 (0.1458)\t\n",
            "Epoch: [4][22/44]\tLoss 2.2563 (2.2815)\tAccu 0.1562 (0.1463)\t\n",
            "Epoch: [4][23/44]\tLoss 2.2681 (2.2809)\tAccu 0.2188 (0.1495)\t\n",
            "Epoch: [4][24/44]\tLoss 2.2745 (2.2807)\tAccu 0.1250 (0.1484)\t\n",
            "Epoch: [4][25/44]\tLoss 2.2893 (2.2810)\tAccu 0.1562 (0.1487)\t\n",
            "Epoch: [4][26/44]\tLoss 2.3001 (2.2817)\tAccu 0.0312 (0.1442)\t\n",
            "Epoch: [4][27/44]\tLoss 2.2886 (2.2820)\tAccu 0.1875 (0.1458)\t\n",
            "Epoch: [4][28/44]\tLoss 2.2981 (2.2826)\tAccu 0.1250 (0.1451)\t\n",
            "Epoch: [4][29/44]\tLoss 2.3339 (2.2843)\tAccu 0.0625 (0.1422)\t\n",
            "Epoch: [4][30/44]\tLoss 2.2928 (2.2846)\tAccu 0.0625 (0.1396)\t\n",
            "Epoch: [4][31/44]\tLoss 2.2790 (2.2844)\tAccu 0.1250 (0.1391)\t\n",
            "Epoch: [4][32/44]\tLoss 2.2646 (2.2838)\tAccu 0.2188 (0.1416)\t\n",
            "Epoch: [4][33/44]\tLoss 2.2590 (2.2831)\tAccu 0.2188 (0.1439)\t\n",
            "Epoch: [4][34/44]\tLoss 2.2372 (2.2817)\tAccu 0.2500 (0.1471)\t\n",
            "Epoch: [4][35/44]\tLoss 2.2349 (2.2804)\tAccu 0.2188 (0.1491)\t\n",
            "Epoch: [4][36/44]\tLoss 2.2932 (2.2807)\tAccu 0.2188 (0.1510)\t\n",
            "Epoch: [4][37/44]\tLoss 2.2328 (2.2794)\tAccu 0.2500 (0.1537)\t\n",
            "Epoch: [4][38/44]\tLoss 2.2502 (2.2787)\tAccu 0.1562 (0.1538)\t\n",
            "Epoch: [4][39/44]\tLoss 2.2523 (2.2780)\tAccu 0.1250 (0.1530)\t\n",
            "Epoch: [4][40/44]\tLoss 2.2935 (2.2784)\tAccu 0.1250 (0.1523)\t\n",
            "Epoch: [4][41/44]\tLoss 2.2635 (2.2780)\tAccu 0.1875 (0.1532)\t\n",
            "Epoch: [4][42/44]\tLoss 2.2604 (2.2776)\tAccu 0.1250 (0.1525)\t\n",
            "Epoch: [4][43/44]\tLoss 2.2869 (2.2778)\tAccu 0.1875 (0.1533)\t\n",
            "Epoch: [4][44/44]\tLoss 2.1838 (2.2762)\tAccu 0.2083 (0.1546)\t\n",
            "Accu 0.1275\t\n",
            "Epoch: [5][1/44]\tLoss 2.2993 (2.2993)\tAccu 0.0938 (0.0938)\t\n",
            "Epoch: [5][2/44]\tLoss 2.2803 (2.2898)\tAccu 0.1875 (0.1406)\t\n",
            "Epoch: [5][3/44]\tLoss 2.2742 (2.2846)\tAccu 0.1875 (0.1562)\t\n",
            "Epoch: [5][4/44]\tLoss 2.2616 (2.2789)\tAccu 0.1875 (0.1641)\t\n",
            "Epoch: [5][5/44]\tLoss 2.2605 (2.2752)\tAccu 0.0625 (0.1437)\t\n",
            "Epoch: [5][6/44]\tLoss 2.2694 (2.2742)\tAccu 0.1875 (0.1510)\t\n",
            "Epoch: [5][7/44]\tLoss 2.2780 (2.2748)\tAccu 0.1250 (0.1473)\t\n",
            "Epoch: [5][8/44]\tLoss 2.2626 (2.2732)\tAccu 0.2188 (0.1562)\t\n",
            "Epoch: [5][9/44]\tLoss 2.2755 (2.2735)\tAccu 0.0938 (0.1493)\t\n",
            "Epoch: [5][10/44]\tLoss 2.2572 (2.2719)\tAccu 0.0938 (0.1437)\t\n",
            "Epoch: [5][11/44]\tLoss 2.2817 (2.2728)\tAccu 0.0938 (0.1392)\t\n",
            "Epoch: [5][12/44]\tLoss 2.2485 (2.2707)\tAccu 0.1250 (0.1380)\t\n",
            "Epoch: [5][13/44]\tLoss 2.2716 (2.2708)\tAccu 0.1875 (0.1418)\t\n",
            "Epoch: [5][14/44]\tLoss 2.2404 (2.2686)\tAccu 0.2500 (0.1496)\t\n",
            "Epoch: [5][15/44]\tLoss 2.2814 (2.2695)\tAccu 0.1875 (0.1521)\t\n",
            "Epoch: [5][16/44]\tLoss 2.2521 (2.2684)\tAccu 0.1562 (0.1523)\t\n",
            "Epoch: [5][17/44]\tLoss 2.2305 (2.2662)\tAccu 0.3438 (0.1636)\t\n",
            "Epoch: [5][18/44]\tLoss 2.2767 (2.2668)\tAccu 0.1250 (0.1615)\t\n",
            "Epoch: [5][19/44]\tLoss 2.2670 (2.2668)\tAccu 0.2188 (0.1645)\t\n",
            "Epoch: [5][20/44]\tLoss 2.2608 (2.2665)\tAccu 0.1875 (0.1656)\t\n",
            "Epoch: [5][21/44]\tLoss 2.3006 (2.2681)\tAccu 0.1250 (0.1637)\t\n",
            "Epoch: [5][22/44]\tLoss 2.2301 (2.2664)\tAccu 0.1562 (0.1634)\t\n",
            "Epoch: [5][23/44]\tLoss 2.2441 (2.2654)\tAccu 0.1875 (0.1644)\t\n",
            "Epoch: [5][24/44]\tLoss 2.2697 (2.2656)\tAccu 0.1562 (0.1641)\t\n",
            "Epoch: [5][25/44]\tLoss 2.2861 (2.2664)\tAccu 0.1562 (0.1638)\t\n",
            "Epoch: [5][26/44]\tLoss 2.2970 (2.2676)\tAccu 0.0312 (0.1587)\t\n",
            "Epoch: [5][27/44]\tLoss 2.2823 (2.2681)\tAccu 0.1250 (0.1574)\t\n",
            "Epoch: [5][28/44]\tLoss 2.2861 (2.2688)\tAccu 0.0938 (0.1551)\t\n",
            "Epoch: [5][29/44]\tLoss 2.3544 (2.2717)\tAccu 0.0625 (0.1519)\t\n",
            "Epoch: [5][30/44]\tLoss 2.2788 (2.2720)\tAccu 0.0938 (0.1500)\t\n",
            "Epoch: [5][31/44]\tLoss 2.2615 (2.2716)\tAccu 0.2188 (0.1522)\t\n",
            "Epoch: [5][32/44]\tLoss 2.2410 (2.2707)\tAccu 0.2500 (0.1553)\t\n",
            "Epoch: [5][33/44]\tLoss 2.2378 (2.2697)\tAccu 0.2500 (0.1581)\t\n",
            "Epoch: [5][34/44]\tLoss 2.1910 (2.2674)\tAccu 0.2812 (0.1618)\t\n",
            "Epoch: [5][35/44]\tLoss 2.1864 (2.2650)\tAccu 0.2188 (0.1634)\t\n",
            "Epoch: [5][36/44]\tLoss 2.2994 (2.2660)\tAccu 0.1562 (0.1632)\t\n",
            "Epoch: [5][37/44]\tLoss 2.1914 (2.2640)\tAccu 0.1875 (0.1639)\t\n",
            "Epoch: [5][38/44]\tLoss 2.2235 (2.2629)\tAccu 0.2188 (0.1653)\t\n",
            "Epoch: [5][39/44]\tLoss 2.2314 (2.2621)\tAccu 0.1562 (0.1651)\t\n",
            "Epoch: [5][40/44]\tLoss 2.2814 (2.2626)\tAccu 0.1562 (0.1648)\t\n",
            "Epoch: [5][41/44]\tLoss 2.2499 (2.2623)\tAccu 0.1562 (0.1646)\t\n",
            "Epoch: [5][42/44]\tLoss 2.2321 (2.2616)\tAccu 0.1562 (0.1644)\t\n",
            "Epoch: [5][43/44]\tLoss 2.2906 (2.2622)\tAccu 0.1562 (0.1642)\t\n",
            "Epoch: [5][44/44]\tLoss 2.1314 (2.2600)\tAccu 0.2500 (0.1662)\t\n",
            "Accu 0.1400\t\n",
            "Epoch: [6][1/44]\tLoss 2.3017 (2.3017)\tAccu 0.0938 (0.0938)\t\n",
            "Epoch: [6][2/44]\tLoss 2.2580 (2.2799)\tAccu 0.2188 (0.1562)\t\n",
            "Epoch: [6][3/44]\tLoss 2.2673 (2.2757)\tAccu 0.2188 (0.1771)\t\n",
            "Epoch: [6][4/44]\tLoss 2.2587 (2.2714)\tAccu 0.1875 (0.1797)\t\n",
            "Epoch: [6][5/44]\tLoss 2.2536 (2.2679)\tAccu 0.0938 (0.1625)\t\n",
            "Epoch: [6][6/44]\tLoss 2.2223 (2.2603)\tAccu 0.2812 (0.1823)\t\n",
            "Epoch: [6][7/44]\tLoss 2.2642 (2.2608)\tAccu 0.1562 (0.1786)\t\n",
            "Epoch: [6][8/44]\tLoss 2.2233 (2.2561)\tAccu 0.2500 (0.1875)\t\n",
            "Epoch: [6][9/44]\tLoss 2.2542 (2.2559)\tAccu 0.0625 (0.1736)\t\n",
            "Epoch: [6][10/44]\tLoss 2.2380 (2.2541)\tAccu 0.1875 (0.1750)\t\n",
            "Epoch: [6][11/44]\tLoss 2.2562 (2.2543)\tAccu 0.0938 (0.1676)\t\n",
            "Epoch: [6][12/44]\tLoss 2.2261 (2.2520)\tAccu 0.2188 (0.1719)\t\n",
            "Epoch: [6][13/44]\tLoss 2.2467 (2.2516)\tAccu 0.2188 (0.1755)\t\n",
            "Epoch: [6][14/44]\tLoss 2.1785 (2.2463)\tAccu 0.3125 (0.1853)\t\n",
            "Epoch: [6][15/44]\tLoss 2.2605 (2.2473)\tAccu 0.2188 (0.1875)\t\n",
            "Epoch: [6][16/44]\tLoss 2.2369 (2.2466)\tAccu 0.1562 (0.1855)\t\n",
            "Epoch: [6][17/44]\tLoss 2.2021 (2.2440)\tAccu 0.2500 (0.1893)\t\n",
            "Epoch: [6][18/44]\tLoss 2.2523 (2.2445)\tAccu 0.1250 (0.1858)\t\n",
            "Epoch: [6][19/44]\tLoss 2.2569 (2.2451)\tAccu 0.2188 (0.1875)\t\n",
            "Epoch: [6][20/44]\tLoss 2.2417 (2.2450)\tAccu 0.1562 (0.1859)\t\n",
            "Epoch: [6][21/44]\tLoss 2.3149 (2.2483)\tAccu 0.1250 (0.1830)\t\n",
            "Epoch: [6][22/44]\tLoss 2.2088 (2.2465)\tAccu 0.1562 (0.1818)\t\n",
            "Epoch: [6][23/44]\tLoss 2.2266 (2.2456)\tAccu 0.1250 (0.1793)\t\n",
            "Epoch: [6][24/44]\tLoss 2.2701 (2.2466)\tAccu 0.1562 (0.1784)\t\n",
            "Epoch: [6][25/44]\tLoss 2.2699 (2.2476)\tAccu 0.1562 (0.1775)\t\n",
            "Epoch: [6][26/44]\tLoss 2.2688 (2.2484)\tAccu 0.0938 (0.1743)\t\n",
            "Epoch: [6][27/44]\tLoss 2.2741 (2.2493)\tAccu 0.1562 (0.1736)\t\n",
            "Epoch: [6][28/44]\tLoss 2.2550 (2.2495)\tAccu 0.0938 (0.1708)\t\n",
            "Epoch: [6][29/44]\tLoss 2.3676 (2.2536)\tAccu 0.1250 (0.1692)\t\n",
            "Epoch: [6][30/44]\tLoss 2.2500 (2.2535)\tAccu 0.1875 (0.1698)\t\n",
            "Epoch: [6][31/44]\tLoss 2.2442 (2.2532)\tAccu 0.1875 (0.1704)\t\n",
            "Epoch: [6][32/44]\tLoss 2.2046 (2.2517)\tAccu 0.2188 (0.1719)\t\n",
            "Epoch: [6][33/44]\tLoss 2.2148 (2.2506)\tAccu 0.2188 (0.1733)\t\n",
            "Epoch: [6][34/44]\tLoss 2.1321 (2.2471)\tAccu 0.3125 (0.1774)\t\n",
            "Epoch: [6][35/44]\tLoss 2.1279 (2.2437)\tAccu 0.2188 (0.1786)\t\n",
            "Epoch: [6][36/44]\tLoss 2.3205 (2.2458)\tAccu 0.1562 (0.1780)\t\n",
            "Epoch: [6][37/44]\tLoss 2.1491 (2.2432)\tAccu 0.2188 (0.1791)\t\n",
            "Epoch: [6][38/44]\tLoss 2.1962 (2.2420)\tAccu 0.1875 (0.1793)\t\n",
            "Epoch: [6][39/44]\tLoss 2.2063 (2.2410)\tAccu 0.1875 (0.1795)\t\n",
            "Epoch: [6][40/44]\tLoss 2.2586 (2.2415)\tAccu 0.1562 (0.1789)\t\n",
            "Epoch: [6][41/44]\tLoss 2.2342 (2.2413)\tAccu 0.1250 (0.1776)\t\n",
            "Epoch: [6][42/44]\tLoss 2.1893 (2.2401)\tAccu 0.1562 (0.1771)\t\n",
            "Epoch: [6][43/44]\tLoss 2.2907 (2.2412)\tAccu 0.1562 (0.1766)\t\n",
            "Epoch: [6][44/44]\tLoss 2.0946 (2.2387)\tAccu 0.2500 (0.1783)\t\n",
            "Accu 0.1625\t\n",
            "Epoch: [7][1/44]\tLoss 2.2868 (2.2868)\tAccu 0.1250 (0.1250)\t\n",
            "Epoch: [7][2/44]\tLoss 2.2164 (2.2516)\tAccu 0.2500 (0.1875)\t\n",
            "Epoch: [7][3/44]\tLoss 2.2627 (2.2553)\tAccu 0.2188 (0.1979)\t\n",
            "Epoch: [7][4/44]\tLoss 2.2601 (2.2565)\tAccu 0.1875 (0.1953)\t\n",
            "Epoch: [7][5/44]\tLoss 2.2505 (2.2553)\tAccu 0.0938 (0.1750)\t\n",
            "Epoch: [7][6/44]\tLoss 2.1457 (2.2370)\tAccu 0.3125 (0.1979)\t\n",
            "Epoch: [7][7/44]\tLoss 2.2468 (2.2384)\tAccu 0.1875 (0.1964)\t\n",
            "Epoch: [7][8/44]\tLoss 2.1733 (2.2303)\tAccu 0.2500 (0.2031)\t\n",
            "Epoch: [7][9/44]\tLoss 2.2344 (2.2307)\tAccu 0.1250 (0.1944)\t\n",
            "Epoch: [7][10/44]\tLoss 2.2213 (2.2298)\tAccu 0.1562 (0.1906)\t\n",
            "Epoch: [7][11/44]\tLoss 2.2261 (2.2295)\tAccu 0.1250 (0.1847)\t\n",
            "Epoch: [7][12/44]\tLoss 2.2014 (2.2271)\tAccu 0.2500 (0.1901)\t\n",
            "Epoch: [7][13/44]\tLoss 2.2227 (2.2268)\tAccu 0.2500 (0.1947)\t\n",
            "Epoch: [7][14/44]\tLoss 2.1205 (2.2192)\tAccu 0.2812 (0.2009)\t\n",
            "Epoch: [7][15/44]\tLoss 2.2474 (2.2211)\tAccu 0.2188 (0.2021)\t\n",
            "Epoch: [7][16/44]\tLoss 2.2112 (2.2205)\tAccu 0.2188 (0.2031)\t\n",
            "Epoch: [7][17/44]\tLoss 2.1648 (2.2172)\tAccu 0.2188 (0.2040)\t\n",
            "Epoch: [7][18/44]\tLoss 2.2241 (2.2176)\tAccu 0.1250 (0.1997)\t\n",
            "Epoch: [7][19/44]\tLoss 2.2494 (2.2192)\tAccu 0.2500 (0.2023)\t\n",
            "Epoch: [7][20/44]\tLoss 2.2091 (2.2187)\tAccu 0.1562 (0.2000)\t\n",
            "Epoch: [7][21/44]\tLoss 2.3224 (2.2237)\tAccu 0.1250 (0.1964)\t\n",
            "Epoch: [7][22/44]\tLoss 2.1940 (2.2223)\tAccu 0.1875 (0.1960)\t\n",
            "Epoch: [7][23/44]\tLoss 2.2205 (2.2223)\tAccu 0.1562 (0.1943)\t\n",
            "Epoch: [7][24/44]\tLoss 2.2674 (2.2241)\tAccu 0.1562 (0.1927)\t\n",
            "Epoch: [7][25/44]\tLoss 2.2402 (2.2248)\tAccu 0.1250 (0.1900)\t\n",
            "Epoch: [7][26/44]\tLoss 2.2172 (2.2245)\tAccu 0.1875 (0.1899)\t\n",
            "Epoch: [7][27/44]\tLoss 2.2644 (2.2260)\tAccu 0.1875 (0.1898)\t\n",
            "Epoch: [7][28/44]\tLoss 2.2182 (2.2257)\tAccu 0.1250 (0.1875)\t\n",
            "Epoch: [7][29/44]\tLoss 2.3688 (2.2306)\tAccu 0.1250 (0.1853)\t\n",
            "Epoch: [7][30/44]\tLoss 2.2160 (2.2301)\tAccu 0.2812 (0.1885)\t\n",
            "Epoch: [7][31/44]\tLoss 2.2292 (2.2301)\tAccu 0.2188 (0.1895)\t\n",
            "Epoch: [7][32/44]\tLoss 2.1589 (2.2279)\tAccu 0.2500 (0.1914)\t\n",
            "Epoch: [7][33/44]\tLoss 2.1926 (2.2268)\tAccu 0.2812 (0.1941)\t\n",
            "Epoch: [7][34/44]\tLoss 2.0797 (2.2225)\tAccu 0.2812 (0.1967)\t\n",
            "Epoch: [7][35/44]\tLoss 2.0787 (2.2184)\tAccu 0.2500 (0.1982)\t\n",
            "Epoch: [7][36/44]\tLoss 2.3426 (2.2218)\tAccu 0.1562 (0.1970)\t\n",
            "Epoch: [7][37/44]\tLoss 2.1119 (2.2189)\tAccu 0.2812 (0.1993)\t\n",
            "Epoch: [7][38/44]\tLoss 2.1660 (2.2175)\tAccu 0.1875 (0.1990)\t\n",
            "Epoch: [7][39/44]\tLoss 2.1718 (2.2163)\tAccu 0.2188 (0.1995)\t\n",
            "Epoch: [7][40/44]\tLoss 2.2218 (2.2164)\tAccu 0.1250 (0.1977)\t\n",
            "Epoch: [7][41/44]\tLoss 2.2089 (2.2162)\tAccu 0.1875 (0.1974)\t\n",
            "Epoch: [7][42/44]\tLoss 2.1354 (2.2143)\tAccu 0.2500 (0.1987)\t\n",
            "Epoch: [7][43/44]\tLoss 2.2781 (2.2158)\tAccu 0.2812 (0.2006)\t\n",
            "Epoch: [7][44/44]\tLoss 2.0817 (2.2135)\tAccu 0.2083 (0.2008)\t\n",
            "Accu 0.1525\t\n",
            "Epoch: [8][1/44]\tLoss 2.2605 (2.2605)\tAccu 0.2500 (0.2500)\t\n",
            "Epoch: [8][2/44]\tLoss 2.1750 (2.2178)\tAccu 0.1562 (0.2031)\t\n",
            "Epoch: [8][3/44]\tLoss 2.2562 (2.2306)\tAccu 0.2188 (0.2083)\t\n",
            "Epoch: [8][4/44]\tLoss 2.2518 (2.2359)\tAccu 0.1875 (0.2031)\t\n",
            "Epoch: [8][5/44]\tLoss 2.2489 (2.2385)\tAccu 0.0938 (0.1812)\t\n",
            "Epoch: [8][6/44]\tLoss 2.0637 (2.2093)\tAccu 0.3438 (0.2083)\t\n",
            "Epoch: [8][7/44]\tLoss 2.2300 (2.2123)\tAccu 0.1562 (0.2009)\t\n",
            "Epoch: [8][8/44]\tLoss 2.1276 (2.2017)\tAccu 0.3125 (0.2148)\t\n",
            "Epoch: [8][9/44]\tLoss 2.2174 (2.2034)\tAccu 0.1250 (0.2049)\t\n",
            "Epoch: [8][10/44]\tLoss 2.2011 (2.2032)\tAccu 0.1875 (0.2031)\t\n",
            "Epoch: [8][11/44]\tLoss 2.1970 (2.2026)\tAccu 0.1562 (0.1989)\t\n",
            "Epoch: [8][12/44]\tLoss 2.1711 (2.2000)\tAccu 0.2500 (0.2031)\t\n",
            "Epoch: [8][13/44]\tLoss 2.1996 (2.2000)\tAccu 0.1875 (0.2019)\t\n",
            "Epoch: [8][14/44]\tLoss 2.0773 (2.1912)\tAccu 0.3125 (0.2098)\t\n",
            "Epoch: [8][15/44]\tLoss 2.2459 (2.1949)\tAccu 0.2500 (0.2125)\t\n",
            "Epoch: [8][16/44]\tLoss 2.1808 (2.1940)\tAccu 0.1875 (0.2109)\t\n",
            "Epoch: [8][17/44]\tLoss 2.1233 (2.1898)\tAccu 0.2812 (0.2151)\t\n",
            "Epoch: [8][18/44]\tLoss 2.1947 (2.1901)\tAccu 0.1875 (0.2135)\t\n",
            "Epoch: [8][19/44]\tLoss 2.2389 (2.1927)\tAccu 0.2500 (0.2155)\t\n",
            "Epoch: [8][20/44]\tLoss 2.1786 (2.1920)\tAccu 0.1562 (0.2125)\t\n",
            "Epoch: [8][21/44]\tLoss 2.3264 (2.1984)\tAccu 0.1562 (0.2098)\t\n",
            "Epoch: [8][22/44]\tLoss 2.1801 (2.1975)\tAccu 0.1875 (0.2088)\t\n",
            "Epoch: [8][23/44]\tLoss 2.2141 (2.1983)\tAccu 0.1875 (0.2079)\t\n",
            "Epoch: [8][24/44]\tLoss 2.2578 (2.2007)\tAccu 0.1875 (0.2070)\t\n",
            "Epoch: [8][25/44]\tLoss 2.2130 (2.2012)\tAccu 0.2188 (0.2075)\t\n",
            "Epoch: [8][26/44]\tLoss 2.1692 (2.2000)\tAccu 0.1875 (0.2067)\t\n",
            "Epoch: [8][27/44]\tLoss 2.2532 (2.2020)\tAccu 0.2188 (0.2072)\t\n",
            "Epoch: [8][28/44]\tLoss 2.1907 (2.2016)\tAccu 0.1250 (0.2042)\t\n",
            "Epoch: [8][29/44]\tLoss 2.3649 (2.2072)\tAccu 0.1250 (0.2015)\t\n",
            "Epoch: [8][30/44]\tLoss 2.1895 (2.2066)\tAccu 0.2812 (0.2042)\t\n",
            "Epoch: [8][31/44]\tLoss 2.2133 (2.2068)\tAccu 0.1562 (0.2026)\t\n",
            "Epoch: [8][32/44]\tLoss 2.1097 (2.2038)\tAccu 0.2500 (0.2041)\t\n",
            "Epoch: [8][33/44]\tLoss 2.1680 (2.2027)\tAccu 0.2812 (0.2064)\t\n",
            "Epoch: [8][34/44]\tLoss 2.0359 (2.1978)\tAccu 0.2812 (0.2086)\t\n",
            "Epoch: [8][35/44]\tLoss 2.0402 (2.1933)\tAccu 0.3125 (0.2116)\t\n",
            "Epoch: [8][36/44]\tLoss 2.3546 (2.1978)\tAccu 0.1562 (0.2101)\t\n",
            "Epoch: [8][37/44]\tLoss 2.0754 (2.1945)\tAccu 0.2812 (0.2120)\t\n",
            "Epoch: [8][38/44]\tLoss 2.1382 (2.1930)\tAccu 0.1562 (0.2105)\t\n",
            "Epoch: [8][39/44]\tLoss 2.1343 (2.1915)\tAccu 0.3125 (0.2131)\t\n",
            "Epoch: [8][40/44]\tLoss 2.1854 (2.1913)\tAccu 0.1250 (0.2109)\t\n",
            "Epoch: [8][41/44]\tLoss 2.1824 (2.1911)\tAccu 0.2188 (0.2111)\t\n",
            "Epoch: [8][42/44]\tLoss 2.0881 (2.1887)\tAccu 0.2812 (0.2128)\t\n",
            "Epoch: [8][43/44]\tLoss 2.2598 (2.1903)\tAccu 0.3125 (0.2151)\t\n",
            "Epoch: [8][44/44]\tLoss 2.0727 (2.1883)\tAccu 0.2500 (0.2159)\t\n",
            "Accu 0.1525\t\n",
            "Epoch: [9][1/44]\tLoss 2.2391 (2.2391)\tAccu 0.2812 (0.2812)\t\n",
            "Epoch: [9][2/44]\tLoss 2.1413 (2.1902)\tAccu 0.2500 (0.2656)\t\n",
            "Epoch: [9][3/44]\tLoss 2.2456 (2.2087)\tAccu 0.1875 (0.2396)\t\n",
            "Epoch: [9][4/44]\tLoss 2.2312 (2.2143)\tAccu 0.1875 (0.2266)\t\n",
            "Epoch: [9][5/44]\tLoss 2.2423 (2.2199)\tAccu 0.1875 (0.2188)\t\n",
            "Epoch: [9][6/44]\tLoss 1.9923 (2.1820)\tAccu 0.3750 (0.2448)\t\n",
            "Epoch: [9][7/44]\tLoss 2.2117 (2.1862)\tAccu 0.1875 (0.2366)\t\n",
            "Epoch: [9][8/44]\tLoss 2.0856 (2.1736)\tAccu 0.2812 (0.2422)\t\n",
            "Epoch: [9][9/44]\tLoss 2.1979 (2.1763)\tAccu 0.1875 (0.2361)\t\n",
            "Epoch: [9][10/44]\tLoss 2.1828 (2.1770)\tAccu 0.1875 (0.2313)\t\n",
            "Epoch: [9][11/44]\tLoss 2.1739 (2.1767)\tAccu 0.1875 (0.2273)\t\n",
            "Epoch: [9][12/44]\tLoss 2.1399 (2.1736)\tAccu 0.2812 (0.2318)\t\n",
            "Epoch: [9][13/44]\tLoss 2.1778 (2.1740)\tAccu 0.2188 (0.2308)\t\n",
            "Epoch: [9][14/44]\tLoss 2.0456 (2.1648)\tAccu 0.3125 (0.2366)\t\n",
            "Epoch: [9][15/44]\tLoss 2.2500 (2.1705)\tAccu 0.2188 (0.2354)\t\n",
            "Epoch: [9][16/44]\tLoss 2.1579 (2.1697)\tAccu 0.1875 (0.2324)\t\n",
            "Epoch: [9][17/44]\tLoss 2.0866 (2.1648)\tAccu 0.3438 (0.2390)\t\n",
            "Epoch: [9][18/44]\tLoss 2.1664 (2.1649)\tAccu 0.2188 (0.2378)\t\n",
            "Epoch: [9][19/44]\tLoss 2.2230 (2.1679)\tAccu 0.2188 (0.2368)\t\n",
            "Epoch: [9][20/44]\tLoss 2.1597 (2.1675)\tAccu 0.1875 (0.2344)\t\n",
            "Epoch: [9][21/44]\tLoss 2.3270 (2.1751)\tAccu 0.1562 (0.2307)\t\n",
            "Epoch: [9][22/44]\tLoss 2.1619 (2.1745)\tAccu 0.1562 (0.2273)\t\n",
            "Epoch: [9][23/44]\tLoss 2.2000 (2.1756)\tAccu 0.1875 (0.2255)\t\n",
            "Epoch: [9][24/44]\tLoss 2.2409 (2.1784)\tAccu 0.2188 (0.2253)\t\n",
            "Epoch: [9][25/44]\tLoss 2.1891 (2.1788)\tAccu 0.1875 (0.2238)\t\n",
            "Epoch: [9][26/44]\tLoss 2.1288 (2.1769)\tAccu 0.2188 (0.2236)\t\n",
            "Epoch: [9][27/44]\tLoss 2.2373 (2.1791)\tAccu 0.2188 (0.2234)\t\n",
            "Epoch: [9][28/44]\tLoss 2.1699 (2.1788)\tAccu 0.1562 (0.2210)\t\n",
            "Epoch: [9][29/44]\tLoss 2.3590 (2.1850)\tAccu 0.0938 (0.2166)\t\n",
            "Epoch: [9][30/44]\tLoss 2.1737 (2.1846)\tAccu 0.2812 (0.2188)\t\n",
            "Epoch: [9][31/44]\tLoss 2.1964 (2.1850)\tAccu 0.1562 (0.2167)\t\n",
            "Epoch: [9][32/44]\tLoss 2.0637 (2.1812)\tAccu 0.2500 (0.2178)\t\n",
            "Epoch: [9][33/44]\tLoss 2.1438 (2.1801)\tAccu 0.2500 (0.2188)\t\n",
            "Epoch: [9][34/44]\tLoss 1.9996 (2.1748)\tAccu 0.3125 (0.2215)\t\n",
            "Epoch: [9][35/44]\tLoss 2.0097 (2.1700)\tAccu 0.3125 (0.2241)\t\n",
            "Epoch: [9][36/44]\tLoss 2.3604 (2.1753)\tAccu 0.1562 (0.2222)\t\n",
            "Epoch: [9][37/44]\tLoss 2.0425 (2.1717)\tAccu 0.3125 (0.2247)\t\n",
            "Epoch: [9][38/44]\tLoss 2.1157 (2.1703)\tAccu 0.1875 (0.2237)\t\n",
            "Epoch: [9][39/44]\tLoss 2.1004 (2.1685)\tAccu 0.3438 (0.2268)\t\n",
            "Epoch: [9][40/44]\tLoss 2.1531 (2.1681)\tAccu 0.1875 (0.2258)\t\n",
            "Epoch: [9][41/44]\tLoss 2.1591 (2.1679)\tAccu 0.1875 (0.2248)\t\n",
            "Epoch: [9][42/44]\tLoss 2.0495 (2.1651)\tAccu 0.2812 (0.2262)\t\n",
            "Epoch: [9][43/44]\tLoss 2.2378 (2.1667)\tAccu 0.3125 (0.2282)\t\n",
            "Epoch: [9][44/44]\tLoss 2.0546 (2.1648)\tAccu 0.2500 (0.2287)\t\n",
            "Accu 0.1575\t\n",
            "Epoch: [10][1/44]\tLoss 2.2210 (2.2210)\tAccu 0.2812 (0.2812)\t\n",
            "Epoch: [10][2/44]\tLoss 2.1110 (2.1660)\tAccu 0.2812 (0.2812)\t\n",
            "Epoch: [10][3/44]\tLoss 2.2321 (2.1880)\tAccu 0.2812 (0.2812)\t\n",
            "Epoch: [10][4/44]\tLoss 2.2078 (2.1930)\tAccu 0.1875 (0.2578)\t\n",
            "Epoch: [10][5/44]\tLoss 2.2314 (2.2007)\tAccu 0.1875 (0.2437)\t\n",
            "Epoch: [10][6/44]\tLoss 1.9355 (2.1565)\tAccu 0.4375 (0.2760)\t\n",
            "Epoch: [10][7/44]\tLoss 2.1905 (2.1613)\tAccu 0.1875 (0.2634)\t\n",
            "Epoch: [10][8/44]\tLoss 2.0470 (2.1470)\tAccu 0.3438 (0.2734)\t\n",
            "Epoch: [10][9/44]\tLoss 2.1741 (2.1501)\tAccu 0.2500 (0.2708)\t\n",
            "Epoch: [10][10/44]\tLoss 2.1695 (2.1520)\tAccu 0.2188 (0.2656)\t\n",
            "Epoch: [10][11/44]\tLoss 2.1551 (2.1523)\tAccu 0.2500 (0.2642)\t\n",
            "Epoch: [10][12/44]\tLoss 2.1102 (2.1488)\tAccu 0.3125 (0.2682)\t\n",
            "Epoch: [10][13/44]\tLoss 2.1581 (2.1495)\tAccu 0.2500 (0.2668)\t\n",
            "Epoch: [10][14/44]\tLoss 2.0201 (2.1402)\tAccu 0.3438 (0.2723)\t\n",
            "Epoch: [10][15/44]\tLoss 2.2518 (2.1477)\tAccu 0.2188 (0.2687)\t\n",
            "Epoch: [10][16/44]\tLoss 2.1406 (2.1472)\tAccu 0.1875 (0.2637)\t\n",
            "Epoch: [10][17/44]\tLoss 2.0555 (2.1418)\tAccu 0.3125 (0.2665)\t\n",
            "Epoch: [10][18/44]\tLoss 2.1412 (2.1418)\tAccu 0.2188 (0.2639)\t\n",
            "Epoch: [10][19/44]\tLoss 2.2042 (2.1451)\tAccu 0.2188 (0.2615)\t\n",
            "Epoch: [10][20/44]\tLoss 2.1501 (2.1453)\tAccu 0.2188 (0.2594)\t\n",
            "Epoch: [10][21/44]\tLoss 2.3210 (2.1537)\tAccu 0.1875 (0.2560)\t\n",
            "Epoch: [10][22/44]\tLoss 2.1409 (2.1531)\tAccu 0.1562 (0.2514)\t\n",
            "Epoch: [10][23/44]\tLoss 2.1787 (2.1542)\tAccu 0.1875 (0.2486)\t\n",
            "Epoch: [10][24/44]\tLoss 2.2196 (2.1570)\tAccu 0.2500 (0.2487)\t\n",
            "Epoch: [10][25/44]\tLoss 2.1660 (2.1573)\tAccu 0.1562 (0.2450)\t\n",
            "Epoch: [10][26/44]\tLoss 2.0913 (2.1548)\tAccu 0.2188 (0.2440)\t\n",
            "Epoch: [10][27/44]\tLoss 2.2155 (2.1570)\tAccu 0.2188 (0.2431)\t\n",
            "Epoch: [10][28/44]\tLoss 2.1495 (2.1568)\tAccu 0.1562 (0.2400)\t\n",
            "Epoch: [10][29/44]\tLoss 2.3501 (2.1634)\tAccu 0.0938 (0.2349)\t\n",
            "Epoch: [10][30/44]\tLoss 2.1612 (2.1634)\tAccu 0.3125 (0.2375)\t\n",
            "Epoch: [10][31/44]\tLoss 2.1793 (2.1639)\tAccu 0.1875 (0.2359)\t\n",
            "Epoch: [10][32/44]\tLoss 2.0204 (2.1594)\tAccu 0.3438 (0.2393)\t\n",
            "Epoch: [10][33/44]\tLoss 2.1204 (2.1582)\tAccu 0.2500 (0.2396)\t\n",
            "Epoch: [10][34/44]\tLoss 1.9703 (2.1527)\tAccu 0.3125 (0.2417)\t\n",
            "Epoch: [10][35/44]\tLoss 1.9841 (2.1479)\tAccu 0.3125 (0.2437)\t\n",
            "Epoch: [10][36/44]\tLoss 2.3655 (2.1539)\tAccu 0.1562 (0.2413)\t\n",
            "Epoch: [10][37/44]\tLoss 2.0142 (2.1501)\tAccu 0.3125 (0.2432)\t\n",
            "Epoch: [10][38/44]\tLoss 2.0966 (2.1487)\tAccu 0.2188 (0.2426)\t\n",
            "Epoch: [10][39/44]\tLoss 2.0695 (2.1467)\tAccu 0.3750 (0.2460)\t\n",
            "Epoch: [10][40/44]\tLoss 2.1222 (2.1461)\tAccu 0.2500 (0.2461)\t\n",
            "Epoch: [10][41/44]\tLoss 2.1374 (2.1459)\tAccu 0.2500 (0.2462)\t\n",
            "Epoch: [10][42/44]\tLoss 2.0147 (2.1427)\tAccu 0.3125 (0.2478)\t\n",
            "Epoch: [10][43/44]\tLoss 2.2128 (2.1444)\tAccu 0.3125 (0.2493)\t\n",
            "Epoch: [10][44/44]\tLoss 2.0342 (2.1425)\tAccu 0.2500 (0.2493)\t\n",
            "Accu 0.1600\t\n",
            "Epoch: [11][1/44]\tLoss 2.2023 (2.2023)\tAccu 0.2812 (0.2812)\t\n",
            "Epoch: [11][2/44]\tLoss 2.0803 (2.1413)\tAccu 0.2188 (0.2500)\t\n",
            "Epoch: [11][3/44]\tLoss 2.2180 (2.1669)\tAccu 0.2812 (0.2604)\t\n",
            "Epoch: [11][4/44]\tLoss 2.1864 (2.1718)\tAccu 0.1875 (0.2422)\t\n",
            "Epoch: [11][5/44]\tLoss 2.2186 (2.1811)\tAccu 0.1875 (0.2313)\t\n",
            "Epoch: [11][6/44]\tLoss 1.8871 (2.1321)\tAccu 0.4688 (0.2708)\t\n",
            "Epoch: [11][7/44]\tLoss 2.1660 (2.1370)\tAccu 0.2188 (0.2634)\t\n",
            "Epoch: [11][8/44]\tLoss 2.0086 (2.1209)\tAccu 0.3125 (0.2695)\t\n",
            "Epoch: [11][9/44]\tLoss 2.1464 (2.1238)\tAccu 0.2812 (0.2708)\t\n",
            "Epoch: [11][10/44]\tLoss 2.1580 (2.1272)\tAccu 0.2500 (0.2687)\t\n",
            "Epoch: [11][11/44]\tLoss 2.1378 (2.1281)\tAccu 0.2500 (0.2670)\t\n",
            "Epoch: [11][12/44]\tLoss 2.0789 (2.1240)\tAccu 0.3125 (0.2708)\t\n",
            "Epoch: [11][13/44]\tLoss 2.1352 (2.1249)\tAccu 0.2188 (0.2668)\t\n",
            "Epoch: [11][14/44]\tLoss 1.9974 (2.1158)\tAccu 0.3438 (0.2723)\t\n",
            "Epoch: [11][15/44]\tLoss 2.2513 (2.1248)\tAccu 0.2188 (0.2687)\t\n",
            "Epoch: [11][16/44]\tLoss 2.1246 (2.1248)\tAccu 0.2188 (0.2656)\t\n",
            "Epoch: [11][17/44]\tLoss 2.0266 (2.1190)\tAccu 0.3438 (0.2702)\t\n",
            "Epoch: [11][18/44]\tLoss 2.1152 (2.1188)\tAccu 0.2500 (0.2691)\t\n",
            "Epoch: [11][19/44]\tLoss 2.1850 (2.1223)\tAccu 0.2188 (0.2664)\t\n",
            "Epoch: [11][20/44]\tLoss 2.1431 (2.1233)\tAccu 0.2500 (0.2656)\t\n",
            "Epoch: [11][21/44]\tLoss 2.3110 (2.1323)\tAccu 0.2188 (0.2634)\t\n",
            "Epoch: [11][22/44]\tLoss 2.1182 (2.1316)\tAccu 0.1562 (0.2585)\t\n",
            "Epoch: [11][23/44]\tLoss 2.1515 (2.1325)\tAccu 0.1562 (0.2541)\t\n",
            "Epoch: [11][24/44]\tLoss 2.1959 (2.1351)\tAccu 0.2500 (0.2539)\t\n",
            "Epoch: [11][25/44]\tLoss 2.1408 (2.1354)\tAccu 0.1562 (0.2500)\t\n",
            "Epoch: [11][26/44]\tLoss 2.0504 (2.1321)\tAccu 0.2500 (0.2500)\t\n",
            "Epoch: [11][27/44]\tLoss 2.1866 (2.1341)\tAccu 0.2188 (0.2488)\t\n",
            "Epoch: [11][28/44]\tLoss 2.1264 (2.1338)\tAccu 0.1875 (0.2467)\t\n",
            "Epoch: [11][29/44]\tLoss 2.3358 (2.1408)\tAccu 0.1250 (0.2425)\t\n",
            "Epoch: [11][30/44]\tLoss 2.1500 (2.1411)\tAccu 0.2812 (0.2437)\t\n",
            "Epoch: [11][31/44]\tLoss 2.1631 (2.1418)\tAccu 0.1562 (0.2409)\t\n",
            "Epoch: [11][32/44]\tLoss 1.9766 (2.1367)\tAccu 0.3438 (0.2441)\t\n",
            "Epoch: [11][33/44]\tLoss 2.0982 (2.1355)\tAccu 0.2188 (0.2434)\t\n",
            "Epoch: [11][34/44]\tLoss 1.9472 (2.1300)\tAccu 0.3438 (0.2463)\t\n",
            "Epoch: [11][35/44]\tLoss 1.9623 (2.1252)\tAccu 0.3438 (0.2491)\t\n",
            "Epoch: [11][36/44]\tLoss 2.3710 (2.1320)\tAccu 0.1562 (0.2465)\t\n",
            "Epoch: [11][37/44]\tLoss 1.9905 (2.1282)\tAccu 0.3125 (0.2483)\t\n",
            "Epoch: [11][38/44]\tLoss 2.0798 (2.1269)\tAccu 0.2500 (0.2484)\t\n",
            "Epoch: [11][39/44]\tLoss 2.0400 (2.1247)\tAccu 0.4062 (0.2524)\t\n",
            "Epoch: [11][40/44]\tLoss 2.0898 (2.1238)\tAccu 0.2812 (0.2531)\t\n",
            "Epoch: [11][41/44]\tLoss 2.1144 (2.1236)\tAccu 0.3438 (0.2553)\t\n",
            "Epoch: [11][42/44]\tLoss 1.9809 (2.1202)\tAccu 0.3125 (0.2567)\t\n",
            "Epoch: [11][43/44]\tLoss 2.1831 (2.1216)\tAccu 0.3438 (0.2587)\t\n",
            "Epoch: [11][44/44]\tLoss 2.0184 (2.1199)\tAccu 0.2917 (0.2595)\t\n",
            "Accu 0.1675\t\n",
            "Epoch: [12][1/44]\tLoss 2.1802 (2.1802)\tAccu 0.3125 (0.3125)\t\n",
            "Epoch: [12][2/44]\tLoss 2.0505 (2.1154)\tAccu 0.2812 (0.2969)\t\n",
            "Epoch: [12][3/44]\tLoss 2.2021 (2.1443)\tAccu 0.3125 (0.3021)\t\n",
            "Epoch: [12][4/44]\tLoss 2.1646 (2.1494)\tAccu 0.2812 (0.2969)\t\n",
            "Epoch: [12][5/44]\tLoss 2.2048 (2.1604)\tAccu 0.2188 (0.2812)\t\n",
            "Epoch: [12][6/44]\tLoss 1.8443 (2.1078)\tAccu 0.4375 (0.3073)\t\n",
            "Epoch: [12][7/44]\tLoss 2.1371 (2.1119)\tAccu 0.2188 (0.2946)\t\n",
            "Epoch: [12][8/44]\tLoss 1.9713 (2.0944)\tAccu 0.3438 (0.3008)\t\n",
            "Epoch: [12][9/44]\tLoss 2.1139 (2.0965)\tAccu 0.3438 (0.3056)\t\n",
            "Epoch: [12][10/44]\tLoss 2.1456 (2.1014)\tAccu 0.2500 (0.3000)\t\n",
            "Epoch: [12][11/44]\tLoss 2.1200 (2.1031)\tAccu 0.2500 (0.2955)\t\n",
            "Epoch: [12][12/44]\tLoss 2.0442 (2.0982)\tAccu 0.3125 (0.2969)\t\n",
            "Epoch: [12][13/44]\tLoss 2.1079 (2.0990)\tAccu 0.2812 (0.2957)\t\n",
            "Epoch: [12][14/44]\tLoss 1.9761 (2.0902)\tAccu 0.3438 (0.2991)\t\n",
            "Epoch: [12][15/44]\tLoss 2.2456 (2.1005)\tAccu 0.2188 (0.2938)\t\n",
            "Epoch: [12][16/44]\tLoss 2.1089 (2.1011)\tAccu 0.2188 (0.2891)\t\n",
            "Epoch: [12][17/44]\tLoss 1.9985 (2.0950)\tAccu 0.3750 (0.2941)\t\n",
            "Epoch: [12][18/44]\tLoss 2.0896 (2.0947)\tAccu 0.2500 (0.2917)\t\n",
            "Epoch: [12][19/44]\tLoss 2.1638 (2.0984)\tAccu 0.2188 (0.2878)\t\n",
            "Epoch: [12][20/44]\tLoss 2.1391 (2.1004)\tAccu 0.2188 (0.2844)\t\n",
            "Epoch: [12][21/44]\tLoss 2.2950 (2.1097)\tAccu 0.1875 (0.2798)\t\n",
            "Epoch: [12][22/44]\tLoss 2.0934 (2.1089)\tAccu 0.1562 (0.2741)\t\n",
            "Epoch: [12][23/44]\tLoss 2.1179 (2.1093)\tAccu 0.1875 (0.2704)\t\n",
            "Epoch: [12][24/44]\tLoss 2.1688 (2.1118)\tAccu 0.2500 (0.2695)\t\n",
            "Epoch: [12][25/44]\tLoss 2.1146 (2.1119)\tAccu 0.1875 (0.2662)\t\n",
            "Epoch: [12][26/44]\tLoss 2.0042 (2.1078)\tAccu 0.2500 (0.2656)\t\n",
            "Epoch: [12][27/44]\tLoss 2.1520 (2.1094)\tAccu 0.2188 (0.2639)\t\n",
            "Epoch: [12][28/44]\tLoss 2.1002 (2.1091)\tAccu 0.1562 (0.2600)\t\n",
            "Epoch: [12][29/44]\tLoss 2.3166 (2.1162)\tAccu 0.1250 (0.2554)\t\n",
            "Epoch: [12][30/44]\tLoss 2.1368 (2.1169)\tAccu 0.2812 (0.2562)\t\n",
            "Epoch: [12][31/44]\tLoss 2.1475 (2.1179)\tAccu 0.1250 (0.2520)\t\n",
            "Epoch: [12][32/44]\tLoss 1.9310 (2.1121)\tAccu 0.3125 (0.2539)\t\n",
            "Epoch: [12][33/44]\tLoss 2.0761 (2.1110)\tAccu 0.2500 (0.2538)\t\n",
            "Epoch: [12][34/44]\tLoss 1.9304 (2.1057)\tAccu 0.4375 (0.2592)\t\n",
            "Epoch: [12][35/44]\tLoss 1.9426 (2.1010)\tAccu 0.3438 (0.2616)\t\n",
            "Epoch: [12][36/44]\tLoss 2.3754 (2.1086)\tAccu 0.1562 (0.2587)\t\n",
            "Epoch: [12][37/44]\tLoss 1.9697 (2.1049)\tAccu 0.3125 (0.2601)\t\n",
            "Epoch: [12][38/44]\tLoss 2.0644 (2.1038)\tAccu 0.2500 (0.2599)\t\n",
            "Epoch: [12][39/44]\tLoss 2.0114 (2.1014)\tAccu 0.3438 (0.2620)\t\n",
            "Epoch: [12][40/44]\tLoss 2.0549 (2.1003)\tAccu 0.2500 (0.2617)\t\n",
            "Epoch: [12][41/44]\tLoss 2.0885 (2.1000)\tAccu 0.2812 (0.2622)\t\n",
            "Epoch: [12][42/44]\tLoss 1.9467 (2.0963)\tAccu 0.3125 (0.2634)\t\n",
            "Epoch: [12][43/44]\tLoss 2.1481 (2.0975)\tAccu 0.3750 (0.2660)\t\n",
            "Epoch: [12][44/44]\tLoss 2.0124 (2.0961)\tAccu 0.2500 (0.2656)\t\n",
            "Accu 0.1700\t\n",
            "Epoch: [13][1/44]\tLoss 2.1539 (2.1539)\tAccu 0.3125 (0.3125)\t\n",
            "Epoch: [13][2/44]\tLoss 2.0210 (2.0875)\tAccu 0.3125 (0.3125)\t\n",
            "Epoch: [13][3/44]\tLoss 2.1856 (2.1202)\tAccu 0.3125 (0.3125)\t\n",
            "Epoch: [13][4/44]\tLoss 2.1443 (2.1262)\tAccu 0.2812 (0.3047)\t\n",
            "Epoch: [13][5/44]\tLoss 2.1897 (2.1389)\tAccu 0.2188 (0.2875)\t\n",
            "Epoch: [13][6/44]\tLoss 1.8065 (2.0835)\tAccu 0.5312 (0.3281)\t\n",
            "Epoch: [13][7/44]\tLoss 2.1015 (2.0861)\tAccu 0.2188 (0.3125)\t\n",
            "Epoch: [13][8/44]\tLoss 1.9339 (2.0671)\tAccu 0.3438 (0.3164)\t\n",
            "Epoch: [13][9/44]\tLoss 2.0762 (2.0681)\tAccu 0.3438 (0.3194)\t\n",
            "Epoch: [13][10/44]\tLoss 2.1304 (2.0743)\tAccu 0.2500 (0.3125)\t\n",
            "Epoch: [13][11/44]\tLoss 2.1001 (2.0766)\tAccu 0.2500 (0.3068)\t\n",
            "Epoch: [13][12/44]\tLoss 2.0061 (2.0708)\tAccu 0.3750 (0.3125)\t\n",
            "Epoch: [13][13/44]\tLoss 2.0747 (2.0711)\tAccu 0.2812 (0.3101)\t\n",
            "Epoch: [13][14/44]\tLoss 1.9556 (2.0628)\tAccu 0.3125 (0.3103)\t\n",
            "Epoch: [13][15/44]\tLoss 2.2329 (2.0742)\tAccu 0.2500 (0.3063)\t\n",
            "Epoch: [13][16/44]\tLoss 2.0921 (2.0753)\tAccu 0.2500 (0.3027)\t\n",
            "Epoch: [13][17/44]\tLoss 1.9706 (2.0691)\tAccu 0.3750 (0.3070)\t\n",
            "Epoch: [13][18/44]\tLoss 2.0651 (2.0689)\tAccu 0.2500 (0.3038)\t\n",
            "Epoch: [13][19/44]\tLoss 2.1415 (2.0727)\tAccu 0.2500 (0.3010)\t\n",
            "Epoch: [13][20/44]\tLoss 2.1360 (2.0759)\tAccu 0.2188 (0.2969)\t\n",
            "Epoch: [13][21/44]\tLoss 2.2714 (2.0852)\tAccu 0.1250 (0.2887)\t\n",
            "Epoch: [13][22/44]\tLoss 2.0688 (2.0845)\tAccu 0.1250 (0.2812)\t\n",
            "Epoch: [13][23/44]\tLoss 2.0805 (2.0843)\tAccu 0.1875 (0.2772)\t\n",
            "Epoch: [13][24/44]\tLoss 2.1366 (2.0865)\tAccu 0.2188 (0.2747)\t\n",
            "Epoch: [13][25/44]\tLoss 2.0877 (2.0865)\tAccu 0.1875 (0.2712)\t\n",
            "Epoch: [13][26/44]\tLoss 1.9521 (2.0813)\tAccu 0.4062 (0.2764)\t\n",
            "Epoch: [13][27/44]\tLoss 2.1149 (2.0826)\tAccu 0.2188 (0.2743)\t\n",
            "Epoch: [13][28/44]\tLoss 2.0702 (2.0821)\tAccu 0.1875 (0.2712)\t\n",
            "Epoch: [13][29/44]\tLoss 2.2896 (2.0893)\tAccu 0.1250 (0.2662)\t\n",
            "Epoch: [13][30/44]\tLoss 2.1145 (2.0901)\tAccu 0.2500 (0.2656)\t\n",
            "Epoch: [13][31/44]\tLoss 2.1278 (2.0914)\tAccu 0.1250 (0.2611)\t\n",
            "Epoch: [13][32/44]\tLoss 1.8841 (2.0849)\tAccu 0.3438 (0.2637)\t\n",
            "Epoch: [13][33/44]\tLoss 2.0524 (2.0839)\tAccu 0.2500 (0.2633)\t\n",
            "Epoch: [13][34/44]\tLoss 1.9186 (2.0790)\tAccu 0.3750 (0.2665)\t\n",
            "Epoch: [13][35/44]\tLoss 1.9214 (2.0745)\tAccu 0.3750 (0.2696)\t\n",
            "Epoch: [13][36/44]\tLoss 2.3715 (2.0828)\tAccu 0.1250 (0.2656)\t\n",
            "Epoch: [13][37/44]\tLoss 1.9500 (2.0792)\tAccu 0.3438 (0.2677)\t\n",
            "Epoch: [13][38/44]\tLoss 2.0479 (2.0784)\tAccu 0.2500 (0.2673)\t\n",
            "Epoch: [13][39/44]\tLoss 1.9830 (2.0759)\tAccu 0.3438 (0.2692)\t\n",
            "Epoch: [13][40/44]\tLoss 2.0172 (2.0745)\tAccu 0.3125 (0.2703)\t\n",
            "Epoch: [13][41/44]\tLoss 2.0591 (2.0741)\tAccu 0.2812 (0.2706)\t\n",
            "Epoch: [13][42/44]\tLoss 1.9109 (2.0702)\tAccu 0.3750 (0.2731)\t\n",
            "Epoch: [13][43/44]\tLoss 2.1092 (2.0711)\tAccu 0.4062 (0.2762)\t\n",
            "Epoch: [13][44/44]\tLoss 2.0055 (2.0700)\tAccu 0.3333 (0.2775)\t\n",
            "Accu 0.1775\t\n",
            "Epoch: [14][1/44]\tLoss 2.1259 (2.1259)\tAccu 0.3125 (0.3125)\t\n",
            "Epoch: [14][2/44]\tLoss 1.9881 (2.0570)\tAccu 0.3125 (0.3125)\t\n",
            "Epoch: [14][3/44]\tLoss 2.1666 (2.0935)\tAccu 0.2500 (0.2917)\t\n",
            "Epoch: [14][4/44]\tLoss 2.1286 (2.1023)\tAccu 0.2812 (0.2891)\t\n",
            "Epoch: [14][5/44]\tLoss 2.1702 (2.1159)\tAccu 0.2188 (0.2750)\t\n",
            "Epoch: [14][6/44]\tLoss 1.7776 (2.0595)\tAccu 0.4688 (0.3073)\t\n",
            "Epoch: [14][7/44]\tLoss 2.0582 (2.0593)\tAccu 0.2812 (0.3036)\t\n",
            "Epoch: [14][8/44]\tLoss 1.8957 (2.0389)\tAccu 0.3125 (0.3047)\t\n",
            "Epoch: [14][9/44]\tLoss 2.0336 (2.0383)\tAccu 0.3750 (0.3125)\t\n",
            "Epoch: [14][10/44]\tLoss 2.1090 (2.0453)\tAccu 0.1875 (0.3000)\t\n",
            "Epoch: [14][11/44]\tLoss 2.0721 (2.0478)\tAccu 0.2500 (0.2955)\t\n",
            "Epoch: [14][12/44]\tLoss 1.9642 (2.0408)\tAccu 0.3750 (0.3021)\t\n",
            "Epoch: [14][13/44]\tLoss 2.0343 (2.0403)\tAccu 0.3125 (0.3029)\t\n",
            "Epoch: [14][14/44]\tLoss 1.9346 (2.0328)\tAccu 0.3438 (0.3058)\t\n",
            "Epoch: [14][15/44]\tLoss 2.2117 (2.0447)\tAccu 0.2812 (0.3042)\t\n",
            "Epoch: [14][16/44]\tLoss 2.0699 (2.0463)\tAccu 0.2812 (0.3027)\t\n",
            "Epoch: [14][17/44]\tLoss 1.9390 (2.0400)\tAccu 0.3438 (0.3051)\t\n",
            "Epoch: [14][18/44]\tLoss 2.0404 (2.0400)\tAccu 0.2188 (0.3003)\t\n",
            "Epoch: [14][19/44]\tLoss 2.1165 (2.0440)\tAccu 0.2500 (0.2977)\t\n",
            "Epoch: [14][20/44]\tLoss 2.1350 (2.0485)\tAccu 0.2812 (0.2969)\t\n",
            "Epoch: [14][21/44]\tLoss 2.2374 (2.0575)\tAccu 0.0938 (0.2872)\t\n",
            "Epoch: [14][22/44]\tLoss 2.0416 (2.0568)\tAccu 0.1562 (0.2812)\t\n",
            "Epoch: [14][23/44]\tLoss 2.0397 (2.0561)\tAccu 0.1250 (0.2745)\t\n",
            "Epoch: [14][24/44]\tLoss 2.1002 (2.0579)\tAccu 0.2500 (0.2734)\t\n",
            "Epoch: [14][25/44]\tLoss 2.0610 (2.0580)\tAccu 0.1875 (0.2700)\t\n",
            "Epoch: [14][26/44]\tLoss 1.9000 (2.0520)\tAccu 0.4375 (0.2764)\t\n",
            "Epoch: [14][27/44]\tLoss 2.0770 (2.0529)\tAccu 0.1875 (0.2731)\t\n",
            "Epoch: [14][28/44]\tLoss 2.0337 (2.0522)\tAccu 0.2812 (0.2734)\t\n",
            "Epoch: [14][29/44]\tLoss 2.2541 (2.0592)\tAccu 0.1562 (0.2694)\t\n",
            "Epoch: [14][30/44]\tLoss 2.0827 (2.0599)\tAccu 0.2500 (0.2687)\t\n",
            "Epoch: [14][31/44]\tLoss 2.0991 (2.0612)\tAccu 0.1562 (0.2651)\t\n",
            "Epoch: [14][32/44]\tLoss 1.8361 (2.0542)\tAccu 0.4375 (0.2705)\t\n",
            "Epoch: [14][33/44]\tLoss 2.0228 (2.0532)\tAccu 0.2812 (0.2708)\t\n",
            "Epoch: [14][34/44]\tLoss 1.9046 (2.0489)\tAccu 0.3438 (0.2730)\t\n",
            "Epoch: [14][35/44]\tLoss 1.8938 (2.0444)\tAccu 0.3750 (0.2759)\t\n",
            "Epoch: [14][36/44]\tLoss 2.3589 (2.0532)\tAccu 0.1250 (0.2717)\t\n",
            "Epoch: [14][37/44]\tLoss 1.9270 (2.0498)\tAccu 0.3438 (0.2736)\t\n",
            "Epoch: [14][38/44]\tLoss 2.0279 (2.0492)\tAccu 0.2812 (0.2738)\t\n",
            "Epoch: [14][39/44]\tLoss 1.9537 (2.0467)\tAccu 0.3125 (0.2748)\t\n",
            "Epoch: [14][40/44]\tLoss 1.9772 (2.0450)\tAccu 0.3750 (0.2773)\t\n",
            "Epoch: [14][41/44]\tLoss 2.0264 (2.0445)\tAccu 0.2812 (0.2774)\t\n",
            "Epoch: [14][42/44]\tLoss 1.8712 (2.0404)\tAccu 0.4688 (0.2820)\t\n",
            "Epoch: [14][43/44]\tLoss 2.0686 (2.0411)\tAccu 0.4062 (0.2849)\t\n",
            "Epoch: [14][44/44]\tLoss 1.9789 (2.0400)\tAccu 0.3333 (0.2860)\t\n",
            "Accu 0.1775\t\n",
            "Epoch: [15][1/44]\tLoss 2.0968 (2.0968)\tAccu 0.2812 (0.2812)\t\n",
            "Epoch: [15][2/44]\tLoss 1.9482 (2.0225)\tAccu 0.2500 (0.2656)\t\n",
            "Epoch: [15][3/44]\tLoss 2.1441 (2.0630)\tAccu 0.2188 (0.2500)\t\n",
            "Epoch: [15][4/44]\tLoss 2.1222 (2.0778)\tAccu 0.2812 (0.2578)\t\n",
            "Epoch: [15][5/44]\tLoss 2.1439 (2.0910)\tAccu 0.2500 (0.2562)\t\n",
            "Epoch: [15][6/44]\tLoss 1.7569 (2.0354)\tAccu 0.5000 (0.2969)\t\n",
            "Epoch: [15][7/44]\tLoss 2.0095 (2.0317)\tAccu 0.3125 (0.2991)\t\n",
            "Epoch: [15][8/44]\tLoss 1.8532 (2.0094)\tAccu 0.2812 (0.2969)\t\n",
            "Epoch: [15][9/44]\tLoss 1.9876 (2.0069)\tAccu 0.3750 (0.3056)\t\n",
            "Epoch: [15][10/44]\tLoss 2.0763 (2.0139)\tAccu 0.2188 (0.2969)\t\n",
            "Epoch: [15][11/44]\tLoss 2.0276 (2.0151)\tAccu 0.3438 (0.3011)\t\n",
            "Epoch: [15][12/44]\tLoss 1.9196 (2.0072)\tAccu 0.3750 (0.3073)\t\n",
            "Epoch: [15][13/44]\tLoss 1.9870 (2.0056)\tAccu 0.3125 (0.3077)\t\n",
            "Epoch: [15][14/44]\tLoss 1.9140 (1.9991)\tAccu 0.3438 (0.3103)\t\n",
            "Epoch: [15][15/44]\tLoss 2.1836 (2.0114)\tAccu 0.2500 (0.3063)\t\n",
            "Epoch: [15][16/44]\tLoss 2.0355 (2.0129)\tAccu 0.3125 (0.3066)\t\n",
            "Epoch: [15][17/44]\tLoss 1.8982 (2.0061)\tAccu 0.3125 (0.3070)\t\n",
            "Epoch: [15][18/44]\tLoss 2.0096 (2.0063)\tAccu 0.2500 (0.3038)\t\n",
            "Epoch: [15][19/44]\tLoss 2.0792 (2.0102)\tAccu 0.2500 (0.3010)\t\n",
            "Epoch: [15][20/44]\tLoss 2.1403 (2.0167)\tAccu 0.2812 (0.3000)\t\n",
            "Epoch: [15][21/44]\tLoss 2.1885 (2.0249)\tAccu 0.1250 (0.2917)\t\n",
            "Epoch: [15][22/44]\tLoss 2.0090 (2.0241)\tAccu 0.2500 (0.2898)\t\n",
            "Epoch: [15][23/44]\tLoss 1.9962 (2.0229)\tAccu 0.1562 (0.2840)\t\n",
            "Epoch: [15][24/44]\tLoss 2.0593 (2.0244)\tAccu 0.3125 (0.2852)\t\n",
            "Epoch: [15][25/44]\tLoss 2.0323 (2.0248)\tAccu 0.1875 (0.2812)\t\n",
            "Epoch: [15][26/44]\tLoss 1.8566 (2.0183)\tAccu 0.5000 (0.2897)\t\n",
            "Epoch: [15][27/44]\tLoss 2.0357 (2.0189)\tAccu 0.2188 (0.2870)\t\n",
            "Epoch: [15][28/44]\tLoss 1.9913 (2.0179)\tAccu 0.2500 (0.2857)\t\n",
            "Epoch: [15][29/44]\tLoss 2.2109 (2.0246)\tAccu 0.1875 (0.2823)\t\n",
            "Epoch: [15][30/44]\tLoss 2.0466 (2.0253)\tAccu 0.2812 (0.2823)\t\n",
            "Epoch: [15][31/44]\tLoss 2.0597 (2.0264)\tAccu 0.2188 (0.2802)\t\n",
            "Epoch: [15][32/44]\tLoss 1.7894 (2.0190)\tAccu 0.4375 (0.2852)\t\n",
            "Epoch: [15][33/44]\tLoss 1.9884 (2.0181)\tAccu 0.2812 (0.2850)\t\n",
            "Epoch: [15][34/44]\tLoss 1.8838 (2.0142)\tAccu 0.3125 (0.2858)\t\n",
            "Epoch: [15][35/44]\tLoss 1.8562 (2.0096)\tAccu 0.4062 (0.2893)\t\n",
            "Epoch: [15][36/44]\tLoss 2.3431 (2.0189)\tAccu 0.1875 (0.2865)\t\n",
            "Epoch: [15][37/44]\tLoss 1.8995 (2.0157)\tAccu 0.4062 (0.2897)\t\n",
            "Epoch: [15][38/44]\tLoss 1.9936 (2.0151)\tAccu 0.2812 (0.2895)\t\n",
            "Epoch: [15][39/44]\tLoss 1.9208 (2.0127)\tAccu 0.3125 (0.2901)\t\n",
            "Epoch: [15][40/44]\tLoss 1.9307 (2.0106)\tAccu 0.4375 (0.2938)\t\n",
            "Epoch: [15][41/44]\tLoss 1.9871 (2.0101)\tAccu 0.2812 (0.2934)\t\n",
            "Epoch: [15][42/44]\tLoss 1.8211 (2.0056)\tAccu 0.5312 (0.2991)\t\n",
            "Epoch: [15][43/44]\tLoss 2.0268 (2.0061)\tAccu 0.4375 (0.3023)\t\n",
            "Epoch: [15][44/44]\tLoss 1.9348 (2.0048)\tAccu 0.3750 (0.3040)\t\n",
            "Accu 0.1725\t\n",
            "Epoch: [16][1/44]\tLoss 2.0648 (2.0648)\tAccu 0.2188 (0.2188)\t\n",
            "Epoch: [16][2/44]\tLoss 1.9020 (1.9834)\tAccu 0.3438 (0.2812)\t\n",
            "Epoch: [16][3/44]\tLoss 2.1182 (2.0284)\tAccu 0.2500 (0.2708)\t\n",
            "Epoch: [16][4/44]\tLoss 2.1223 (2.0519)\tAccu 0.2500 (0.2656)\t\n",
            "Epoch: [16][5/44]\tLoss 2.1093 (2.0633)\tAccu 0.3125 (0.2750)\t\n",
            "Epoch: [16][6/44]\tLoss 1.7321 (2.0081)\tAccu 0.5000 (0.3125)\t\n",
            "Epoch: [16][7/44]\tLoss 1.9583 (2.0010)\tAccu 0.3125 (0.3125)\t\n",
            "Epoch: [16][8/44]\tLoss 1.8021 (1.9762)\tAccu 0.3125 (0.3125)\t\n",
            "Epoch: [16][9/44]\tLoss 1.9380 (1.9719)\tAccu 0.3750 (0.3194)\t\n",
            "Epoch: [16][10/44]\tLoss 2.0304 (1.9778)\tAccu 0.2500 (0.3125)\t\n",
            "Epoch: [16][11/44]\tLoss 1.9689 (1.9770)\tAccu 0.3750 (0.3182)\t\n",
            "Epoch: [16][12/44]\tLoss 1.8744 (1.9684)\tAccu 0.3750 (0.3229)\t\n",
            "Epoch: [16][13/44]\tLoss 1.9363 (1.9659)\tAccu 0.3125 (0.3221)\t\n",
            "Epoch: [16][14/44]\tLoss 1.8921 (1.9607)\tAccu 0.3438 (0.3237)\t\n",
            "Epoch: [16][15/44]\tLoss 2.1519 (1.9734)\tAccu 0.2500 (0.3187)\t\n",
            "Epoch: [16][16/44]\tLoss 1.9836 (1.9740)\tAccu 0.3438 (0.3203)\t\n",
            "Epoch: [16][17/44]\tLoss 1.8381 (1.9661)\tAccu 0.3125 (0.3199)\t\n",
            "Epoch: [16][18/44]\tLoss 1.9645 (1.9660)\tAccu 0.2500 (0.3160)\t\n",
            "Epoch: [16][19/44]\tLoss 2.0204 (1.9688)\tAccu 0.3438 (0.3174)\t\n",
            "Epoch: [16][20/44]\tLoss 2.1529 (1.9780)\tAccu 0.2812 (0.3156)\t\n",
            "Epoch: [16][21/44]\tLoss 2.1249 (1.9850)\tAccu 0.2188 (0.3110)\t\n",
            "Epoch: [16][22/44]\tLoss 1.9687 (1.9843)\tAccu 0.2500 (0.3082)\t\n",
            "Epoch: [16][23/44]\tLoss 1.9422 (1.9825)\tAccu 0.1562 (0.3016)\t\n",
            "Epoch: [16][24/44]\tLoss 2.0085 (1.9835)\tAccu 0.3125 (0.3021)\t\n",
            "Epoch: [16][25/44]\tLoss 1.9908 (1.9838)\tAccu 0.2812 (0.3013)\t\n",
            "Epoch: [16][26/44]\tLoss 1.8159 (1.9774)\tAccu 0.4688 (0.3077)\t\n",
            "Epoch: [16][27/44]\tLoss 1.9826 (1.9776)\tAccu 0.3125 (0.3079)\t\n",
            "Epoch: [16][28/44]\tLoss 1.9382 (1.9762)\tAccu 0.2812 (0.3069)\t\n",
            "Epoch: [16][29/44]\tLoss 2.1611 (1.9825)\tAccu 0.1875 (0.3028)\t\n",
            "Epoch: [16][30/44]\tLoss 1.9993 (1.9831)\tAccu 0.2812 (0.3021)\t\n",
            "Epoch: [16][31/44]\tLoss 2.0070 (1.9839)\tAccu 0.2812 (0.3014)\t\n",
            "Epoch: [16][32/44]\tLoss 1.7402 (1.9762)\tAccu 0.5312 (0.3086)\t\n",
            "Epoch: [16][33/44]\tLoss 1.9619 (1.9758)\tAccu 0.3125 (0.3087)\t\n",
            "Epoch: [16][34/44]\tLoss 1.8551 (1.9723)\tAccu 0.3750 (0.3107)\t\n",
            "Epoch: [16][35/44]\tLoss 1.8081 (1.9676)\tAccu 0.4062 (0.3134)\t\n",
            "Epoch: [16][36/44]\tLoss 2.3179 (1.9773)\tAccu 0.2188 (0.3108)\t\n",
            "Epoch: [16][37/44]\tLoss 1.8619 (1.9742)\tAccu 0.4375 (0.3142)\t\n",
            "Epoch: [16][38/44]\tLoss 1.9333 (1.9731)\tAccu 0.3438 (0.3150)\t\n",
            "Epoch: [16][39/44]\tLoss 1.8750 (1.9706)\tAccu 0.4062 (0.3173)\t\n",
            "Epoch: [16][40/44]\tLoss 1.8742 (1.9682)\tAccu 0.5000 (0.3219)\t\n",
            "Epoch: [16][41/44]\tLoss 1.9361 (1.9674)\tAccu 0.3750 (0.3232)\t\n",
            "Epoch: [16][42/44]\tLoss 1.7586 (1.9624)\tAccu 0.5312 (0.3281)\t\n",
            "Epoch: [16][43/44]\tLoss 1.9810 (1.9629)\tAccu 0.4375 (0.3307)\t\n",
            "Epoch: [16][44/44]\tLoss 1.8770 (1.9614)\tAccu 0.4167 (0.3326)\t\n",
            "Accu 0.1650\t\n",
            "Epoch: [17][1/44]\tLoss 2.0260 (2.0260)\tAccu 0.3125 (0.3125)\t\n",
            "Epoch: [17][2/44]\tLoss 1.8517 (1.9389)\tAccu 0.3750 (0.3438)\t\n",
            "Epoch: [17][3/44]\tLoss 2.0787 (1.9855)\tAccu 0.2500 (0.3125)\t\n",
            "Epoch: [17][4/44]\tLoss 2.1124 (2.0172)\tAccu 0.2500 (0.2969)\t\n",
            "Epoch: [17][5/44]\tLoss 2.0583 (2.0254)\tAccu 0.3125 (0.3000)\t\n",
            "Epoch: [17][6/44]\tLoss 1.6955 (1.9704)\tAccu 0.5000 (0.3333)\t\n",
            "Epoch: [17][7/44]\tLoss 1.9032 (1.9608)\tAccu 0.3125 (0.3304)\t\n",
            "Epoch: [17][8/44]\tLoss 1.7473 (1.9341)\tAccu 0.3438 (0.3320)\t\n",
            "Epoch: [17][9/44]\tLoss 1.8805 (1.9282)\tAccu 0.3750 (0.3368)\t\n",
            "Epoch: [17][10/44]\tLoss 1.9768 (1.9330)\tAccu 0.2500 (0.3281)\t\n",
            "Epoch: [17][11/44]\tLoss 1.9011 (1.9301)\tAccu 0.4375 (0.3381)\t\n",
            "Epoch: [17][12/44]\tLoss 1.8281 (1.9216)\tAccu 0.3438 (0.3385)\t\n",
            "Epoch: [17][13/44]\tLoss 1.8802 (1.9184)\tAccu 0.3125 (0.3365)\t\n",
            "Epoch: [17][14/44]\tLoss 1.8570 (1.9140)\tAccu 0.3438 (0.3371)\t\n",
            "Epoch: [17][15/44]\tLoss 2.1038 (1.9267)\tAccu 0.2812 (0.3333)\t\n",
            "Epoch: [17][16/44]\tLoss 1.9123 (1.9258)\tAccu 0.3125 (0.3320)\t\n",
            "Epoch: [17][17/44]\tLoss 1.7521 (1.9156)\tAccu 0.3750 (0.3346)\t\n",
            "Epoch: [17][18/44]\tLoss 1.9033 (1.9149)\tAccu 0.3438 (0.3351)\t\n",
            "Epoch: [17][19/44]\tLoss 1.9407 (1.9163)\tAccu 0.3438 (0.3355)\t\n",
            "Epoch: [17][20/44]\tLoss 2.1666 (1.9288)\tAccu 0.3125 (0.3344)\t\n",
            "Epoch: [17][21/44]\tLoss 2.0501 (1.9346)\tAccu 0.2500 (0.3304)\t\n",
            "Epoch: [17][22/44]\tLoss 1.9190 (1.9338)\tAccu 0.3438 (0.3310)\t\n",
            "Epoch: [17][23/44]\tLoss 1.8699 (1.9311)\tAccu 0.2812 (0.3288)\t\n",
            "Epoch: [17][24/44]\tLoss 1.9409 (1.9315)\tAccu 0.3438 (0.3294)\t\n",
            "Epoch: [17][25/44]\tLoss 1.9291 (1.9314)\tAccu 0.3750 (0.3312)\t\n",
            "Epoch: [17][26/44]\tLoss 1.7644 (1.9250)\tAccu 0.4062 (0.3341)\t\n",
            "Epoch: [17][27/44]\tLoss 1.9154 (1.9246)\tAccu 0.3438 (0.3345)\t\n",
            "Epoch: [17][28/44]\tLoss 1.8634 (1.9224)\tAccu 0.4062 (0.3371)\t\n",
            "Epoch: [17][29/44]\tLoss 2.0956 (1.9284)\tAccu 0.2188 (0.3330)\t\n",
            "Epoch: [17][30/44]\tLoss 1.9197 (1.9281)\tAccu 0.3438 (0.3333)\t\n",
            "Epoch: [17][31/44]\tLoss 1.9279 (1.9281)\tAccu 0.3125 (0.3327)\t\n",
            "Epoch: [17][32/44]\tLoss 1.6835 (1.9205)\tAccu 0.5312 (0.3389)\t\n",
            "Epoch: [17][33/44]\tLoss 1.9511 (1.9214)\tAccu 0.3125 (0.3381)\t\n",
            "Epoch: [17][34/44]\tLoss 1.8188 (1.9184)\tAccu 0.3438 (0.3382)\t\n",
            "Epoch: [17][35/44]\tLoss 1.7442 (1.9134)\tAccu 0.4062 (0.3402)\t\n",
            "Epoch: [17][36/44]\tLoss 2.2575 (1.9229)\tAccu 0.2812 (0.3385)\t\n",
            "Epoch: [17][37/44]\tLoss 1.8075 (1.9198)\tAccu 0.4688 (0.3421)\t\n",
            "Epoch: [17][38/44]\tLoss 1.8488 (1.9180)\tAccu 0.4062 (0.3438)\t\n",
            "Epoch: [17][39/44]\tLoss 1.8108 (1.9152)\tAccu 0.5000 (0.3478)\t\n",
            "Epoch: [17][40/44]\tLoss 1.8084 (1.9125)\tAccu 0.5000 (0.3516)\t\n",
            "Epoch: [17][41/44]\tLoss 1.8653 (1.9114)\tAccu 0.3750 (0.3521)\t\n",
            "Epoch: [17][42/44]\tLoss 1.6801 (1.9059)\tAccu 0.6562 (0.3594)\t\n",
            "Epoch: [17][43/44]\tLoss 1.9236 (1.9063)\tAccu 0.4375 (0.3612)\t\n",
            "Epoch: [17][44/44]\tLoss 1.7701 (1.9040)\tAccu 0.5000 (0.3643)\t\n",
            "Accu 0.1675\t\n",
            "Epoch: [18][1/44]\tLoss 1.9746 (1.9746)\tAccu 0.3125 (0.3125)\t\n",
            "Epoch: [18][2/44]\tLoss 1.7868 (1.8807)\tAccu 0.4688 (0.3906)\t\n",
            "Epoch: [18][3/44]\tLoss 2.0102 (1.9239)\tAccu 0.3125 (0.3646)\t\n",
            "Epoch: [18][4/44]\tLoss 2.0750 (1.9616)\tAccu 0.3125 (0.3516)\t\n",
            "Epoch: [18][5/44]\tLoss 1.9789 (1.9651)\tAccu 0.4062 (0.3625)\t\n",
            "Epoch: [18][6/44]\tLoss 1.6341 (1.9099)\tAccu 0.5000 (0.3854)\t\n",
            "Epoch: [18][7/44]\tLoss 1.8459 (1.9008)\tAccu 0.4062 (0.3884)\t\n",
            "Epoch: [18][8/44]\tLoss 1.6770 (1.8728)\tAccu 0.4062 (0.3906)\t\n",
            "Epoch: [18][9/44]\tLoss 1.8076 (1.8655)\tAccu 0.4688 (0.3993)\t\n",
            "Epoch: [18][10/44]\tLoss 1.9115 (1.8701)\tAccu 0.3125 (0.3906)\t\n",
            "Epoch: [18][11/44]\tLoss 1.8190 (1.8655)\tAccu 0.4375 (0.3949)\t\n",
            "Epoch: [18][12/44]\tLoss 1.7714 (1.8576)\tAccu 0.3438 (0.3906)\t\n",
            "Epoch: [18][13/44]\tLoss 1.8142 (1.8543)\tAccu 0.3750 (0.3894)\t\n",
            "Epoch: [18][14/44]\tLoss 1.7769 (1.8488)\tAccu 0.3750 (0.3884)\t\n",
            "Epoch: [18][15/44]\tLoss 2.0272 (1.8607)\tAccu 0.3750 (0.3875)\t\n",
            "Epoch: [18][16/44]\tLoss 1.8134 (1.8577)\tAccu 0.3438 (0.3848)\t\n",
            "Epoch: [18][17/44]\tLoss 1.6339 (1.8445)\tAccu 0.4375 (0.3879)\t\n",
            "Epoch: [18][18/44]\tLoss 1.8343 (1.8440)\tAccu 0.3438 (0.3854)\t\n",
            "Epoch: [18][19/44]\tLoss 1.8459 (1.8441)\tAccu 0.4375 (0.3882)\t\n",
            "Epoch: [18][20/44]\tLoss 2.1600 (1.8599)\tAccu 0.2500 (0.3812)\t\n",
            "Epoch: [18][21/44]\tLoss 1.9583 (1.8646)\tAccu 0.3750 (0.3810)\t\n",
            "Epoch: [18][22/44]\tLoss 1.8562 (1.8642)\tAccu 0.4062 (0.3821)\t\n",
            "Epoch: [18][23/44]\tLoss 1.7734 (1.8602)\tAccu 0.3438 (0.3804)\t\n",
            "Epoch: [18][24/44]\tLoss 1.8534 (1.8600)\tAccu 0.3750 (0.3802)\t\n",
            "Epoch: [18][25/44]\tLoss 1.8467 (1.8594)\tAccu 0.4375 (0.3825)\t\n",
            "Epoch: [18][26/44]\tLoss 1.6889 (1.8529)\tAccu 0.4688 (0.3858)\t\n",
            "Epoch: [18][27/44]\tLoss 1.8293 (1.8520)\tAccu 0.4375 (0.3877)\t\n",
            "Epoch: [18][28/44]\tLoss 1.7427 (1.8481)\tAccu 0.5312 (0.3929)\t\n",
            "Epoch: [18][29/44]\tLoss 1.9906 (1.8530)\tAccu 0.2500 (0.3879)\t\n",
            "Epoch: [18][30/44]\tLoss 1.7781 (1.8505)\tAccu 0.5312 (0.3927)\t\n",
            "Epoch: [18][31/44]\tLoss 1.8134 (1.8493)\tAccu 0.3750 (0.3921)\t\n",
            "Epoch: [18][32/44]\tLoss 1.6224 (1.8422)\tAccu 0.5000 (0.3955)\t\n",
            "Epoch: [18][33/44]\tLoss 1.9311 (1.8449)\tAccu 0.3438 (0.3939)\t\n",
            "Epoch: [18][34/44]\tLoss 1.7621 (1.8425)\tAccu 0.4375 (0.3952)\t\n",
            "Epoch: [18][35/44]\tLoss 1.6664 (1.8374)\tAccu 0.3438 (0.3937)\t\n",
            "Epoch: [18][36/44]\tLoss 2.1707 (1.8467)\tAccu 0.2500 (0.3898)\t\n",
            "Epoch: [18][37/44]\tLoss 1.7352 (1.8437)\tAccu 0.5000 (0.3927)\t\n",
            "Epoch: [18][38/44]\tLoss 1.7318 (1.8407)\tAccu 0.4688 (0.3947)\t\n",
            "Epoch: [18][39/44]\tLoss 1.7158 (1.8375)\tAccu 0.4688 (0.3966)\t\n",
            "Epoch: [18][40/44]\tLoss 1.7501 (1.8354)\tAccu 0.5000 (0.3992)\t\n",
            "Epoch: [18][41/44]\tLoss 1.7639 (1.8336)\tAccu 0.4375 (0.4002)\t\n",
            "Epoch: [18][42/44]\tLoss 1.5948 (1.8279)\tAccu 0.6562 (0.4062)\t\n",
            "Epoch: [18][43/44]\tLoss 1.8355 (1.8281)\tAccu 0.4688 (0.4077)\t\n",
            "Epoch: [18][44/44]\tLoss 1.6022 (1.8242)\tAccu 0.5417 (0.4107)\t\n",
            "Accu 0.1925\t\n",
            "Epoch: [19][1/44]\tLoss 1.8881 (1.8881)\tAccu 0.3125 (0.3125)\t\n",
            "Epoch: [19][2/44]\tLoss 1.6815 (1.7848)\tAccu 0.5000 (0.4062)\t\n",
            "Epoch: [19][3/44]\tLoss 1.9140 (1.8279)\tAccu 0.3125 (0.3750)\t\n",
            "Epoch: [19][4/44]\tLoss 1.9901 (1.8684)\tAccu 0.3438 (0.3672)\t\n",
            "Epoch: [19][5/44]\tLoss 1.8737 (1.8695)\tAccu 0.4375 (0.3812)\t\n",
            "Epoch: [19][6/44]\tLoss 1.5272 (1.8124)\tAccu 0.5000 (0.4010)\t\n",
            "Epoch: [19][7/44]\tLoss 1.7816 (1.8080)\tAccu 0.4062 (0.4018)\t\n",
            "Epoch: [19][8/44]\tLoss 1.6135 (1.7837)\tAccu 0.4062 (0.4023)\t\n",
            "Epoch: [19][9/44]\tLoss 1.6924 (1.7736)\tAccu 0.5000 (0.4132)\t\n",
            "Epoch: [19][10/44]\tLoss 1.7976 (1.7760)\tAccu 0.4062 (0.4125)\t\n",
            "Epoch: [19][11/44]\tLoss 1.7140 (1.7703)\tAccu 0.4688 (0.4176)\t\n",
            "Epoch: [19][12/44]\tLoss 1.6723 (1.7622)\tAccu 0.4062 (0.4167)\t\n",
            "Epoch: [19][13/44]\tLoss 1.7294 (1.7597)\tAccu 0.4375 (0.4183)\t\n",
            "Epoch: [19][14/44]\tLoss 1.6514 (1.7519)\tAccu 0.4688 (0.4219)\t\n",
            "Epoch: [19][15/44]\tLoss 1.8971 (1.7616)\tAccu 0.4375 (0.4229)\t\n",
            "Epoch: [19][16/44]\tLoss 1.6662 (1.7556)\tAccu 0.4375 (0.4238)\t\n",
            "Epoch: [19][17/44]\tLoss 1.4932 (1.7402)\tAccu 0.5000 (0.4283)\t\n",
            "Epoch: [19][18/44]\tLoss 1.7605 (1.7413)\tAccu 0.3750 (0.4253)\t\n",
            "Epoch: [19][19/44]\tLoss 1.7423 (1.7414)\tAccu 0.4375 (0.4260)\t\n",
            "Epoch: [19][20/44]\tLoss 2.1109 (1.7599)\tAccu 0.3125 (0.4203)\t\n",
            "Epoch: [19][21/44]\tLoss 1.8367 (1.7635)\tAccu 0.4375 (0.4211)\t\n",
            "Epoch: [19][22/44]\tLoss 1.7609 (1.7634)\tAccu 0.4062 (0.4205)\t\n",
            "Epoch: [19][23/44]\tLoss 1.6558 (1.7587)\tAccu 0.3750 (0.4185)\t\n",
            "Epoch: [19][24/44]\tLoss 1.7545 (1.7585)\tAccu 0.4375 (0.4193)\t\n",
            "Epoch: [19][25/44]\tLoss 1.7440 (1.7580)\tAccu 0.5312 (0.4238)\t\n",
            "Epoch: [19][26/44]\tLoss 1.5742 (1.7509)\tAccu 0.5312 (0.4279)\t\n",
            "Epoch: [19][27/44]\tLoss 1.7089 (1.7493)\tAccu 0.5625 (0.4329)\t\n",
            "Epoch: [19][28/44]\tLoss 1.6224 (1.7448)\tAccu 0.5312 (0.4364)\t\n",
            "Epoch: [19][29/44]\tLoss 1.8153 (1.7472)\tAccu 0.3438 (0.4332)\t\n",
            "Epoch: [19][30/44]\tLoss 1.5995 (1.7423)\tAccu 0.5312 (0.4365)\t\n",
            "Epoch: [19][31/44]\tLoss 1.6822 (1.7404)\tAccu 0.4375 (0.4365)\t\n",
            "Epoch: [19][32/44]\tLoss 1.5583 (1.7347)\tAccu 0.5312 (0.4395)\t\n",
            "Epoch: [19][33/44]\tLoss 1.8787 (1.7390)\tAccu 0.3750 (0.4375)\t\n",
            "Epoch: [19][34/44]\tLoss 1.7016 (1.7379)\tAccu 0.5000 (0.4393)\t\n",
            "Epoch: [19][35/44]\tLoss 1.5910 (1.7337)\tAccu 0.3438 (0.4366)\t\n",
            "Epoch: [19][36/44]\tLoss 2.0293 (1.7420)\tAccu 0.2812 (0.4323)\t\n",
            "Epoch: [19][37/44]\tLoss 1.6251 (1.7388)\tAccu 0.5000 (0.4341)\t\n",
            "Epoch: [19][38/44]\tLoss 1.5695 (1.7343)\tAccu 0.5312 (0.4367)\t\n",
            "Epoch: [19][39/44]\tLoss 1.6104 (1.7312)\tAccu 0.5000 (0.4383)\t\n",
            "Epoch: [19][40/44]\tLoss 1.7148 (1.7308)\tAccu 0.4375 (0.4383)\t\n",
            "Epoch: [19][41/44]\tLoss 1.6538 (1.7289)\tAccu 0.5000 (0.4398)\t\n",
            "Epoch: [19][42/44]\tLoss 1.4896 (1.7232)\tAccu 0.6562 (0.4449)\t\n",
            "Epoch: [19][43/44]\tLoss 1.6582 (1.7217)\tAccu 0.5000 (0.4462)\t\n",
            "Epoch: [19][44/44]\tLoss 1.4692 (1.7173)\tAccu 0.6250 (0.4503)\t\n",
            "Accu 0.2200\t\n",
            "Epoch: [20][1/44]\tLoss 1.7452 (1.7452)\tAccu 0.3750 (0.3750)\t\n",
            "Epoch: [20][2/44]\tLoss 1.5371 (1.6412)\tAccu 0.5000 (0.4375)\t\n",
            "Epoch: [20][3/44]\tLoss 1.7891 (1.6905)\tAccu 0.3438 (0.4062)\t\n",
            "Epoch: [20][4/44]\tLoss 1.9105 (1.7455)\tAccu 0.3750 (0.3984)\t\n",
            "Epoch: [20][5/44]\tLoss 1.7366 (1.7437)\tAccu 0.4375 (0.4062)\t\n",
            "Epoch: [20][6/44]\tLoss 1.4111 (1.6883)\tAccu 0.5312 (0.4271)\t\n",
            "Epoch: [20][7/44]\tLoss 1.6688 (1.6855)\tAccu 0.4062 (0.4241)\t\n",
            "Epoch: [20][8/44]\tLoss 1.5381 (1.6671)\tAccu 0.4375 (0.4258)\t\n",
            "Epoch: [20][9/44]\tLoss 1.5131 (1.6500)\tAccu 0.5625 (0.4410)\t\n",
            "Epoch: [20][10/44]\tLoss 1.6636 (1.6513)\tAccu 0.3750 (0.4344)\t\n",
            "Epoch: [20][11/44]\tLoss 1.5692 (1.6439)\tAccu 0.5000 (0.4403)\t\n",
            "Epoch: [20][12/44]\tLoss 1.5124 (1.6329)\tAccu 0.4688 (0.4427)\t\n",
            "Epoch: [20][13/44]\tLoss 1.6098 (1.6311)\tAccu 0.5312 (0.4495)\t\n",
            "Epoch: [20][14/44]\tLoss 1.5245 (1.6235)\tAccu 0.5000 (0.4531)\t\n",
            "Epoch: [20][15/44]\tLoss 1.7412 (1.6313)\tAccu 0.4688 (0.4542)\t\n",
            "Epoch: [20][16/44]\tLoss 1.4779 (1.6218)\tAccu 0.5312 (0.4590)\t\n",
            "Epoch: [20][17/44]\tLoss 1.3482 (1.6057)\tAccu 0.5625 (0.4651)\t\n",
            "Epoch: [20][18/44]\tLoss 1.6340 (1.6072)\tAccu 0.4062 (0.4618)\t\n",
            "Epoch: [20][19/44]\tLoss 1.5901 (1.6063)\tAccu 0.4375 (0.4605)\t\n",
            "Epoch: [20][20/44]\tLoss 2.0013 (1.6261)\tAccu 0.3750 (0.4562)\t\n",
            "Epoch: [20][21/44]\tLoss 1.6766 (1.6285)\tAccu 0.4688 (0.4568)\t\n",
            "Epoch: [20][22/44]\tLoss 1.6036 (1.6274)\tAccu 0.5000 (0.4588)\t\n",
            "Epoch: [20][23/44]\tLoss 1.4985 (1.6218)\tAccu 0.4688 (0.4592)\t\n",
            "Epoch: [20][24/44]\tLoss 1.6161 (1.6215)\tAccu 0.5000 (0.4609)\t\n",
            "Epoch: [20][25/44]\tLoss 1.6035 (1.6208)\tAccu 0.5938 (0.4662)\t\n",
            "Epoch: [20][26/44]\tLoss 1.4213 (1.6131)\tAccu 0.5938 (0.4712)\t\n",
            "Epoch: [20][27/44]\tLoss 1.5507 (1.6108)\tAccu 0.6250 (0.4769)\t\n",
            "Epoch: [20][28/44]\tLoss 1.4723 (1.6059)\tAccu 0.6250 (0.4821)\t\n",
            "Epoch: [20][29/44]\tLoss 1.5872 (1.6052)\tAccu 0.4688 (0.4817)\t\n",
            "Epoch: [20][30/44]\tLoss 1.3881 (1.5980)\tAccu 0.5625 (0.4844)\t\n",
            "Epoch: [20][31/44]\tLoss 1.4913 (1.5945)\tAccu 0.5625 (0.4869)\t\n",
            "Epoch: [20][32/44]\tLoss 1.4411 (1.5897)\tAccu 0.5000 (0.4873)\t\n",
            "Epoch: [20][33/44]\tLoss 1.7096 (1.5934)\tAccu 0.4062 (0.4848)\t\n",
            "Epoch: [20][34/44]\tLoss 1.6222 (1.5942)\tAccu 0.5000 (0.4853)\t\n",
            "Epoch: [20][35/44]\tLoss 1.4563 (1.5903)\tAccu 0.4062 (0.4830)\t\n",
            "Epoch: [20][36/44]\tLoss 1.8325 (1.5970)\tAccu 0.3438 (0.4792)\t\n",
            "Epoch: [20][37/44]\tLoss 1.4513 (1.5931)\tAccu 0.5000 (0.4797)\t\n",
            "Epoch: [20][38/44]\tLoss 1.4163 (1.5884)\tAccu 0.5312 (0.4811)\t\n",
            "Epoch: [20][39/44]\tLoss 1.4824 (1.5857)\tAccu 0.5625 (0.4832)\t\n",
            "Epoch: [20][40/44]\tLoss 1.5947 (1.5859)\tAccu 0.4688 (0.4828)\t\n",
            "Epoch: [20][41/44]\tLoss 1.4974 (1.5838)\tAccu 0.5625 (0.4848)\t\n",
            "Epoch: [20][42/44]\tLoss 1.2988 (1.5770)\tAccu 0.7188 (0.4903)\t\n",
            "Epoch: [20][43/44]\tLoss 1.4583 (1.5742)\tAccu 0.5625 (0.4920)\t\n",
            "Epoch: [20][44/44]\tLoss 1.3390 (1.5702)\tAccu 0.6250 (0.4950)\t\n",
            "Accu 0.2300\t\n",
            "Epoch: [21][1/44]\tLoss 1.5395 (1.5395)\tAccu 0.4688 (0.4688)\t\n",
            "Epoch: [21][2/44]\tLoss 1.3421 (1.4408)\tAccu 0.6562 (0.5625)\t\n",
            "Epoch: [21][3/44]\tLoss 1.6099 (1.4971)\tAccu 0.4375 (0.5208)\t\n",
            "Epoch: [21][4/44]\tLoss 1.8875 (1.5947)\tAccu 0.3750 (0.4844)\t\n",
            "Epoch: [21][5/44]\tLoss 1.5745 (1.5907)\tAccu 0.4688 (0.4813)\t\n",
            "Epoch: [21][6/44]\tLoss 1.2930 (1.5411)\tAccu 0.5625 (0.4948)\t\n",
            "Epoch: [21][7/44]\tLoss 1.5448 (1.5416)\tAccu 0.4688 (0.4911)\t\n",
            "Epoch: [21][8/44]\tLoss 1.3858 (1.5221)\tAccu 0.5000 (0.4922)\t\n",
            "Epoch: [21][9/44]\tLoss 1.3200 (1.4997)\tAccu 0.6250 (0.5069)\t\n",
            "Epoch: [21][10/44]\tLoss 1.4692 (1.4966)\tAccu 0.5312 (0.5094)\t\n",
            "Epoch: [21][11/44]\tLoss 1.3817 (1.4862)\tAccu 0.5312 (0.5114)\t\n",
            "Epoch: [21][12/44]\tLoss 1.2983 (1.4705)\tAccu 0.5938 (0.5182)\t\n",
            "Epoch: [21][13/44]\tLoss 1.4763 (1.4710)\tAccu 0.5625 (0.5216)\t\n",
            "Epoch: [21][14/44]\tLoss 1.3838 (1.4647)\tAccu 0.5000 (0.5201)\t\n",
            "Epoch: [21][15/44]\tLoss 1.5721 (1.4719)\tAccu 0.4375 (0.5146)\t\n",
            "Epoch: [21][16/44]\tLoss 1.2864 (1.4603)\tAccu 0.5938 (0.5195)\t\n",
            "Epoch: [21][17/44]\tLoss 1.1890 (1.4443)\tAccu 0.6250 (0.5257)\t\n",
            "Epoch: [21][18/44]\tLoss 1.4088 (1.4424)\tAccu 0.5000 (0.5243)\t\n",
            "Epoch: [21][19/44]\tLoss 1.4149 (1.4409)\tAccu 0.5625 (0.5263)\t\n",
            "Epoch: [21][20/44]\tLoss 1.8178 (1.4598)\tAccu 0.4375 (0.5219)\t\n",
            "Epoch: [21][21/44]\tLoss 1.5016 (1.4618)\tAccu 0.5000 (0.5208)\t\n",
            "Epoch: [21][22/44]\tLoss 1.4042 (1.4591)\tAccu 0.5625 (0.5227)\t\n",
            "Epoch: [21][23/44]\tLoss 1.3143 (1.4528)\tAccu 0.5625 (0.5245)\t\n",
            "Epoch: [21][24/44]\tLoss 1.4566 (1.4530)\tAccu 0.5000 (0.5234)\t\n",
            "Epoch: [21][25/44]\tLoss 1.4451 (1.4527)\tAccu 0.5625 (0.5250)\t\n",
            "Epoch: [21][26/44]\tLoss 1.2192 (1.4437)\tAccu 0.7500 (0.5337)\t\n",
            "Epoch: [21][27/44]\tLoss 1.3455 (1.4401)\tAccu 0.6562 (0.5382)\t\n",
            "Epoch: [21][28/44]\tLoss 1.2920 (1.4348)\tAccu 0.7188 (0.5446)\t\n",
            "Epoch: [21][29/44]\tLoss 1.3725 (1.4326)\tAccu 0.5625 (0.5453)\t\n",
            "Epoch: [21][30/44]\tLoss 1.1919 (1.4246)\tAccu 0.6562 (0.5490)\t\n",
            "Epoch: [21][31/44]\tLoss 1.2473 (1.4189)\tAccu 0.7188 (0.5544)\t\n",
            "Epoch: [21][32/44]\tLoss 1.2656 (1.4141)\tAccu 0.5938 (0.5557)\t\n",
            "Epoch: [21][33/44]\tLoss 1.5867 (1.4193)\tAccu 0.4062 (0.5511)\t\n",
            "Epoch: [21][34/44]\tLoss 1.4973 (1.4216)\tAccu 0.5000 (0.5496)\t\n",
            "Epoch: [21][35/44]\tLoss 1.2419 (1.4165)\tAccu 0.5938 (0.5509)\t\n",
            "Epoch: [21][36/44]\tLoss 1.6628 (1.4233)\tAccu 0.3750 (0.5460)\t\n",
            "Epoch: [21][37/44]\tLoss 1.2316 (1.4181)\tAccu 0.6250 (0.5481)\t\n",
            "Epoch: [21][38/44]\tLoss 1.2891 (1.4148)\tAccu 0.5000 (0.5469)\t\n",
            "Epoch: [21][39/44]\tLoss 1.2685 (1.4110)\tAccu 0.5938 (0.5481)\t\n",
            "Epoch: [21][40/44]\tLoss 1.3912 (1.4105)\tAccu 0.5000 (0.5469)\t\n",
            "Epoch: [21][41/44]\tLoss 1.2704 (1.4071)\tAccu 0.6562 (0.5495)\t\n",
            "Epoch: [21][42/44]\tLoss 1.0669 (1.3990)\tAccu 0.6562 (0.5521)\t\n",
            "Epoch: [21][43/44]\tLoss 1.2652 (1.3959)\tAccu 0.6250 (0.5538)\t\n",
            "Epoch: [21][44/44]\tLoss 1.0796 (1.3905)\tAccu 0.7083 (0.5573)\t\n",
            "Accu 0.2525\t\n",
            "Epoch: [22][1/44]\tLoss 1.3159 (1.3159)\tAccu 0.5938 (0.5938)\t\n",
            "Epoch: [22][2/44]\tLoss 1.1870 (1.2514)\tAccu 0.7500 (0.6719)\t\n",
            "Epoch: [22][3/44]\tLoss 1.4121 (1.3050)\tAccu 0.5312 (0.6250)\t\n",
            "Epoch: [22][4/44]\tLoss 1.6982 (1.4033)\tAccu 0.4375 (0.5781)\t\n",
            "Epoch: [22][5/44]\tLoss 1.3800 (1.3986)\tAccu 0.5625 (0.5750)\t\n",
            "Epoch: [22][6/44]\tLoss 1.1417 (1.3558)\tAccu 0.5938 (0.5781)\t\n",
            "Epoch: [22][7/44]\tLoss 1.3690 (1.3577)\tAccu 0.5000 (0.5670)\t\n",
            "Epoch: [22][8/44]\tLoss 1.2444 (1.3435)\tAccu 0.6250 (0.5742)\t\n",
            "Epoch: [22][9/44]\tLoss 1.1378 (1.3207)\tAccu 0.6875 (0.5868)\t\n",
            "Epoch: [22][10/44]\tLoss 1.2438 (1.3130)\tAccu 0.5938 (0.5875)\t\n",
            "Epoch: [22][11/44]\tLoss 1.1599 (1.2991)\tAccu 0.6250 (0.5909)\t\n",
            "Epoch: [22][12/44]\tLoss 1.1333 (1.2853)\tAccu 0.6250 (0.5938)\t\n",
            "Epoch: [22][13/44]\tLoss 1.3077 (1.2870)\tAccu 0.5312 (0.5889)\t\n",
            "Epoch: [22][14/44]\tLoss 1.2191 (1.2821)\tAccu 0.5312 (0.5848)\t\n",
            "Epoch: [22][15/44]\tLoss 1.3753 (1.2883)\tAccu 0.5000 (0.5792)\t\n",
            "Epoch: [22][16/44]\tLoss 1.0859 (1.2757)\tAccu 0.6250 (0.5820)\t\n",
            "Epoch: [22][17/44]\tLoss 1.0230 (1.2608)\tAccu 0.6875 (0.5882)\t\n",
            "Epoch: [22][18/44]\tLoss 1.1428 (1.2543)\tAccu 0.6875 (0.5938)\t\n",
            "Epoch: [22][19/44]\tLoss 1.1901 (1.2509)\tAccu 0.6875 (0.5987)\t\n",
            "Epoch: [22][20/44]\tLoss 1.6380 (1.2702)\tAccu 0.5625 (0.5969)\t\n",
            "Epoch: [22][21/44]\tLoss 1.2736 (1.2704)\tAccu 0.5938 (0.5967)\t\n",
            "Epoch: [22][22/44]\tLoss 1.1811 (1.2663)\tAccu 0.6875 (0.6009)\t\n",
            "Epoch: [22][23/44]\tLoss 1.1258 (1.2602)\tAccu 0.6875 (0.6046)\t\n",
            "Epoch: [22][24/44]\tLoss 1.2459 (1.2596)\tAccu 0.5625 (0.6029)\t\n",
            "Epoch: [22][25/44]\tLoss 1.2525 (1.2593)\tAccu 0.6875 (0.6062)\t\n",
            "Epoch: [22][26/44]\tLoss 1.0101 (1.2498)\tAccu 0.7188 (0.6106)\t\n",
            "Epoch: [22][27/44]\tLoss 1.1441 (1.2458)\tAccu 0.5938 (0.6100)\t\n",
            "Epoch: [22][28/44]\tLoss 1.0481 (1.2388)\tAccu 0.7500 (0.6150)\t\n",
            "Epoch: [22][29/44]\tLoss 1.1409 (1.2354)\tAccu 0.7188 (0.6185)\t\n",
            "Epoch: [22][30/44]\tLoss 1.0877 (1.2305)\tAccu 0.6562 (0.6198)\t\n",
            "Epoch: [22][31/44]\tLoss 1.0620 (1.2250)\tAccu 0.6562 (0.6210)\t\n",
            "Epoch: [22][32/44]\tLoss 0.9652 (1.2169)\tAccu 0.7188 (0.6240)\t\n",
            "Epoch: [22][33/44]\tLoss 1.4510 (1.2240)\tAccu 0.4688 (0.6193)\t\n",
            "Epoch: [22][34/44]\tLoss 1.3548 (1.2279)\tAccu 0.5938 (0.6186)\t\n",
            "Epoch: [22][35/44]\tLoss 0.9273 (1.2193)\tAccu 0.6562 (0.6196)\t\n",
            "Epoch: [22][36/44]\tLoss 1.3721 (1.2235)\tAccu 0.5312 (0.6172)\t\n",
            "Epoch: [22][37/44]\tLoss 1.0141 (1.2179)\tAccu 0.6250 (0.6174)\t\n",
            "Epoch: [22][38/44]\tLoss 1.2122 (1.2177)\tAccu 0.5938 (0.6168)\t\n",
            "Epoch: [22][39/44]\tLoss 1.0074 (1.2123)\tAccu 0.6875 (0.6186)\t\n",
            "Epoch: [22][40/44]\tLoss 1.0094 (1.2073)\tAccu 0.7188 (0.6211)\t\n",
            "Epoch: [22][41/44]\tLoss 0.9496 (1.2010)\tAccu 0.7188 (0.6235)\t\n",
            "Epoch: [22][42/44]\tLoss 0.8938 (1.1937)\tAccu 0.7188 (0.6257)\t\n",
            "Epoch: [22][43/44]\tLoss 1.1790 (1.1933)\tAccu 0.6562 (0.6265)\t\n",
            "Epoch: [22][44/44]\tLoss 0.6946 (1.1848)\tAccu 0.7500 (0.6293)\t\n",
            "Accu 0.2450\t\n",
            "Epoch: [23][1/44]\tLoss 1.0855 (1.0855)\tAccu 0.6562 (0.6562)\t\n",
            "Epoch: [23][2/44]\tLoss 0.9955 (1.0405)\tAccu 0.8125 (0.7344)\t\n",
            "Epoch: [23][3/44]\tLoss 1.1345 (1.0719)\tAccu 0.7500 (0.7396)\t\n",
            "Epoch: [23][4/44]\tLoss 1.3410 (1.1392)\tAccu 0.6875 (0.7266)\t\n",
            "Epoch: [23][5/44]\tLoss 1.2109 (1.1535)\tAccu 0.5938 (0.7000)\t\n",
            "Epoch: [23][6/44]\tLoss 0.8888 (1.1094)\tAccu 0.7812 (0.7135)\t\n",
            "Epoch: [23][7/44]\tLoss 1.1405 (1.1138)\tAccu 0.6250 (0.7009)\t\n",
            "Epoch: [23][8/44]\tLoss 1.0016 (1.0998)\tAccu 0.6250 (0.6914)\t\n",
            "Epoch: [23][9/44]\tLoss 0.9936 (1.0880)\tAccu 0.7500 (0.6979)\t\n",
            "Epoch: [23][10/44]\tLoss 1.1198 (1.0912)\tAccu 0.6250 (0.6906)\t\n",
            "Epoch: [23][11/44]\tLoss 0.9217 (1.0758)\tAccu 0.8125 (0.7017)\t\n",
            "Epoch: [23][12/44]\tLoss 0.8698 (1.0586)\tAccu 0.7188 (0.7031)\t\n",
            "Epoch: [23][13/44]\tLoss 1.0491 (1.0579)\tAccu 0.7500 (0.7067)\t\n",
            "Epoch: [23][14/44]\tLoss 1.0764 (1.0592)\tAccu 0.5625 (0.6964)\t\n",
            "Epoch: [23][15/44]\tLoss 1.1690 (1.0665)\tAccu 0.5625 (0.6875)\t\n",
            "Epoch: [23][16/44]\tLoss 0.8346 (1.0520)\tAccu 0.7188 (0.6895)\t\n",
            "Epoch: [23][17/44]\tLoss 0.8878 (1.0424)\tAccu 0.7188 (0.6912)\t\n",
            "Epoch: [23][18/44]\tLoss 0.8992 (1.0344)\tAccu 0.7188 (0.6927)\t\n",
            "Epoch: [23][19/44]\tLoss 0.9797 (1.0315)\tAccu 0.7812 (0.6974)\t\n",
            "Epoch: [23][20/44]\tLoss 1.6286 (1.0614)\tAccu 0.5000 (0.6875)\t\n",
            "Epoch: [23][21/44]\tLoss 1.0936 (1.0629)\tAccu 0.5938 (0.6830)\t\n",
            "Epoch: [23][22/44]\tLoss 0.8585 (1.0536)\tAccu 0.7500 (0.6861)\t\n",
            "Epoch: [23][23/44]\tLoss 0.8744 (1.0458)\tAccu 0.7500 (0.6889)\t\n",
            "Epoch: [23][24/44]\tLoss 0.9766 (1.0429)\tAccu 0.7500 (0.6914)\t\n",
            "Epoch: [23][25/44]\tLoss 1.0425 (1.0429)\tAccu 0.6562 (0.6900)\t\n",
            "Epoch: [23][26/44]\tLoss 0.8071 (1.0339)\tAccu 0.7812 (0.6935)\t\n",
            "Epoch: [23][27/44]\tLoss 0.8855 (1.0284)\tAccu 0.7812 (0.6968)\t\n",
            "Epoch: [23][28/44]\tLoss 0.7878 (1.0198)\tAccu 0.8125 (0.7009)\t\n",
            "Epoch: [23][29/44]\tLoss 0.8800 (1.0150)\tAccu 0.7812 (0.7037)\t\n",
            "Epoch: [23][30/44]\tLoss 0.7592 (1.0064)\tAccu 0.8125 (0.7073)\t\n",
            "Epoch: [23][31/44]\tLoss 0.8341 (1.0009)\tAccu 0.7188 (0.7077)\t\n",
            "Epoch: [23][32/44]\tLoss 0.8198 (0.9952)\tAccu 0.7812 (0.7100)\t\n",
            "Epoch: [23][33/44]\tLoss 1.0390 (0.9965)\tAccu 0.6875 (0.7093)\t\n",
            "Epoch: [23][34/44]\tLoss 1.0498 (0.9981)\tAccu 0.6250 (0.7068)\t\n",
            "Epoch: [23][35/44]\tLoss 0.7768 (0.9918)\tAccu 0.7812 (0.7089)\t\n",
            "Epoch: [23][36/44]\tLoss 1.0271 (0.9928)\tAccu 0.6875 (0.7083)\t\n",
            "Epoch: [23][37/44]\tLoss 0.7364 (0.9858)\tAccu 0.7812 (0.7103)\t\n",
            "Epoch: [23][38/44]\tLoss 1.0814 (0.9883)\tAccu 0.6250 (0.7081)\t\n",
            "Epoch: [23][39/44]\tLoss 0.7540 (0.9823)\tAccu 0.7812 (0.7099)\t\n",
            "Epoch: [23][40/44]\tLoss 0.7508 (0.9766)\tAccu 0.8125 (0.7125)\t\n",
            "Epoch: [23][41/44]\tLoss 0.6520 (0.9686)\tAccu 0.8750 (0.7165)\t\n",
            "Epoch: [23][42/44]\tLoss 0.6099 (0.9601)\tAccu 0.8438 (0.7195)\t\n",
            "Epoch: [23][43/44]\tLoss 1.0755 (0.9628)\tAccu 0.6875 (0.7188)\t\n",
            "Epoch: [23][44/44]\tLoss 0.4939 (0.9547)\tAccu 0.8750 (0.7223)\t\n",
            "Accu 0.2250\t\n",
            "Epoch: [24][1/44]\tLoss 0.8805 (0.8805)\tAccu 0.6562 (0.6562)\t\n",
            "Epoch: [24][2/44]\tLoss 0.7680 (0.8242)\tAccu 0.8438 (0.7500)\t\n",
            "Epoch: [24][3/44]\tLoss 0.8075 (0.8186)\tAccu 0.7500 (0.7500)\t\n",
            "Epoch: [24][4/44]\tLoss 0.9813 (0.8593)\tAccu 0.7500 (0.7500)\t\n",
            "Epoch: [24][5/44]\tLoss 0.9898 (0.8854)\tAccu 0.6875 (0.7375)\t\n",
            "Epoch: [24][6/44]\tLoss 0.7781 (0.8675)\tAccu 0.8125 (0.7500)\t\n",
            "Epoch: [24][7/44]\tLoss 0.8874 (0.8704)\tAccu 0.6875 (0.7411)\t\n",
            "Epoch: [24][8/44]\tLoss 0.7950 (0.8609)\tAccu 0.6875 (0.7344)\t\n",
            "Epoch: [24][9/44]\tLoss 0.7363 (0.8471)\tAccu 0.8438 (0.7465)\t\n",
            "Epoch: [24][10/44]\tLoss 0.8842 (0.8508)\tAccu 0.7188 (0.7438)\t\n",
            "Epoch: [24][11/44]\tLoss 0.8261 (0.8486)\tAccu 0.7500 (0.7443)\t\n",
            "Epoch: [24][12/44]\tLoss 0.6339 (0.8307)\tAccu 0.8438 (0.7526)\t\n",
            "Epoch: [24][13/44]\tLoss 0.8200 (0.8298)\tAccu 0.7812 (0.7548)\t\n",
            "Epoch: [24][14/44]\tLoss 0.6474 (0.8168)\tAccu 0.7812 (0.7567)\t\n",
            "Epoch: [24][15/44]\tLoss 0.8341 (0.8180)\tAccu 0.7812 (0.7583)\t\n",
            "Epoch: [24][16/44]\tLoss 0.6292 (0.8062)\tAccu 0.9062 (0.7676)\t\n",
            "Epoch: [24][17/44]\tLoss 0.6805 (0.7988)\tAccu 0.8125 (0.7702)\t\n",
            "Epoch: [24][18/44]\tLoss 0.6222 (0.7890)\tAccu 0.7812 (0.7708)\t\n",
            "Epoch: [24][19/44]\tLoss 0.7322 (0.7860)\tAccu 0.7812 (0.7714)\t\n",
            "Epoch: [24][20/44]\tLoss 1.5607 (0.8247)\tAccu 0.5312 (0.7594)\t\n",
            "Epoch: [24][21/44]\tLoss 0.9727 (0.8318)\tAccu 0.7188 (0.7574)\t\n",
            "Epoch: [24][22/44]\tLoss 0.6211 (0.8222)\tAccu 0.8125 (0.7599)\t\n",
            "Epoch: [24][23/44]\tLoss 0.6194 (0.8134)\tAccu 0.9375 (0.7677)\t\n",
            "Epoch: [24][24/44]\tLoss 0.6707 (0.8074)\tAccu 0.8438 (0.7708)\t\n",
            "Epoch: [24][25/44]\tLoss 0.7888 (0.8067)\tAccu 0.7188 (0.7688)\t\n",
            "Epoch: [24][26/44]\tLoss 0.6582 (0.8010)\tAccu 0.8125 (0.7704)\t\n",
            "Epoch: [24][27/44]\tLoss 0.6986 (0.7972)\tAccu 0.7812 (0.7708)\t\n",
            "Epoch: [24][28/44]\tLoss 0.5793 (0.7894)\tAccu 0.8438 (0.7734)\t\n",
            "Epoch: [24][29/44]\tLoss 0.6131 (0.7833)\tAccu 0.8125 (0.7748)\t\n",
            "Epoch: [24][30/44]\tLoss 0.6121 (0.7776)\tAccu 0.8750 (0.7781)\t\n",
            "Epoch: [24][31/44]\tLoss 0.6215 (0.7726)\tAccu 0.8438 (0.7802)\t\n",
            "Epoch: [24][32/44]\tLoss 0.5827 (0.7666)\tAccu 0.8125 (0.7812)\t\n",
            "Epoch: [24][33/44]\tLoss 0.7763 (0.7669)\tAccu 0.7812 (0.7812)\t\n",
            "Epoch: [24][34/44]\tLoss 0.7973 (0.7678)\tAccu 0.8438 (0.7831)\t\n",
            "Epoch: [24][35/44]\tLoss 0.5116 (0.7605)\tAccu 0.9062 (0.7866)\t\n",
            "Epoch: [24][36/44]\tLoss 0.7453 (0.7601)\tAccu 0.8125 (0.7873)\t\n",
            "Epoch: [24][37/44]\tLoss 0.5542 (0.7545)\tAccu 0.8438 (0.7889)\t\n",
            "Epoch: [24][38/44]\tLoss 0.8662 (0.7574)\tAccu 0.7812 (0.7887)\t\n",
            "Epoch: [24][39/44]\tLoss 0.5102 (0.7511)\tAccu 0.8125 (0.7893)\t\n",
            "Epoch: [24][40/44]\tLoss 0.5332 (0.7457)\tAccu 0.9062 (0.7922)\t\n",
            "Epoch: [24][41/44]\tLoss 0.4698 (0.7389)\tAccu 0.9375 (0.7957)\t\n",
            "Epoch: [24][42/44]\tLoss 0.4783 (0.7327)\tAccu 0.8438 (0.7969)\t\n",
            "Epoch: [24][43/44]\tLoss 0.9064 (0.7368)\tAccu 0.6875 (0.7943)\t\n",
            "Epoch: [24][44/44]\tLoss 0.3326 (0.7298)\tAccu 0.9167 (0.7971)\t\n",
            "Accu 0.2325\t\n",
            "Epoch: [25][1/44]\tLoss 0.5815 (0.5815)\tAccu 0.8125 (0.8125)\t\n",
            "Epoch: [25][2/44]\tLoss 0.7475 (0.6645)\tAccu 0.7812 (0.7969)\t\n",
            "Epoch: [25][3/44]\tLoss 0.7130 (0.6807)\tAccu 0.7500 (0.7812)\t\n",
            "Epoch: [25][4/44]\tLoss 0.7968 (0.7097)\tAccu 0.7812 (0.7812)\t\n",
            "Epoch: [25][5/44]\tLoss 0.5532 (0.6784)\tAccu 0.9688 (0.8187)\t\n",
            "Epoch: [25][6/44]\tLoss 0.5889 (0.6635)\tAccu 0.8750 (0.8281)\t\n",
            "Epoch: [25][7/44]\tLoss 0.6960 (0.6681)\tAccu 0.8438 (0.8304)\t\n",
            "Epoch: [25][8/44]\tLoss 0.5723 (0.6562)\tAccu 0.7812 (0.8242)\t\n",
            "Epoch: [25][9/44]\tLoss 0.5650 (0.6460)\tAccu 0.8438 (0.8264)\t\n",
            "Epoch: [25][10/44]\tLoss 0.4965 (0.6311)\tAccu 0.8125 (0.8250)\t\n",
            "Epoch: [25][11/44]\tLoss 0.5726 (0.6258)\tAccu 0.8125 (0.8239)\t\n",
            "Epoch: [25][12/44]\tLoss 0.5252 (0.6174)\tAccu 0.8438 (0.8255)\t\n",
            "Epoch: [25][13/44]\tLoss 0.7055 (0.6242)\tAccu 0.8125 (0.8245)\t\n",
            "Epoch: [25][14/44]\tLoss 0.3665 (0.6057)\tAccu 0.9688 (0.8348)\t\n",
            "Epoch: [25][15/44]\tLoss 0.4537 (0.5956)\tAccu 0.9062 (0.8396)\t\n",
            "Epoch: [25][16/44]\tLoss 0.5243 (0.5912)\tAccu 0.8438 (0.8398)\t\n",
            "Epoch: [25][17/44]\tLoss 0.4315 (0.5818)\tAccu 0.9375 (0.8456)\t\n",
            "Epoch: [25][18/44]\tLoss 0.5182 (0.5782)\tAccu 0.8125 (0.8438)\t\n",
            "Epoch: [25][19/44]\tLoss 0.7771 (0.5887)\tAccu 0.7188 (0.8372)\t\n",
            "Epoch: [25][20/44]\tLoss 0.9664 (0.6076)\tAccu 0.6875 (0.8297)\t\n",
            "Epoch: [25][21/44]\tLoss 0.7550 (0.6146)\tAccu 0.8125 (0.8289)\t\n",
            "Epoch: [25][22/44]\tLoss 0.5691 (0.6125)\tAccu 0.7500 (0.8253)\t\n",
            "Epoch: [25][23/44]\tLoss 0.3361 (0.6005)\tAccu 1.0000 (0.8329)\t\n",
            "Epoch: [25][24/44]\tLoss 0.5424 (0.5981)\tAccu 0.8438 (0.8333)\t\n",
            "Epoch: [25][25/44]\tLoss 0.6226 (0.5991)\tAccu 0.7500 (0.8300)\t\n",
            "Epoch: [25][26/44]\tLoss 0.5497 (0.5972)\tAccu 0.7812 (0.8281)\t\n",
            "Epoch: [25][27/44]\tLoss 0.4971 (0.5935)\tAccu 0.9062 (0.8310)\t\n",
            "Epoch: [25][28/44]\tLoss 0.5029 (0.5902)\tAccu 0.8438 (0.8315)\t\n",
            "Epoch: [25][29/44]\tLoss 0.4581 (0.5857)\tAccu 0.8750 (0.8330)\t\n",
            "Epoch: [25][30/44]\tLoss 0.7693 (0.5918)\tAccu 0.7812 (0.8313)\t\n",
            "Epoch: [25][31/44]\tLoss 0.6823 (0.5947)\tAccu 0.7812 (0.8296)\t\n",
            "Epoch: [25][32/44]\tLoss 0.3860 (0.5882)\tAccu 0.9688 (0.8340)\t\n",
            "Epoch: [25][33/44]\tLoss 0.5363 (0.5866)\tAccu 0.9375 (0.8371)\t\n",
            "Epoch: [25][34/44]\tLoss 0.6340 (0.5880)\tAccu 0.8125 (0.8364)\t\n",
            "Epoch: [25][35/44]\tLoss 0.5468 (0.5868)\tAccu 0.8125 (0.8357)\t\n",
            "Epoch: [25][36/44]\tLoss 1.1303 (0.6019)\tAccu 0.6250 (0.8299)\t\n",
            "Epoch: [25][37/44]\tLoss 0.5416 (0.6003)\tAccu 0.8125 (0.8294)\t\n",
            "Epoch: [25][38/44]\tLoss 0.4876 (0.5973)\tAccu 0.9375 (0.8322)\t\n",
            "Epoch: [25][39/44]\tLoss 0.3792 (0.5917)\tAccu 0.9062 (0.8341)\t\n",
            "Epoch: [25][40/44]\tLoss 0.6453 (0.5931)\tAccu 0.7500 (0.8320)\t\n",
            "Epoch: [25][41/44]\tLoss 0.5934 (0.5931)\tAccu 0.7812 (0.8308)\t\n",
            "Epoch: [25][42/44]\tLoss 0.3905 (0.5883)\tAccu 0.9375 (0.8333)\t\n",
            "Epoch: [25][43/44]\tLoss 0.8010 (0.5932)\tAccu 0.7188 (0.8307)\t\n",
            "Epoch: [25][44/44]\tLoss 0.4837 (0.5913)\tAccu 0.8333 (0.8307)\t\n",
            "Accu 0.2100\t\n",
            "Epoch: [26][1/44]\tLoss 0.5853 (0.5853)\tAccu 0.7500 (0.7500)\t\n",
            "Epoch: [26][2/44]\tLoss 0.4501 (0.5177)\tAccu 0.9375 (0.8438)\t\n",
            "Epoch: [26][3/44]\tLoss 0.5131 (0.5162)\tAccu 0.8750 (0.8542)\t\n",
            "Epoch: [26][4/44]\tLoss 0.7260 (0.5686)\tAccu 0.8750 (0.8594)\t\n",
            "Epoch: [26][5/44]\tLoss 0.6176 (0.5784)\tAccu 0.8125 (0.8500)\t\n",
            "Epoch: [26][6/44]\tLoss 0.4752 (0.5612)\tAccu 0.8750 (0.8542)\t\n",
            "Epoch: [26][7/44]\tLoss 0.4831 (0.5501)\tAccu 0.8438 (0.8527)\t\n",
            "Epoch: [26][8/44]\tLoss 0.4141 (0.5331)\tAccu 0.9062 (0.8594)\t\n",
            "Epoch: [26][9/44]\tLoss 0.7668 (0.5590)\tAccu 0.6875 (0.8403)\t\n",
            "Epoch: [26][10/44]\tLoss 0.5570 (0.5588)\tAccu 0.8750 (0.8438)\t\n",
            "Epoch: [26][11/44]\tLoss 0.5047 (0.5539)\tAccu 0.8750 (0.8466)\t\n",
            "Epoch: [26][12/44]\tLoss 0.3842 (0.5398)\tAccu 0.9375 (0.8542)\t\n",
            "Epoch: [26][13/44]\tLoss 0.5135 (0.5377)\tAccu 0.9062 (0.8582)\t\n",
            "Epoch: [26][14/44]\tLoss 0.4644 (0.5325)\tAccu 0.8438 (0.8571)\t\n",
            "Epoch: [26][15/44]\tLoss 0.4344 (0.5260)\tAccu 0.9375 (0.8625)\t\n",
            "Epoch: [26][16/44]\tLoss 0.4831 (0.5233)\tAccu 0.8750 (0.8633)\t\n",
            "Epoch: [26][17/44]\tLoss 0.3499 (0.5131)\tAccu 0.8750 (0.8640)\t\n",
            "Epoch: [26][18/44]\tLoss 0.3708 (0.5052)\tAccu 0.9062 (0.8663)\t\n",
            "Epoch: [26][19/44]\tLoss 0.5220 (0.5061)\tAccu 0.8125 (0.8635)\t\n",
            "Epoch: [26][20/44]\tLoss 0.9745 (0.5295)\tAccu 0.6875 (0.8547)\t\n",
            "Epoch: [26][21/44]\tLoss 0.6325 (0.5344)\tAccu 0.7188 (0.8482)\t\n",
            "Epoch: [26][22/44]\tLoss 0.4572 (0.5309)\tAccu 0.8750 (0.8494)\t\n",
            "Epoch: [26][23/44]\tLoss 0.2523 (0.5188)\tAccu 1.0000 (0.8560)\t\n",
            "Epoch: [26][24/44]\tLoss 0.3782 (0.5129)\tAccu 0.9688 (0.8607)\t\n",
            "Epoch: [26][25/44]\tLoss 0.8024 (0.5245)\tAccu 0.7500 (0.8562)\t\n",
            "Epoch: [26][26/44]\tLoss 0.5454 (0.5253)\tAccu 0.8125 (0.8546)\t\n",
            "Epoch: [26][27/44]\tLoss 0.5539 (0.5264)\tAccu 0.8438 (0.8542)\t\n",
            "Epoch: [26][28/44]\tLoss 0.3435 (0.5198)\tAccu 0.9375 (0.8571)\t\n",
            "Epoch: [26][29/44]\tLoss 0.4119 (0.5161)\tAccu 0.9062 (0.8588)\t\n",
            "Epoch: [26][30/44]\tLoss 0.4356 (0.5134)\tAccu 0.9375 (0.8615)\t\n",
            "Epoch: [26][31/44]\tLoss 0.9159 (0.5264)\tAccu 0.5938 (0.8528)\t\n",
            "Epoch: [26][32/44]\tLoss 0.5397 (0.5268)\tAccu 0.8125 (0.8516)\t\n",
            "Epoch: [26][33/44]\tLoss 0.4389 (0.5242)\tAccu 0.9375 (0.8542)\t\n",
            "Epoch: [26][34/44]\tLoss 0.5658 (0.5254)\tAccu 0.8125 (0.8529)\t\n",
            "Epoch: [26][35/44]\tLoss 0.3977 (0.5217)\tAccu 0.9062 (0.8545)\t\n",
            "Epoch: [26][36/44]\tLoss 0.5672 (0.5230)\tAccu 0.8125 (0.8533)\t\n",
            "Epoch: [26][37/44]\tLoss 0.5288 (0.5232)\tAccu 0.8125 (0.8522)\t\n",
            "Epoch: [26][38/44]\tLoss 0.6543 (0.5266)\tAccu 0.7500 (0.8495)\t\n",
            "Epoch: [26][39/44]\tLoss 0.3939 (0.5232)\tAccu 0.8750 (0.8502)\t\n",
            "Epoch: [26][40/44]\tLoss 0.4109 (0.5204)\tAccu 0.8750 (0.8508)\t\n",
            "Epoch: [26][41/44]\tLoss 0.3592 (0.5165)\tAccu 0.8750 (0.8514)\t\n",
            "Epoch: [26][42/44]\tLoss 0.3188 (0.5118)\tAccu 0.9375 (0.8534)\t\n",
            "Epoch: [26][43/44]\tLoss 0.5274 (0.5121)\tAccu 0.8750 (0.8539)\t\n",
            "Epoch: [26][44/44]\tLoss 0.5329 (0.5125)\tAccu 0.8333 (0.8535)\t\n",
            "Accu 0.1850\t\n",
            "Epoch: [27][1/44]\tLoss 0.5073 (0.5073)\tAccu 0.9062 (0.9062)\t\n",
            "Epoch: [27][2/44]\tLoss 0.3092 (0.4082)\tAccu 0.9375 (0.9219)\t\n",
            "Epoch: [27][3/44]\tLoss 0.5722 (0.4629)\tAccu 0.7812 (0.8750)\t\n",
            "Epoch: [27][4/44]\tLoss 0.6783 (0.5168)\tAccu 0.7500 (0.8438)\t\n",
            "Epoch: [27][5/44]\tLoss 0.4097 (0.4954)\tAccu 0.8438 (0.8438)\t\n",
            "Epoch: [27][6/44]\tLoss 0.5632 (0.5067)\tAccu 0.7812 (0.8333)\t\n",
            "Epoch: [27][7/44]\tLoss 0.3030 (0.4776)\tAccu 0.9688 (0.8527)\t\n",
            "Epoch: [27][8/44]\tLoss 0.3828 (0.4657)\tAccu 0.9062 (0.8594)\t\n",
            "Epoch: [27][9/44]\tLoss 0.4869 (0.4681)\tAccu 0.8750 (0.8611)\t\n",
            "Epoch: [27][10/44]\tLoss 0.4048 (0.4617)\tAccu 0.9062 (0.8656)\t\n",
            "Epoch: [27][11/44]\tLoss 0.4334 (0.4592)\tAccu 0.9062 (0.8693)\t\n",
            "Epoch: [27][12/44]\tLoss 0.3887 (0.4533)\tAccu 0.9375 (0.8750)\t\n",
            "Epoch: [27][13/44]\tLoss 0.4365 (0.4520)\tAccu 0.9062 (0.8774)\t\n",
            "Epoch: [27][14/44]\tLoss 0.2643 (0.4386)\tAccu 0.9688 (0.8839)\t\n",
            "Epoch: [27][15/44]\tLoss 0.4828 (0.4415)\tAccu 0.8438 (0.8812)\t\n",
            "Epoch: [27][16/44]\tLoss 0.4523 (0.4422)\tAccu 0.8750 (0.8809)\t\n",
            "Epoch: [27][17/44]\tLoss 0.4021 (0.4398)\tAccu 0.9062 (0.8824)\t\n",
            "Epoch: [27][18/44]\tLoss 0.2540 (0.4295)\tAccu 0.9375 (0.8854)\t\n",
            "Epoch: [27][19/44]\tLoss 0.4314 (0.4296)\tAccu 0.8750 (0.8849)\t\n",
            "Epoch: [27][20/44]\tLoss 0.6951 (0.4429)\tAccu 0.7500 (0.8781)\t\n",
            "Epoch: [27][21/44]\tLoss 0.3968 (0.4407)\tAccu 0.8750 (0.8780)\t\n",
            "Epoch: [27][22/44]\tLoss 0.6205 (0.4489)\tAccu 0.7812 (0.8736)\t\n",
            "Epoch: [27][23/44]\tLoss 0.5073 (0.4514)\tAccu 0.8438 (0.8723)\t\n",
            "Epoch: [27][24/44]\tLoss 0.2764 (0.4441)\tAccu 0.9688 (0.8763)\t\n",
            "Epoch: [27][25/44]\tLoss 0.3516 (0.4404)\tAccu 0.9062 (0.8775)\t\n",
            "Epoch: [27][26/44]\tLoss 0.5868 (0.4460)\tAccu 0.8125 (0.8750)\t\n",
            "Epoch: [27][27/44]\tLoss 0.9157 (0.4634)\tAccu 0.6562 (0.8669)\t\n",
            "Epoch: [27][28/44]\tLoss 0.4314 (0.4623)\tAccu 0.8750 (0.8672)\t\n",
            "Epoch: [27][29/44]\tLoss 0.4405 (0.4615)\tAccu 0.8438 (0.8664)\t\n",
            "Epoch: [27][30/44]\tLoss 0.2844 (0.4556)\tAccu 0.9688 (0.8698)\t\n",
            "Epoch: [27][31/44]\tLoss 0.4726 (0.4562)\tAccu 0.8750 (0.8700)\t\n",
            "Epoch: [27][32/44]\tLoss 0.3556 (0.4530)\tAccu 0.9375 (0.8721)\t\n",
            "Epoch: [27][33/44]\tLoss 0.4131 (0.4518)\tAccu 0.8750 (0.8722)\t\n",
            "Epoch: [27][34/44]\tLoss 0.9433 (0.4663)\tAccu 0.7500 (0.8686)\t\n",
            "Epoch: [27][35/44]\tLoss 0.8257 (0.4766)\tAccu 0.8438 (0.8679)\t\n",
            "Epoch: [27][36/44]\tLoss 0.7211 (0.4834)\tAccu 0.8125 (0.8663)\t\n",
            "Epoch: [27][37/44]\tLoss 0.4640 (0.4828)\tAccu 0.9062 (0.8674)\t\n",
            "Epoch: [27][38/44]\tLoss 0.5234 (0.4839)\tAccu 0.8125 (0.8660)\t\n",
            "Epoch: [27][39/44]\tLoss 0.5651 (0.4860)\tAccu 0.8438 (0.8654)\t\n",
            "Epoch: [27][40/44]\tLoss 0.8865 (0.4960)\tAccu 0.6875 (0.8609)\t\n",
            "Epoch: [27][41/44]\tLoss 0.6199 (0.4990)\tAccu 0.7188 (0.8575)\t\n",
            "Epoch: [27][42/44]\tLoss 0.3282 (0.4949)\tAccu 0.9062 (0.8586)\t\n",
            "Epoch: [27][43/44]\tLoss 0.5159 (0.4954)\tAccu 0.9062 (0.8597)\t\n",
            "Epoch: [27][44/44]\tLoss 0.1783 (0.4900)\tAccu 1.0000 (0.8629)\t\n",
            "Accu 0.2225\t\n",
            "Epoch: [28][1/44]\tLoss 0.7944 (0.7944)\tAccu 0.7500 (0.7500)\t\n",
            "Epoch: [28][2/44]\tLoss 0.4460 (0.6202)\tAccu 0.8750 (0.8125)\t\n",
            "Epoch: [28][3/44]\tLoss 0.6402 (0.6268)\tAccu 0.7500 (0.7917)\t\n",
            "Epoch: [28][4/44]\tLoss 0.6020 (0.6206)\tAccu 0.8125 (0.7969)\t\n",
            "Epoch: [28][5/44]\tLoss 0.5715 (0.6108)\tAccu 0.8125 (0.8000)\t\n",
            "Epoch: [28][6/44]\tLoss 0.5408 (0.5991)\tAccu 0.8125 (0.8021)\t\n",
            "Epoch: [28][7/44]\tLoss 0.4539 (0.5784)\tAccu 0.8438 (0.8080)\t\n",
            "Epoch: [28][8/44]\tLoss 0.5237 (0.5715)\tAccu 0.8438 (0.8125)\t\n",
            "Epoch: [28][9/44]\tLoss 0.6896 (0.5847)\tAccu 0.8438 (0.8160)\t\n",
            "Epoch: [28][10/44]\tLoss 0.7029 (0.5965)\tAccu 0.8750 (0.8219)\t\n",
            "Epoch: [28][11/44]\tLoss 0.4277 (0.5811)\tAccu 0.8438 (0.8239)\t\n",
            "Epoch: [28][12/44]\tLoss 0.3448 (0.5614)\tAccu 0.9375 (0.8333)\t\n",
            "Epoch: [28][13/44]\tLoss 0.5943 (0.5640)\tAccu 0.8125 (0.8317)\t\n",
            "Epoch: [28][14/44]\tLoss 0.4565 (0.5563)\tAccu 0.8750 (0.8348)\t\n",
            "Epoch: [28][15/44]\tLoss 0.5747 (0.5575)\tAccu 0.8125 (0.8333)\t\n",
            "Epoch: [28][16/44]\tLoss 0.5226 (0.5553)\tAccu 0.8125 (0.8320)\t\n",
            "Epoch: [28][17/44]\tLoss 0.3515 (0.5434)\tAccu 0.9062 (0.8364)\t\n",
            "Epoch: [28][18/44]\tLoss 0.7947 (0.5573)\tAccu 0.7812 (0.8333)\t\n",
            "Epoch: [28][19/44]\tLoss 0.4011 (0.5491)\tAccu 0.9062 (0.8372)\t\n",
            "Epoch: [28][20/44]\tLoss 0.5855 (0.5509)\tAccu 0.8438 (0.8375)\t\n",
            "Epoch: [28][21/44]\tLoss 0.5432 (0.5505)\tAccu 0.7812 (0.8348)\t\n",
            "Epoch: [28][22/44]\tLoss 0.3892 (0.5432)\tAccu 0.8750 (0.8366)\t\n",
            "Epoch: [28][23/44]\tLoss 0.3594 (0.5352)\tAccu 0.9062 (0.8397)\t\n",
            "Epoch: [28][24/44]\tLoss 0.2949 (0.5252)\tAccu 0.8750 (0.8411)\t\n",
            "Epoch: [28][25/44]\tLoss 0.6237 (0.5291)\tAccu 0.9062 (0.8438)\t\n",
            "Epoch: [28][26/44]\tLoss 0.3636 (0.5228)\tAccu 0.9375 (0.8474)\t\n",
            "Epoch: [28][27/44]\tLoss 0.3286 (0.5156)\tAccu 0.9062 (0.8495)\t\n",
            "Epoch: [28][28/44]\tLoss 0.4151 (0.5120)\tAccu 0.8750 (0.8504)\t\n",
            "Epoch: [28][29/44]\tLoss 0.5763 (0.5142)\tAccu 0.8125 (0.8491)\t\n",
            "Epoch: [28][30/44]\tLoss 0.4533 (0.5122)\tAccu 0.8750 (0.8500)\t\n",
            "Epoch: [28][31/44]\tLoss 0.2821 (0.5048)\tAccu 0.9375 (0.8528)\t\n",
            "Epoch: [28][32/44]\tLoss 0.4529 (0.5031)\tAccu 0.8438 (0.8525)\t\n",
            "Epoch: [28][33/44]\tLoss 0.4500 (0.5015)\tAccu 0.8750 (0.8532)\t\n",
            "Epoch: [28][34/44]\tLoss 0.3501 (0.4971)\tAccu 0.9062 (0.8548)\t\n",
            "Epoch: [28][35/44]\tLoss 0.3818 (0.4938)\tAccu 0.9062 (0.8562)\t\n",
            "Epoch: [28][36/44]\tLoss 0.3948 (0.4910)\tAccu 0.9062 (0.8576)\t\n",
            "Epoch: [28][37/44]\tLoss 0.3657 (0.4876)\tAccu 0.8438 (0.8573)\t\n",
            "Epoch: [28][38/44]\tLoss 0.7394 (0.4943)\tAccu 0.7500 (0.8544)\t\n",
            "Epoch: [28][39/44]\tLoss 0.3317 (0.4901)\tAccu 0.9375 (0.8566)\t\n",
            "Epoch: [28][40/44]\tLoss 0.1847 (0.4825)\tAccu 0.9688 (0.8594)\t\n",
            "Epoch: [28][41/44]\tLoss 0.3405 (0.4790)\tAccu 0.9062 (0.8605)\t\n",
            "Epoch: [28][42/44]\tLoss 0.3240 (0.4753)\tAccu 0.8750 (0.8609)\t\n",
            "Epoch: [28][43/44]\tLoss 0.4089 (0.4738)\tAccu 0.9375 (0.8626)\t\n",
            "Epoch: [28][44/44]\tLoss 0.1628 (0.4684)\tAccu 0.9583 (0.8648)\t\n",
            "Accu 0.2125\t\n",
            "Epoch: [29][1/44]\tLoss 0.4711 (0.4711)\tAccu 0.8750 (0.8750)\t\n",
            "Epoch: [29][2/44]\tLoss 0.2635 (0.3673)\tAccu 0.9062 (0.8906)\t\n",
            "Epoch: [29][3/44]\tLoss 0.3509 (0.3618)\tAccu 0.9062 (0.8958)\t\n",
            "Epoch: [29][4/44]\tLoss 0.3805 (0.3665)\tAccu 0.8750 (0.8906)\t\n",
            "Epoch: [29][5/44]\tLoss 0.3128 (0.3558)\tAccu 0.9375 (0.9000)\t\n",
            "Epoch: [29][6/44]\tLoss 0.3655 (0.3574)\tAccu 0.9062 (0.9010)\t\n",
            "Epoch: [29][7/44]\tLoss 0.7394 (0.4120)\tAccu 0.7500 (0.8795)\t\n",
            "Epoch: [29][8/44]\tLoss 0.3008 (0.3981)\tAccu 0.8750 (0.8789)\t\n",
            "Epoch: [29][9/44]\tLoss 0.3077 (0.3880)\tAccu 0.9688 (0.8889)\t\n",
            "Epoch: [29][10/44]\tLoss 0.3809 (0.3873)\tAccu 0.9375 (0.8938)\t\n",
            "Epoch: [29][11/44]\tLoss 0.4886 (0.3965)\tAccu 0.7812 (0.8835)\t\n",
            "Epoch: [29][12/44]\tLoss 0.3089 (0.3892)\tAccu 0.9062 (0.8854)\t\n",
            "Epoch: [29][13/44]\tLoss 0.3668 (0.3875)\tAccu 0.9062 (0.8870)\t\n",
            "Epoch: [29][14/44]\tLoss 0.2033 (0.3743)\tAccu 1.0000 (0.8951)\t\n",
            "Epoch: [29][15/44]\tLoss 0.4376 (0.3785)\tAccu 0.8438 (0.8917)\t\n",
            "Epoch: [29][16/44]\tLoss 0.3165 (0.3747)\tAccu 0.9375 (0.8945)\t\n",
            "Epoch: [29][17/44]\tLoss 0.1896 (0.3638)\tAccu 1.0000 (0.9007)\t\n",
            "Epoch: [29][18/44]\tLoss 0.3934 (0.3654)\tAccu 0.8750 (0.8993)\t\n",
            "Epoch: [29][19/44]\tLoss 0.3266 (0.3634)\tAccu 0.9062 (0.8997)\t\n",
            "Epoch: [29][20/44]\tLoss 0.5397 (0.3722)\tAccu 0.8125 (0.8953)\t\n",
            "Epoch: [29][21/44]\tLoss 0.2636 (0.3670)\tAccu 0.9375 (0.8973)\t\n",
            "Epoch: [29][22/44]\tLoss 0.3363 (0.3656)\tAccu 0.9062 (0.8977)\t\n",
            "Epoch: [29][23/44]\tLoss 0.3539 (0.3651)\tAccu 0.8750 (0.8967)\t\n",
            "Epoch: [29][24/44]\tLoss 0.3307 (0.3637)\tAccu 0.9375 (0.8984)\t\n",
            "Epoch: [29][25/44]\tLoss 0.3862 (0.3646)\tAccu 0.9062 (0.8988)\t\n",
            "Epoch: [29][26/44]\tLoss 0.2389 (0.3598)\tAccu 0.9688 (0.9014)\t\n",
            "Epoch: [29][27/44]\tLoss 0.2395 (0.3553)\tAccu 0.9688 (0.9039)\t\n",
            "Epoch: [29][28/44]\tLoss 0.1826 (0.3491)\tAccu 1.0000 (0.9074)\t\n",
            "Epoch: [29][29/44]\tLoss 0.3260 (0.3483)\tAccu 0.8438 (0.9052)\t\n",
            "Epoch: [29][30/44]\tLoss 0.2519 (0.3451)\tAccu 0.9062 (0.9052)\t\n",
            "Epoch: [29][31/44]\tLoss 0.3788 (0.3462)\tAccu 0.8438 (0.9032)\t\n",
            "Epoch: [29][32/44]\tLoss 0.2460 (0.3431)\tAccu 0.9375 (0.9043)\t\n",
            "Epoch: [29][33/44]\tLoss 0.3664 (0.3438)\tAccu 0.9375 (0.9053)\t\n",
            "Epoch: [29][34/44]\tLoss 0.2343 (0.3406)\tAccu 0.9375 (0.9062)\t\n",
            "Epoch: [29][35/44]\tLoss 0.2399 (0.3377)\tAccu 0.9375 (0.9071)\t\n",
            "Epoch: [29][36/44]\tLoss 0.3862 (0.3390)\tAccu 0.8438 (0.9054)\t\n",
            "Epoch: [29][37/44]\tLoss 0.1546 (0.3341)\tAccu 1.0000 (0.9079)\t\n",
            "Epoch: [29][38/44]\tLoss 0.3536 (0.3346)\tAccu 0.8750 (0.9071)\t\n",
            "Epoch: [29][39/44]\tLoss 0.2333 (0.3320)\tAccu 0.9375 (0.9079)\t\n",
            "Epoch: [29][40/44]\tLoss 0.3100 (0.3314)\tAccu 0.8750 (0.9070)\t\n",
            "Epoch: [29][41/44]\tLoss 0.2235 (0.3288)\tAccu 0.9375 (0.9078)\t\n",
            "Epoch: [29][42/44]\tLoss 0.1269 (0.3240)\tAccu 1.0000 (0.9100)\t\n",
            "Epoch: [29][43/44]\tLoss 0.2456 (0.3222)\tAccu 0.9375 (0.9106)\t\n",
            "Epoch: [29][44/44]\tLoss 0.1502 (0.3192)\tAccu 0.9583 (0.9117)\t\n",
            "Accu 0.1975\t\n",
            "Epoch: [30][1/44]\tLoss 0.1847 (0.1847)\tAccu 0.9688 (0.9688)\t\n",
            "Epoch: [30][2/44]\tLoss 0.2236 (0.2042)\tAccu 0.9688 (0.9688)\t\n",
            "Epoch: [30][3/44]\tLoss 0.2581 (0.2221)\tAccu 0.9688 (0.9688)\t\n",
            "Epoch: [30][4/44]\tLoss 0.2550 (0.2304)\tAccu 0.9375 (0.9609)\t\n",
            "Epoch: [30][5/44]\tLoss 0.2073 (0.2257)\tAccu 0.9688 (0.9625)\t\n",
            "Epoch: [30][6/44]\tLoss 0.3428 (0.2452)\tAccu 0.8750 (0.9479)\t\n",
            "Epoch: [30][7/44]\tLoss 0.1690 (0.2344)\tAccu 0.9375 (0.9464)\t\n",
            "Epoch: [30][8/44]\tLoss 0.2275 (0.2335)\tAccu 0.9688 (0.9492)\t\n",
            "Epoch: [30][9/44]\tLoss 0.4535 (0.2579)\tAccu 0.8438 (0.9375)\t\n",
            "Epoch: [30][10/44]\tLoss 0.4748 (0.2796)\tAccu 0.9062 (0.9344)\t\n",
            "Epoch: [30][11/44]\tLoss 0.2923 (0.2808)\tAccu 0.9062 (0.9318)\t\n",
            "Epoch: [30][12/44]\tLoss 0.1957 (0.2737)\tAccu 0.9688 (0.9349)\t\n",
            "Epoch: [30][13/44]\tLoss 0.2220 (0.2697)\tAccu 0.9375 (0.9351)\t\n",
            "Epoch: [30][14/44]\tLoss 0.1206 (0.2591)\tAccu 1.0000 (0.9397)\t\n",
            "Epoch: [30][15/44]\tLoss 0.3137 (0.2627)\tAccu 0.9062 (0.9375)\t\n",
            "Epoch: [30][16/44]\tLoss 0.3329 (0.2671)\tAccu 0.8750 (0.9336)\t\n",
            "Epoch: [30][17/44]\tLoss 0.1333 (0.2592)\tAccu 1.0000 (0.9375)\t\n",
            "Epoch: [30][18/44]\tLoss 0.1590 (0.2537)\tAccu 0.9688 (0.9392)\t\n",
            "Epoch: [30][19/44]\tLoss 0.2843 (0.2553)\tAccu 0.9062 (0.9375)\t\n",
            "Epoch: [30][20/44]\tLoss 0.3274 (0.2589)\tAccu 0.8750 (0.9344)\t\n",
            "Epoch: [30][21/44]\tLoss 0.2108 (0.2566)\tAccu 0.9375 (0.9345)\t\n",
            "Epoch: [30][22/44]\tLoss 0.1706 (0.2527)\tAccu 0.9688 (0.9361)\t\n",
            "Epoch: [30][23/44]\tLoss 0.1054 (0.2463)\tAccu 1.0000 (0.9389)\t\n",
            "Epoch: [30][24/44]\tLoss 0.1871 (0.2438)\tAccu 0.9688 (0.9401)\t\n",
            "Epoch: [30][25/44]\tLoss 0.3488 (0.2480)\tAccu 0.9062 (0.9387)\t\n",
            "Epoch: [30][26/44]\tLoss 0.1709 (0.2450)\tAccu 0.9688 (0.9399)\t\n",
            "Epoch: [30][27/44]\tLoss 0.1594 (0.2419)\tAccu 0.9688 (0.9410)\t\n",
            "Epoch: [30][28/44]\tLoss 0.1512 (0.2386)\tAccu 0.9688 (0.9420)\t\n",
            "Epoch: [30][29/44]\tLoss 0.2703 (0.2397)\tAccu 0.8750 (0.9397)\t\n",
            "Epoch: [30][30/44]\tLoss 0.1263 (0.2359)\tAccu 1.0000 (0.9417)\t\n",
            "Epoch: [30][31/44]\tLoss 0.1255 (0.2324)\tAccu 1.0000 (0.9435)\t\n",
            "Epoch: [30][32/44]\tLoss 0.3128 (0.2349)\tAccu 0.9375 (0.9434)\t\n",
            "Epoch: [30][33/44]\tLoss 0.2163 (0.2343)\tAccu 0.9062 (0.9422)\t\n",
            "Epoch: [30][34/44]\tLoss 0.1078 (0.2306)\tAccu 1.0000 (0.9439)\t\n",
            "Epoch: [30][35/44]\tLoss 0.3172 (0.2331)\tAccu 0.9062 (0.9429)\t\n",
            "Epoch: [30][36/44]\tLoss 0.3751 (0.2370)\tAccu 0.8750 (0.9410)\t\n",
            "Epoch: [30][37/44]\tLoss 0.1534 (0.2348)\tAccu 0.9688 (0.9417)\t\n",
            "Epoch: [30][38/44]\tLoss 0.1856 (0.2335)\tAccu 0.9688 (0.9424)\t\n",
            "Epoch: [30][39/44]\tLoss 0.1344 (0.2309)\tAccu 1.0000 (0.9439)\t\n",
            "Epoch: [30][40/44]\tLoss 0.2029 (0.2302)\tAccu 0.9375 (0.9437)\t\n",
            "Epoch: [30][41/44]\tLoss 0.1336 (0.2279)\tAccu 0.9688 (0.9444)\t\n",
            "Epoch: [30][42/44]\tLoss 0.2589 (0.2286)\tAccu 0.9062 (0.9435)\t\n",
            "Epoch: [30][43/44]\tLoss 0.3769 (0.2321)\tAccu 0.8438 (0.9411)\t\n",
            "Epoch: [30][44/44]\tLoss 0.1427 (0.2305)\tAccu 1.0000 (0.9425)\t\n",
            "Accu 0.1975\t\n",
            "Epoch: [31][1/44]\tLoss 0.2189 (0.2189)\tAccu 0.9375 (0.9375)\t\n",
            "Epoch: [31][2/44]\tLoss 0.2087 (0.2138)\tAccu 0.9688 (0.9531)\t\n",
            "Epoch: [31][3/44]\tLoss 0.2675 (0.2317)\tAccu 0.9062 (0.9375)\t\n",
            "Epoch: [31][4/44]\tLoss 0.2815 (0.2442)\tAccu 0.9375 (0.9375)\t\n",
            "Epoch: [31][5/44]\tLoss 0.1590 (0.2271)\tAccu 0.9688 (0.9437)\t\n",
            "Epoch: [31][6/44]\tLoss 0.3630 (0.2498)\tAccu 0.8750 (0.9323)\t\n",
            "Epoch: [31][7/44]\tLoss 0.1507 (0.2356)\tAccu 1.0000 (0.9420)\t\n",
            "Epoch: [31][8/44]\tLoss 0.3642 (0.2517)\tAccu 0.9062 (0.9375)\t\n",
            "Epoch: [31][9/44]\tLoss 0.2154 (0.2477)\tAccu 0.9688 (0.9410)\t\n",
            "Epoch: [31][10/44]\tLoss 0.1578 (0.2387)\tAccu 0.9688 (0.9437)\t\n",
            "Epoch: [31][11/44]\tLoss 0.1205 (0.2279)\tAccu 0.9688 (0.9460)\t\n",
            "Epoch: [31][12/44]\tLoss 0.2660 (0.2311)\tAccu 0.8750 (0.9401)\t\n",
            "Epoch: [31][13/44]\tLoss 0.2511 (0.2326)\tAccu 0.9375 (0.9399)\t\n",
            "Epoch: [31][14/44]\tLoss 0.1486 (0.2266)\tAccu 0.9688 (0.9420)\t\n",
            "Epoch: [31][15/44]\tLoss 0.0989 (0.2181)\tAccu 1.0000 (0.9458)\t\n",
            "Epoch: [31][16/44]\tLoss 0.0943 (0.2104)\tAccu 1.0000 (0.9492)\t\n",
            "Epoch: [31][17/44]\tLoss 0.2584 (0.2132)\tAccu 0.9375 (0.9485)\t\n",
            "Epoch: [31][18/44]\tLoss 0.1241 (0.2083)\tAccu 1.0000 (0.9514)\t\n",
            "Epoch: [31][19/44]\tLoss 0.2065 (0.2082)\tAccu 0.9062 (0.9490)\t\n",
            "Epoch: [31][20/44]\tLoss 0.3472 (0.2151)\tAccu 0.9062 (0.9469)\t\n",
            "Epoch: [31][21/44]\tLoss 0.2117 (0.2150)\tAccu 0.9688 (0.9479)\t\n",
            "Epoch: [31][22/44]\tLoss 0.1737 (0.2131)\tAccu 1.0000 (0.9503)\t\n",
            "Epoch: [31][23/44]\tLoss 0.1911 (0.2121)\tAccu 0.9688 (0.9511)\t\n",
            "Epoch: [31][24/44]\tLoss 0.2362 (0.2131)\tAccu 0.9375 (0.9505)\t\n",
            "Epoch: [31][25/44]\tLoss 0.1580 (0.2109)\tAccu 0.9688 (0.9513)\t\n",
            "Epoch: [31][26/44]\tLoss 0.1800 (0.2097)\tAccu 0.9375 (0.9507)\t\n",
            "Epoch: [31][27/44]\tLoss 0.1317 (0.2069)\tAccu 1.0000 (0.9525)\t\n",
            "Epoch: [31][28/44]\tLoss 0.1710 (0.2056)\tAccu 0.9375 (0.9520)\t\n",
            "Epoch: [31][29/44]\tLoss 0.1904 (0.2050)\tAccu 0.9688 (0.9526)\t\n",
            "Epoch: [31][30/44]\tLoss 0.2725 (0.2073)\tAccu 0.9375 (0.9521)\t\n",
            "Epoch: [31][31/44]\tLoss 0.1602 (0.2058)\tAccu 0.9688 (0.9526)\t\n",
            "Epoch: [31][32/44]\tLoss 0.1213 (0.2031)\tAccu 1.0000 (0.9541)\t\n",
            "Epoch: [31][33/44]\tLoss 0.1788 (0.2024)\tAccu 0.9375 (0.9536)\t\n",
            "Epoch: [31][34/44]\tLoss 0.1510 (0.2009)\tAccu 0.9688 (0.9540)\t\n",
            "Epoch: [31][35/44]\tLoss 0.2643 (0.2027)\tAccu 0.8750 (0.9518)\t\n",
            "Epoch: [31][36/44]\tLoss 0.1727 (0.2019)\tAccu 0.9688 (0.9523)\t\n",
            "Epoch: [31][37/44]\tLoss 0.1845 (0.2014)\tAccu 0.9688 (0.9527)\t\n",
            "Epoch: [31][38/44]\tLoss 0.2275 (0.2021)\tAccu 0.9375 (0.9523)\t\n",
            "Epoch: [31][39/44]\tLoss 0.1257 (0.2001)\tAccu 1.0000 (0.9535)\t\n",
            "Epoch: [31][40/44]\tLoss 0.0717 (0.1969)\tAccu 1.0000 (0.9547)\t\n",
            "Epoch: [31][41/44]\tLoss 0.0571 (0.1935)\tAccu 1.0000 (0.9558)\t\n",
            "Epoch: [31][42/44]\tLoss 0.0825 (0.1909)\tAccu 1.0000 (0.9568)\t\n",
            "Epoch: [31][43/44]\tLoss 0.2216 (0.1916)\tAccu 0.9375 (0.9564)\t\n",
            "Epoch: [31][44/44]\tLoss 0.4155 (0.1954)\tAccu 0.9167 (0.9555)\t\n",
            "Accu 0.2150\t\n",
            "Epoch: [32][1/44]\tLoss 0.2636 (0.2636)\tAccu 0.9062 (0.9062)\t\n",
            "Epoch: [32][2/44]\tLoss 0.2263 (0.2449)\tAccu 0.9375 (0.9219)\t\n",
            "Epoch: [32][3/44]\tLoss 0.1288 (0.2062)\tAccu 1.0000 (0.9479)\t\n",
            "Epoch: [32][4/44]\tLoss 0.1878 (0.2016)\tAccu 0.9688 (0.9531)\t\n",
            "Epoch: [32][5/44]\tLoss 0.2049 (0.2022)\tAccu 0.9375 (0.9500)\t\n",
            "Epoch: [32][6/44]\tLoss 0.3382 (0.2249)\tAccu 0.9375 (0.9479)\t\n",
            "Epoch: [32][7/44]\tLoss 0.2405 (0.2271)\tAccu 0.9375 (0.9464)\t\n",
            "Epoch: [32][8/44]\tLoss 0.1426 (0.2166)\tAccu 0.9688 (0.9492)\t\n",
            "Epoch: [32][9/44]\tLoss 0.2702 (0.2225)\tAccu 0.9375 (0.9479)\t\n",
            "Epoch: [32][10/44]\tLoss 0.4718 (0.2475)\tAccu 0.8125 (0.9344)\t\n",
            "Epoch: [32][11/44]\tLoss 0.4014 (0.2614)\tAccu 0.8750 (0.9290)\t\n",
            "Epoch: [32][12/44]\tLoss 0.1447 (0.2517)\tAccu 0.9688 (0.9323)\t\n",
            "Epoch: [32][13/44]\tLoss 0.1064 (0.2405)\tAccu 1.0000 (0.9375)\t\n",
            "Epoch: [32][14/44]\tLoss 0.1600 (0.2348)\tAccu 0.9688 (0.9397)\t\n",
            "Epoch: [32][15/44]\tLoss 0.1451 (0.2288)\tAccu 1.0000 (0.9437)\t\n",
            "Epoch: [32][16/44]\tLoss 0.1425 (0.2234)\tAccu 0.9375 (0.9434)\t\n",
            "Epoch: [32][17/44]\tLoss 0.1025 (0.2163)\tAccu 1.0000 (0.9467)\t\n",
            "Epoch: [32][18/44]\tLoss 0.2018 (0.2155)\tAccu 0.9062 (0.9444)\t\n",
            "Epoch: [32][19/44]\tLoss 0.1939 (0.2144)\tAccu 0.9688 (0.9457)\t\n",
            "Epoch: [32][20/44]\tLoss 0.1669 (0.2120)\tAccu 1.0000 (0.9484)\t\n",
            "Epoch: [32][21/44]\tLoss 0.1308 (0.2081)\tAccu 1.0000 (0.9509)\t\n",
            "Epoch: [32][22/44]\tLoss 0.1266 (0.2044)\tAccu 0.9688 (0.9517)\t\n",
            "Epoch: [32][23/44]\tLoss 0.1285 (0.2011)\tAccu 0.9688 (0.9524)\t\n",
            "Epoch: [32][24/44]\tLoss 0.1577 (0.1993)\tAccu 0.9375 (0.9518)\t\n",
            "Epoch: [32][25/44]\tLoss 0.2162 (0.2000)\tAccu 0.9375 (0.9513)\t\n",
            "Epoch: [32][26/44]\tLoss 0.1052 (0.1963)\tAccu 1.0000 (0.9531)\t\n",
            "Epoch: [32][27/44]\tLoss 0.1751 (0.1955)\tAccu 0.9375 (0.9525)\t\n",
            "Epoch: [32][28/44]\tLoss 0.1969 (0.1956)\tAccu 0.9375 (0.9520)\t\n",
            "Epoch: [32][29/44]\tLoss 0.2051 (0.1959)\tAccu 0.9375 (0.9515)\t\n",
            "Epoch: [32][30/44]\tLoss 0.1769 (0.1953)\tAccu 0.9375 (0.9510)\t\n",
            "Epoch: [32][31/44]\tLoss 0.1612 (0.1942)\tAccu 0.9688 (0.9516)\t\n",
            "Epoch: [32][32/44]\tLoss 0.2771 (0.1968)\tAccu 0.9062 (0.9502)\t\n",
            "Epoch: [32][33/44]\tLoss 0.0511 (0.1924)\tAccu 1.0000 (0.9517)\t\n",
            "Epoch: [32][34/44]\tLoss 0.1467 (0.1910)\tAccu 0.9688 (0.9522)\t\n",
            "Epoch: [32][35/44]\tLoss 0.1409 (0.1896)\tAccu 1.0000 (0.9536)\t\n",
            "Epoch: [32][36/44]\tLoss 0.1213 (0.1877)\tAccu 0.9688 (0.9540)\t\n",
            "Epoch: [32][37/44]\tLoss 0.0827 (0.1849)\tAccu 1.0000 (0.9552)\t\n",
            "Epoch: [32][38/44]\tLoss 0.2246 (0.1859)\tAccu 0.8750 (0.9531)\t\n",
            "Epoch: [32][39/44]\tLoss 0.1487 (0.1850)\tAccu 0.9688 (0.9535)\t\n",
            "Epoch: [32][40/44]\tLoss 0.2898 (0.1876)\tAccu 0.8750 (0.9516)\t\n",
            "Epoch: [32][41/44]\tLoss 0.0691 (0.1847)\tAccu 1.0000 (0.9527)\t\n",
            "Epoch: [32][42/44]\tLoss 0.0424 (0.1813)\tAccu 1.0000 (0.9539)\t\n",
            "Epoch: [32][43/44]\tLoss 0.1198 (0.1799)\tAccu 0.9375 (0.9535)\t\n",
            "Epoch: [32][44/44]\tLoss 0.0679 (0.1779)\tAccu 1.0000 (0.9545)\t\n",
            "Accu 0.2000\t\n",
            "Epoch: [33][1/44]\tLoss 0.1477 (0.1477)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [33][2/44]\tLoss 0.0888 (0.1183)\tAccu 0.9688 (0.9844)\t\n",
            "Epoch: [33][3/44]\tLoss 0.1401 (0.1256)\tAccu 0.9688 (0.9792)\t\n",
            "Epoch: [33][4/44]\tLoss 0.2810 (0.1644)\tAccu 0.9062 (0.9609)\t\n",
            "Epoch: [33][5/44]\tLoss 0.1060 (0.1527)\tAccu 0.9688 (0.9625)\t\n",
            "Epoch: [33][6/44]\tLoss 0.1323 (0.1493)\tAccu 0.9375 (0.9583)\t\n",
            "Epoch: [33][7/44]\tLoss 0.0522 (0.1355)\tAccu 1.0000 (0.9643)\t\n",
            "Epoch: [33][8/44]\tLoss 0.0492 (0.1247)\tAccu 1.0000 (0.9688)\t\n",
            "Epoch: [33][9/44]\tLoss 0.1412 (0.1265)\tAccu 0.9688 (0.9688)\t\n",
            "Epoch: [33][10/44]\tLoss 0.1854 (0.1324)\tAccu 0.9375 (0.9656)\t\n",
            "Epoch: [33][11/44]\tLoss 0.0962 (0.1291)\tAccu 0.9688 (0.9659)\t\n",
            "Epoch: [33][12/44]\tLoss 0.0999 (0.1267)\tAccu 0.9688 (0.9661)\t\n",
            "Epoch: [33][13/44]\tLoss 0.1082 (0.1253)\tAccu 0.9688 (0.9663)\t\n",
            "Epoch: [33][14/44]\tLoss 0.1132 (0.1244)\tAccu 0.9688 (0.9665)\t\n",
            "Epoch: [33][15/44]\tLoss 0.3141 (0.1370)\tAccu 0.9062 (0.9625)\t\n",
            "Epoch: [33][16/44]\tLoss 0.0484 (0.1315)\tAccu 1.0000 (0.9648)\t\n",
            "Epoch: [33][17/44]\tLoss 0.2012 (0.1356)\tAccu 0.9375 (0.9632)\t\n",
            "Epoch: [33][18/44]\tLoss 0.1123 (0.1343)\tAccu 0.9688 (0.9635)\t\n",
            "Epoch: [33][19/44]\tLoss 0.1089 (0.1330)\tAccu 0.9688 (0.9638)\t\n",
            "Epoch: [33][20/44]\tLoss 0.3120 (0.1419)\tAccu 0.8750 (0.9594)\t\n",
            "Epoch: [33][21/44]\tLoss 0.1559 (0.1426)\tAccu 0.9688 (0.9598)\t\n",
            "Epoch: [33][22/44]\tLoss 0.0963 (0.1405)\tAccu 1.0000 (0.9616)\t\n",
            "Epoch: [33][23/44]\tLoss 0.0487 (0.1365)\tAccu 1.0000 (0.9633)\t\n",
            "Epoch: [33][24/44]\tLoss 0.0441 (0.1326)\tAccu 1.0000 (0.9648)\t\n",
            "Epoch: [33][25/44]\tLoss 0.1638 (0.1339)\tAccu 0.9375 (0.9637)\t\n",
            "Epoch: [33][26/44]\tLoss 0.1262 (0.1336)\tAccu 0.9688 (0.9639)\t\n",
            "Epoch: [33][27/44]\tLoss 0.0626 (0.1310)\tAccu 1.0000 (0.9653)\t\n",
            "Epoch: [33][28/44]\tLoss 0.1161 (0.1304)\tAccu 0.9688 (0.9654)\t\n",
            "Epoch: [33][29/44]\tLoss 0.0565 (0.1279)\tAccu 1.0000 (0.9666)\t\n",
            "Epoch: [33][30/44]\tLoss 0.0993 (0.1269)\tAccu 0.9688 (0.9667)\t\n",
            "Epoch: [33][31/44]\tLoss 0.0795 (0.1254)\tAccu 0.9688 (0.9667)\t\n",
            "Epoch: [33][32/44]\tLoss 0.0936 (0.1244)\tAccu 0.9688 (0.9668)\t\n",
            "Epoch: [33][33/44]\tLoss 0.2452 (0.1281)\tAccu 0.8750 (0.9640)\t\n",
            "Epoch: [33][34/44]\tLoss 0.1191 (0.1278)\tAccu 0.9375 (0.9632)\t\n",
            "Epoch: [33][35/44]\tLoss 0.1137 (0.1274)\tAccu 1.0000 (0.9643)\t\n",
            "Epoch: [33][36/44]\tLoss 0.0870 (0.1263)\tAccu 0.9688 (0.9644)\t\n",
            "Epoch: [33][37/44]\tLoss 0.2710 (0.1302)\tAccu 0.9062 (0.9628)\t\n",
            "Epoch: [33][38/44]\tLoss 0.0874 (0.1291)\tAccu 1.0000 (0.9638)\t\n",
            "Epoch: [33][39/44]\tLoss 0.1072 (0.1285)\tAccu 0.9688 (0.9639)\t\n",
            "Epoch: [33][40/44]\tLoss 0.0627 (0.1269)\tAccu 1.0000 (0.9648)\t\n",
            "Epoch: [33][41/44]\tLoss 0.1070 (0.1264)\tAccu 0.9688 (0.9649)\t\n",
            "Epoch: [33][42/44]\tLoss 0.0718 (0.1251)\tAccu 1.0000 (0.9658)\t\n",
            "Epoch: [33][43/44]\tLoss 0.0993 (0.1245)\tAccu 0.9688 (0.9658)\t\n",
            "Epoch: [33][44/44]\tLoss 0.0212 (0.1227)\tAccu 1.0000 (0.9666)\t\n",
            "Accu 0.1950\t\n",
            "Epoch: [34][1/44]\tLoss 0.1515 (0.1515)\tAccu 0.9375 (0.9375)\t\n",
            "Epoch: [34][2/44]\tLoss 0.1359 (0.1437)\tAccu 0.9688 (0.9531)\t\n",
            "Epoch: [34][3/44]\tLoss 0.1819 (0.1564)\tAccu 0.9375 (0.9479)\t\n",
            "Epoch: [34][4/44]\tLoss 0.1396 (0.1522)\tAccu 0.9688 (0.9531)\t\n",
            "Epoch: [34][5/44]\tLoss 0.1060 (0.1430)\tAccu 0.9375 (0.9500)\t\n",
            "Epoch: [34][6/44]\tLoss 0.1939 (0.1515)\tAccu 0.9688 (0.9531)\t\n",
            "Epoch: [34][7/44]\tLoss 0.0851 (0.1420)\tAccu 1.0000 (0.9598)\t\n",
            "Epoch: [34][8/44]\tLoss 0.1069 (0.1376)\tAccu 0.9688 (0.9609)\t\n",
            "Epoch: [34][9/44]\tLoss 0.1860 (0.1430)\tAccu 0.9375 (0.9583)\t\n",
            "Epoch: [34][10/44]\tLoss 0.3560 (0.1643)\tAccu 0.9062 (0.9531)\t\n",
            "Epoch: [34][11/44]\tLoss 0.0895 (0.1575)\tAccu 1.0000 (0.9574)\t\n",
            "Epoch: [34][12/44]\tLoss 0.0855 (0.1515)\tAccu 0.9688 (0.9583)\t\n",
            "Epoch: [34][13/44]\tLoss 0.0766 (0.1457)\tAccu 1.0000 (0.9615)\t\n",
            "Epoch: [34][14/44]\tLoss 0.0884 (0.1416)\tAccu 1.0000 (0.9643)\t\n",
            "Epoch: [34][15/44]\tLoss 0.2477 (0.1487)\tAccu 0.9375 (0.9625)\t\n",
            "Epoch: [34][16/44]\tLoss 0.2242 (0.1534)\tAccu 0.9375 (0.9609)\t\n",
            "Epoch: [34][17/44]\tLoss 0.1145 (0.1511)\tAccu 0.9688 (0.9614)\t\n",
            "Epoch: [34][18/44]\tLoss 0.0696 (0.1466)\tAccu 1.0000 (0.9635)\t\n",
            "Epoch: [34][19/44]\tLoss 0.1993 (0.1494)\tAccu 0.9688 (0.9638)\t\n",
            "Epoch: [34][20/44]\tLoss 0.3019 (0.1570)\tAccu 0.9375 (0.9625)\t\n",
            "Epoch: [34][21/44]\tLoss 0.2377 (0.1608)\tAccu 0.9688 (0.9628)\t\n",
            "Epoch: [34][22/44]\tLoss 0.1597 (0.1608)\tAccu 0.9688 (0.9631)\t\n",
            "Epoch: [34][23/44]\tLoss 0.0722 (0.1569)\tAccu 1.0000 (0.9647)\t\n",
            "Epoch: [34][24/44]\tLoss 0.1027 (0.1547)\tAccu 1.0000 (0.9661)\t\n",
            "Epoch: [34][25/44]\tLoss 0.0927 (0.1522)\tAccu 1.0000 (0.9675)\t\n",
            "Epoch: [34][26/44]\tLoss 0.1525 (0.1522)\tAccu 0.9688 (0.9675)\t\n",
            "Epoch: [34][27/44]\tLoss 0.0609 (0.1488)\tAccu 1.0000 (0.9688)\t\n",
            "Epoch: [34][28/44]\tLoss 0.0838 (0.1465)\tAccu 1.0000 (0.9699)\t\n",
            "Epoch: [34][29/44]\tLoss 0.0742 (0.1440)\tAccu 1.0000 (0.9709)\t\n",
            "Epoch: [34][30/44]\tLoss 0.1238 (0.1433)\tAccu 1.0000 (0.9719)\t\n",
            "Epoch: [34][31/44]\tLoss 0.1072 (0.1422)\tAccu 1.0000 (0.9728)\t\n",
            "Epoch: [34][32/44]\tLoss 0.0714 (0.1400)\tAccu 1.0000 (0.9736)\t\n",
            "Epoch: [34][33/44]\tLoss 0.0421 (0.1370)\tAccu 1.0000 (0.9744)\t\n",
            "Epoch: [34][34/44]\tLoss 0.0553 (0.1346)\tAccu 1.0000 (0.9752)\t\n",
            "Epoch: [34][35/44]\tLoss 0.1145 (0.1340)\tAccu 1.0000 (0.9759)\t\n",
            "Epoch: [34][36/44]\tLoss 0.1474 (0.1344)\tAccu 0.9375 (0.9748)\t\n",
            "Epoch: [34][37/44]\tLoss 0.0297 (0.1316)\tAccu 1.0000 (0.9755)\t\n",
            "Epoch: [34][38/44]\tLoss 0.1295 (0.1315)\tAccu 0.9688 (0.9753)\t\n",
            "Epoch: [34][39/44]\tLoss 0.1320 (0.1315)\tAccu 0.9688 (0.9752)\t\n",
            "Epoch: [34][40/44]\tLoss 0.0609 (0.1298)\tAccu 1.0000 (0.9758)\t\n",
            "Epoch: [34][41/44]\tLoss 0.0292 (0.1273)\tAccu 1.0000 (0.9764)\t\n",
            "Epoch: [34][42/44]\tLoss 0.1105 (0.1269)\tAccu 0.9688 (0.9762)\t\n",
            "Epoch: [34][43/44]\tLoss 0.0402 (0.1249)\tAccu 1.0000 (0.9767)\t\n",
            "Epoch: [34][44/44]\tLoss 0.0307 (0.1233)\tAccu 1.0000 (0.9773)\t\n",
            "Accu 0.2075\t\n",
            "Epoch: [35][1/44]\tLoss 0.1450 (0.1450)\tAccu 0.9688 (0.9688)\t\n",
            "Epoch: [35][2/44]\tLoss 0.0544 (0.0997)\tAccu 0.9688 (0.9688)\t\n",
            "Epoch: [35][3/44]\tLoss 0.1442 (0.1145)\tAccu 0.9688 (0.9688)\t\n",
            "Epoch: [35][4/44]\tLoss 0.0839 (0.1069)\tAccu 1.0000 (0.9766)\t\n",
            "Epoch: [35][5/44]\tLoss 0.1403 (0.1136)\tAccu 0.9688 (0.9750)\t\n",
            "Epoch: [35][6/44]\tLoss 0.2655 (0.1389)\tAccu 0.8438 (0.9531)\t\n",
            "Epoch: [35][7/44]\tLoss 0.0434 (0.1252)\tAccu 1.0000 (0.9598)\t\n",
            "Epoch: [35][8/44]\tLoss 0.0590 (0.1169)\tAccu 1.0000 (0.9648)\t\n",
            "Epoch: [35][9/44]\tLoss 0.0652 (0.1112)\tAccu 1.0000 (0.9688)\t\n",
            "Epoch: [35][10/44]\tLoss 0.1485 (0.1149)\tAccu 0.9688 (0.9688)\t\n",
            "Epoch: [35][11/44]\tLoss 0.1303 (0.1163)\tAccu 0.9688 (0.9688)\t\n",
            "Epoch: [35][12/44]\tLoss 0.1305 (0.1175)\tAccu 0.9688 (0.9688)\t\n",
            "Epoch: [35][13/44]\tLoss 0.0742 (0.1142)\tAccu 1.0000 (0.9712)\t\n",
            "Epoch: [35][14/44]\tLoss 0.0438 (0.1092)\tAccu 1.0000 (0.9732)\t\n",
            "Epoch: [35][15/44]\tLoss 0.0870 (0.1077)\tAccu 0.9688 (0.9729)\t\n",
            "Epoch: [35][16/44]\tLoss 0.0697 (0.1053)\tAccu 0.9688 (0.9727)\t\n",
            "Epoch: [35][17/44]\tLoss 0.3072 (0.1172)\tAccu 0.9062 (0.9688)\t\n",
            "Epoch: [35][18/44]\tLoss 0.3100 (0.1279)\tAccu 0.9062 (0.9653)\t\n",
            "Epoch: [35][19/44]\tLoss 0.0785 (0.1253)\tAccu 1.0000 (0.9671)\t\n",
            "Epoch: [35][20/44]\tLoss 0.1127 (0.1247)\tAccu 0.9688 (0.9672)\t\n",
            "Epoch: [35][21/44]\tLoss 0.0672 (0.1219)\tAccu 1.0000 (0.9688)\t\n",
            "Epoch: [35][22/44]\tLoss 0.0521 (0.1188)\tAccu 1.0000 (0.9702)\t\n",
            "Epoch: [35][23/44]\tLoss 0.0896 (0.1175)\tAccu 0.9688 (0.9701)\t\n",
            "Epoch: [35][24/44]\tLoss 0.3643 (0.1278)\tAccu 0.8125 (0.9635)\t\n",
            "Epoch: [35][25/44]\tLoss 0.2274 (0.1318)\tAccu 0.8750 (0.9600)\t\n",
            "Epoch: [35][26/44]\tLoss 0.0762 (0.1296)\tAccu 1.0000 (0.9615)\t\n",
            "Epoch: [35][27/44]\tLoss 0.0912 (0.1282)\tAccu 1.0000 (0.9630)\t\n",
            "Epoch: [35][28/44]\tLoss 0.0279 (0.1246)\tAccu 1.0000 (0.9643)\t\n",
            "Epoch: [35][29/44]\tLoss 0.0511 (0.1221)\tAccu 1.0000 (0.9655)\t\n",
            "Epoch: [35][30/44]\tLoss 0.0472 (0.1196)\tAccu 1.0000 (0.9667)\t\n",
            "Epoch: [35][31/44]\tLoss 0.0838 (0.1184)\tAccu 0.9688 (0.9667)\t\n",
            "Epoch: [35][32/44]\tLoss 0.0445 (0.1161)\tAccu 1.0000 (0.9678)\t\n",
            "Epoch: [35][33/44]\tLoss 0.1310 (0.1166)\tAccu 0.9375 (0.9669)\t\n",
            "Epoch: [35][34/44]\tLoss 0.1019 (0.1161)\tAccu 0.9688 (0.9669)\t\n",
            "Epoch: [35][35/44]\tLoss 0.1686 (0.1176)\tAccu 0.9688 (0.9670)\t\n",
            "Epoch: [35][36/44]\tLoss 0.0632 (0.1161)\tAccu 1.0000 (0.9679)\t\n",
            "Epoch: [35][37/44]\tLoss 0.1438 (0.1169)\tAccu 0.9688 (0.9679)\t\n",
            "Epoch: [35][38/44]\tLoss 0.0811 (0.1159)\tAccu 0.9688 (0.9679)\t\n",
            "Epoch: [35][39/44]\tLoss 0.0419 (0.1140)\tAccu 1.0000 (0.9688)\t\n",
            "Epoch: [35][40/44]\tLoss 0.0439 (0.1123)\tAccu 1.0000 (0.9695)\t\n",
            "Epoch: [35][41/44]\tLoss 0.0444 (0.1106)\tAccu 1.0000 (0.9703)\t\n",
            "Epoch: [35][42/44]\tLoss 0.0501 (0.1092)\tAccu 1.0000 (0.9710)\t\n",
            "Epoch: [35][43/44]\tLoss 0.0734 (0.1083)\tAccu 1.0000 (0.9717)\t\n",
            "Epoch: [35][44/44]\tLoss 0.0669 (0.1076)\tAccu 0.9583 (0.9714)\t\n",
            "Accu 0.2175\t\n",
            "Epoch: [36][1/44]\tLoss 0.0716 (0.0716)\tAccu 0.9688 (0.9688)\t\n",
            "Epoch: [36][2/44]\tLoss 0.0378 (0.0547)\tAccu 1.0000 (0.9844)\t\n",
            "Epoch: [36][3/44]\tLoss 0.1262 (0.0785)\tAccu 0.9688 (0.9792)\t\n",
            "Epoch: [36][4/44]\tLoss 0.0695 (0.0762)\tAccu 1.0000 (0.9844)\t\n",
            "Epoch: [36][5/44]\tLoss 0.0561 (0.0722)\tAccu 1.0000 (0.9875)\t\n",
            "Epoch: [36][6/44]\tLoss 0.0969 (0.0763)\tAccu 0.9688 (0.9844)\t\n",
            "Epoch: [36][7/44]\tLoss 0.1208 (0.0827)\tAccu 0.9688 (0.9821)\t\n",
            "Epoch: [36][8/44]\tLoss 0.1167 (0.0869)\tAccu 0.9375 (0.9766)\t\n",
            "Epoch: [36][9/44]\tLoss 0.1931 (0.0987)\tAccu 0.9375 (0.9722)\t\n",
            "Epoch: [36][10/44]\tLoss 0.1233 (0.1012)\tAccu 0.9688 (0.9719)\t\n",
            "Epoch: [36][11/44]\tLoss 0.0604 (0.0975)\tAccu 1.0000 (0.9744)\t\n",
            "Epoch: [36][12/44]\tLoss 0.0662 (0.0949)\tAccu 1.0000 (0.9766)\t\n",
            "Epoch: [36][13/44]\tLoss 0.1438 (0.0986)\tAccu 0.9375 (0.9736)\t\n",
            "Epoch: [36][14/44]\tLoss 0.1006 (0.0988)\tAccu 0.9688 (0.9732)\t\n",
            "Epoch: [36][15/44]\tLoss 0.1014 (0.0989)\tAccu 0.9688 (0.9729)\t\n",
            "Epoch: [36][16/44]\tLoss 0.0223 (0.0942)\tAccu 1.0000 (0.9746)\t\n",
            "Epoch: [36][17/44]\tLoss 0.0433 (0.0912)\tAccu 1.0000 (0.9761)\t\n",
            "Epoch: [36][18/44]\tLoss 0.0286 (0.0877)\tAccu 1.0000 (0.9774)\t\n",
            "Epoch: [36][19/44]\tLoss 0.4733 (0.1080)\tAccu 0.9062 (0.9737)\t\n",
            "Epoch: [36][20/44]\tLoss 0.1688 (0.1110)\tAccu 0.9375 (0.9719)\t\n",
            "Epoch: [36][21/44]\tLoss 0.0669 (0.1089)\tAccu 0.9688 (0.9717)\t\n",
            "Epoch: [36][22/44]\tLoss 0.1044 (0.1087)\tAccu 0.9688 (0.9716)\t\n",
            "Epoch: [36][23/44]\tLoss 0.0151 (0.1047)\tAccu 1.0000 (0.9728)\t\n",
            "Epoch: [36][24/44]\tLoss 0.0445 (0.1021)\tAccu 1.0000 (0.9740)\t\n",
            "Epoch: [36][25/44]\tLoss 0.3004 (0.1101)\tAccu 0.8750 (0.9700)\t\n",
            "Epoch: [36][26/44]\tLoss 0.2902 (0.1170)\tAccu 0.9375 (0.9688)\t\n",
            "Epoch: [36][27/44]\tLoss 0.2566 (0.1222)\tAccu 0.9062 (0.9664)\t\n",
            "Epoch: [36][28/44]\tLoss 0.4073 (0.1324)\tAccu 0.9062 (0.9643)\t\n",
            "Epoch: [36][29/44]\tLoss 0.1011 (0.1313)\tAccu 0.9688 (0.9644)\t\n",
            "Epoch: [36][30/44]\tLoss 0.0825 (0.1297)\tAccu 1.0000 (0.9656)\t\n",
            "Epoch: [36][31/44]\tLoss 0.1102 (0.1290)\tAccu 0.9688 (0.9657)\t\n",
            "Epoch: [36][32/44]\tLoss 0.1776 (0.1305)\tAccu 0.9375 (0.9648)\t\n",
            "Epoch: [36][33/44]\tLoss 0.0757 (0.1289)\tAccu 1.0000 (0.9659)\t\n",
            "Epoch: [36][34/44]\tLoss 0.0825 (0.1275)\tAccu 1.0000 (0.9669)\t\n",
            "Epoch: [36][35/44]\tLoss 0.1716 (0.1288)\tAccu 0.9688 (0.9670)\t\n",
            "Epoch: [36][36/44]\tLoss 0.4317 (0.1372)\tAccu 0.9375 (0.9661)\t\n",
            "Epoch: [36][37/44]\tLoss 0.1334 (0.1371)\tAccu 0.9688 (0.9662)\t\n",
            "Epoch: [36][38/44]\tLoss 0.5553 (0.1481)\tAccu 0.8750 (0.9638)\t\n",
            "Epoch: [36][39/44]\tLoss 0.5485 (0.1584)\tAccu 0.8750 (0.9615)\t\n",
            "Epoch: [36][40/44]\tLoss 0.4064 (0.1646)\tAccu 0.9375 (0.9609)\t\n",
            "Epoch: [36][41/44]\tLoss 0.3233 (0.1684)\tAccu 0.9375 (0.9604)\t\n",
            "Epoch: [36][42/44]\tLoss 0.0753 (0.1662)\tAccu 1.0000 (0.9613)\t\n",
            "Epoch: [36][43/44]\tLoss 0.0819 (0.1643)\tAccu 1.0000 (0.9622)\t\n",
            "Epoch: [36][44/44]\tLoss 0.0889 (0.1630)\tAccu 1.0000 (0.9631)\t\n",
            "Accu 0.1950\t\n",
            "Epoch: [37][1/44]\tLoss 0.1104 (0.1104)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [37][2/44]\tLoss 0.1603 (0.1354)\tAccu 0.9375 (0.9688)\t\n",
            "Epoch: [37][3/44]\tLoss 0.3728 (0.2145)\tAccu 0.9062 (0.9479)\t\n",
            "Epoch: [37][4/44]\tLoss 0.2295 (0.2183)\tAccu 0.9375 (0.9453)\t\n",
            "Epoch: [37][5/44]\tLoss 0.1183 (0.1983)\tAccu 1.0000 (0.9563)\t\n",
            "Epoch: [37][6/44]\tLoss 0.3948 (0.2310)\tAccu 0.9062 (0.9479)\t\n",
            "Epoch: [37][7/44]\tLoss 0.2063 (0.2275)\tAccu 0.9688 (0.9509)\t\n",
            "Epoch: [37][8/44]\tLoss 0.2029 (0.2244)\tAccu 0.9375 (0.9492)\t\n",
            "Epoch: [37][9/44]\tLoss 0.2444 (0.2266)\tAccu 0.9375 (0.9479)\t\n",
            "Epoch: [37][10/44]\tLoss 0.2680 (0.2308)\tAccu 0.9062 (0.9437)\t\n",
            "Epoch: [37][11/44]\tLoss 0.1305 (0.2217)\tAccu 0.9688 (0.9460)\t\n",
            "Epoch: [37][12/44]\tLoss 0.0700 (0.2090)\tAccu 1.0000 (0.9505)\t\n",
            "Epoch: [37][13/44]\tLoss 0.0403 (0.1960)\tAccu 1.0000 (0.9543)\t\n",
            "Epoch: [37][14/44]\tLoss 0.0932 (0.1887)\tAccu 0.9688 (0.9554)\t\n",
            "Epoch: [37][15/44]\tLoss 0.1570 (0.1866)\tAccu 0.9375 (0.9542)\t\n",
            "Epoch: [37][16/44]\tLoss 0.1904 (0.1868)\tAccu 0.9688 (0.9551)\t\n",
            "Epoch: [37][17/44]\tLoss 0.2891 (0.1928)\tAccu 0.8750 (0.9504)\t\n",
            "Epoch: [37][18/44]\tLoss 0.1605 (0.1910)\tAccu 0.9688 (0.9514)\t\n",
            "Epoch: [37][19/44]\tLoss 0.0710 (0.1847)\tAccu 1.0000 (0.9539)\t\n",
            "Epoch: [37][20/44]\tLoss 0.2020 (0.1856)\tAccu 0.9375 (0.9531)\t\n",
            "Epoch: [37][21/44]\tLoss 0.1753 (0.1851)\tAccu 0.9375 (0.9524)\t\n",
            "Epoch: [37][22/44]\tLoss 0.0886 (0.1807)\tAccu 1.0000 (0.9545)\t\n",
            "Epoch: [37][23/44]\tLoss 0.0252 (0.1740)\tAccu 1.0000 (0.9565)\t\n",
            "Epoch: [37][24/44]\tLoss 0.0224 (0.1676)\tAccu 1.0000 (0.9583)\t\n",
            "Epoch: [37][25/44]\tLoss 0.0758 (0.1640)\tAccu 1.0000 (0.9600)\t\n",
            "Epoch: [37][26/44]\tLoss 0.3101 (0.1696)\tAccu 0.9688 (0.9603)\t\n",
            "Epoch: [37][27/44]\tLoss 0.0746 (0.1661)\tAccu 1.0000 (0.9618)\t\n",
            "Epoch: [37][28/44]\tLoss 0.0991 (0.1637)\tAccu 1.0000 (0.9632)\t\n",
            "Epoch: [37][29/44]\tLoss 0.0912 (0.1612)\tAccu 0.9688 (0.9634)\t\n",
            "Epoch: [37][30/44]\tLoss 0.1110 (0.1595)\tAccu 0.9375 (0.9625)\t\n",
            "Epoch: [37][31/44]\tLoss 0.2738 (0.1632)\tAccu 0.9062 (0.9607)\t\n",
            "Epoch: [37][32/44]\tLoss 0.1884 (0.1640)\tAccu 0.9375 (0.9600)\t\n",
            "Epoch: [37][33/44]\tLoss 0.0505 (0.1605)\tAccu 1.0000 (0.9612)\t\n",
            "Epoch: [37][34/44]\tLoss 0.1035 (0.1589)\tAccu 1.0000 (0.9623)\t\n",
            "Epoch: [37][35/44]\tLoss 0.1353 (0.1582)\tAccu 0.9375 (0.9616)\t\n",
            "Epoch: [37][36/44]\tLoss 0.1470 (0.1579)\tAccu 0.9688 (0.9618)\t\n",
            "Epoch: [37][37/44]\tLoss 0.1338 (0.1572)\tAccu 0.9688 (0.9620)\t\n",
            "Epoch: [37][38/44]\tLoss 0.1796 (0.1578)\tAccu 0.9688 (0.9622)\t\n",
            "Epoch: [37][39/44]\tLoss 0.1211 (0.1569)\tAccu 0.9688 (0.9623)\t\n",
            "Epoch: [37][40/44]\tLoss 0.2296 (0.1587)\tAccu 0.9688 (0.9625)\t\n",
            "Epoch: [37][41/44]\tLoss 0.1260 (0.1579)\tAccu 0.9375 (0.9619)\t\n",
            "Epoch: [37][42/44]\tLoss 0.4927 (0.1659)\tAccu 0.8750 (0.9598)\t\n",
            "Epoch: [37][43/44]\tLoss 0.3627 (0.1704)\tAccu 0.9062 (0.9586)\t\n",
            "Epoch: [37][44/44]\tLoss 0.0417 (0.1682)\tAccu 1.0000 (0.9595)\t\n",
            "Accu 0.2100\t\n",
            "Epoch: [38][1/44]\tLoss 0.0552 (0.0552)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [38][2/44]\tLoss 0.2687 (0.1619)\tAccu 0.9062 (0.9531)\t\n",
            "Epoch: [38][3/44]\tLoss 0.0782 (0.1340)\tAccu 1.0000 (0.9688)\t\n",
            "Epoch: [38][4/44]\tLoss 0.1237 (0.1314)\tAccu 0.9688 (0.9688)\t\n",
            "Epoch: [38][5/44]\tLoss 0.0951 (0.1242)\tAccu 0.9688 (0.9688)\t\n",
            "Epoch: [38][6/44]\tLoss 0.2845 (0.1509)\tAccu 0.8750 (0.9531)\t\n",
            "Epoch: [38][7/44]\tLoss 0.1527 (0.1511)\tAccu 0.9688 (0.9554)\t\n",
            "Epoch: [38][8/44]\tLoss 0.4099 (0.1835)\tAccu 0.9062 (0.9492)\t\n",
            "Epoch: [38][9/44]\tLoss 0.6053 (0.2303)\tAccu 0.7812 (0.9306)\t\n",
            "Epoch: [38][10/44]\tLoss 0.2865 (0.2360)\tAccu 0.9688 (0.9344)\t\n",
            "Epoch: [38][11/44]\tLoss 0.2743 (0.2394)\tAccu 0.8750 (0.9290)\t\n",
            "Epoch: [38][12/44]\tLoss 0.1813 (0.2346)\tAccu 0.9688 (0.9323)\t\n",
            "Epoch: [38][13/44]\tLoss 0.2135 (0.2330)\tAccu 0.9375 (0.9327)\t\n",
            "Epoch: [38][14/44]\tLoss 0.0542 (0.2202)\tAccu 1.0000 (0.9375)\t\n",
            "Epoch: [38][15/44]\tLoss 0.2507 (0.2222)\tAccu 0.9375 (0.9375)\t\n",
            "Epoch: [38][16/44]\tLoss 0.1378 (0.2170)\tAccu 0.9688 (0.9395)\t\n",
            "Epoch: [38][17/44]\tLoss 0.2715 (0.2202)\tAccu 0.8750 (0.9357)\t\n",
            "Epoch: [38][18/44]\tLoss 0.3502 (0.2274)\tAccu 0.9062 (0.9340)\t\n",
            "Epoch: [38][19/44]\tLoss 0.6335 (0.2488)\tAccu 0.8750 (0.9309)\t\n",
            "Epoch: [38][20/44]\tLoss 0.2077 (0.2467)\tAccu 0.9375 (0.9313)\t\n",
            "Epoch: [38][21/44]\tLoss 0.2654 (0.2476)\tAccu 0.9375 (0.9315)\t\n",
            "Epoch: [38][22/44]\tLoss 0.2601 (0.2482)\tAccu 0.9062 (0.9304)\t\n",
            "Epoch: [38][23/44]\tLoss 0.0435 (0.2393)\tAccu 1.0000 (0.9334)\t\n",
            "Epoch: [38][24/44]\tLoss 0.0513 (0.2314)\tAccu 1.0000 (0.9362)\t\n",
            "Epoch: [38][25/44]\tLoss 0.0772 (0.2253)\tAccu 1.0000 (0.9387)\t\n",
            "Epoch: [38][26/44]\tLoss 0.0730 (0.2194)\tAccu 1.0000 (0.9411)\t\n",
            "Epoch: [38][27/44]\tLoss 0.0818 (0.2143)\tAccu 1.0000 (0.9433)\t\n",
            "Epoch: [38][28/44]\tLoss 0.2039 (0.2139)\tAccu 0.9375 (0.9431)\t\n",
            "Epoch: [38][29/44]\tLoss 0.3417 (0.2183)\tAccu 0.8438 (0.9397)\t\n",
            "Epoch: [38][30/44]\tLoss 0.4301 (0.2254)\tAccu 0.8438 (0.9365)\t\n",
            "Epoch: [38][31/44]\tLoss 0.0938 (0.2212)\tAccu 0.9688 (0.9375)\t\n",
            "Epoch: [38][32/44]\tLoss 0.0603 (0.2161)\tAccu 1.0000 (0.9395)\t\n",
            "Epoch: [38][33/44]\tLoss 0.0861 (0.2122)\tAccu 0.9688 (0.9403)\t\n",
            "Epoch: [38][34/44]\tLoss 0.0987 (0.2089)\tAccu 1.0000 (0.9421)\t\n",
            "Epoch: [38][35/44]\tLoss 0.1063 (0.2059)\tAccu 0.9688 (0.9429)\t\n",
            "Epoch: [38][36/44]\tLoss 0.1291 (0.2038)\tAccu 0.9375 (0.9427)\t\n",
            "Epoch: [38][37/44]\tLoss 0.0865 (0.2006)\tAccu 1.0000 (0.9443)\t\n",
            "Epoch: [38][38/44]\tLoss 0.1693 (0.1998)\tAccu 0.9688 (0.9449)\t\n",
            "Epoch: [38][39/44]\tLoss 0.0905 (0.1970)\tAccu 1.0000 (0.9463)\t\n",
            "Epoch: [38][40/44]\tLoss 0.0880 (0.1943)\tAccu 0.9688 (0.9469)\t\n",
            "Epoch: [38][41/44]\tLoss 0.1302 (0.1927)\tAccu 0.9688 (0.9474)\t\n",
            "Epoch: [38][42/44]\tLoss 0.3185 (0.1957)\tAccu 0.8750 (0.9457)\t\n",
            "Epoch: [38][43/44]\tLoss 0.0431 (0.1922)\tAccu 1.0000 (0.9469)\t\n",
            "Epoch: [38][44/44]\tLoss 0.0573 (0.1898)\tAccu 0.9583 (0.9472)\t\n",
            "Accu 0.1975\t\n",
            "Epoch: [39][1/44]\tLoss 0.2757 (0.2757)\tAccu 0.9688 (0.9688)\t\n",
            "Epoch: [39][2/44]\tLoss 0.1206 (0.1982)\tAccu 0.9375 (0.9531)\t\n",
            "Epoch: [39][3/44]\tLoss 0.2853 (0.2272)\tAccu 0.9062 (0.9375)\t\n",
            "Epoch: [39][4/44]\tLoss 0.1404 (0.2055)\tAccu 0.9688 (0.9453)\t\n",
            "Epoch: [39][5/44]\tLoss 0.0634 (0.1771)\tAccu 1.0000 (0.9563)\t\n",
            "Epoch: [39][6/44]\tLoss 0.3310 (0.2027)\tAccu 0.9375 (0.9531)\t\n",
            "Epoch: [39][7/44]\tLoss 0.1878 (0.2006)\tAccu 0.9375 (0.9509)\t\n",
            "Epoch: [39][8/44]\tLoss 0.0828 (0.1859)\tAccu 1.0000 (0.9570)\t\n",
            "Epoch: [39][9/44]\tLoss 0.2208 (0.1898)\tAccu 0.8750 (0.9479)\t\n",
            "Epoch: [39][10/44]\tLoss 0.3375 (0.2045)\tAccu 0.9062 (0.9437)\t\n",
            "Epoch: [39][11/44]\tLoss 0.6194 (0.2422)\tAccu 0.8438 (0.9347)\t\n",
            "Epoch: [39][12/44]\tLoss 0.4514 (0.2597)\tAccu 0.8438 (0.9271)\t\n",
            "Epoch: [39][13/44]\tLoss 0.1903 (0.2543)\tAccu 0.9375 (0.9279)\t\n",
            "Epoch: [39][14/44]\tLoss 0.0679 (0.2410)\tAccu 0.9688 (0.9308)\t\n",
            "Epoch: [39][15/44]\tLoss 0.2543 (0.2419)\tAccu 0.9688 (0.9333)\t\n",
            "Epoch: [39][16/44]\tLoss 0.2868 (0.2447)\tAccu 0.9062 (0.9316)\t\n",
            "Epoch: [39][17/44]\tLoss 0.3880 (0.2531)\tAccu 0.9062 (0.9301)\t\n",
            "Epoch: [39][18/44]\tLoss 0.1343 (0.2465)\tAccu 0.9688 (0.9323)\t\n",
            "Epoch: [39][19/44]\tLoss 0.0991 (0.2388)\tAccu 1.0000 (0.9359)\t\n",
            "Epoch: [39][20/44]\tLoss 0.2205 (0.2379)\tAccu 0.9375 (0.9359)\t\n",
            "Epoch: [39][21/44]\tLoss 0.1563 (0.2340)\tAccu 1.0000 (0.9390)\t\n",
            "Epoch: [39][22/44]\tLoss 0.2666 (0.2355)\tAccu 0.9062 (0.9375)\t\n",
            "Epoch: [39][23/44]\tLoss 0.1433 (0.2315)\tAccu 0.9688 (0.9389)\t\n",
            "Epoch: [39][24/44]\tLoss 0.2361 (0.2317)\tAccu 0.9375 (0.9388)\t\n",
            "Epoch: [39][25/44]\tLoss 0.0270 (0.2235)\tAccu 1.0000 (0.9413)\t\n",
            "Epoch: [39][26/44]\tLoss 0.2495 (0.2245)\tAccu 0.9375 (0.9411)\t\n",
            "Epoch: [39][27/44]\tLoss 0.4344 (0.2322)\tAccu 0.9062 (0.9398)\t\n",
            "Epoch: [39][28/44]\tLoss 0.1258 (0.2284)\tAccu 0.9375 (0.9397)\t\n",
            "Epoch: [39][29/44]\tLoss 0.1062 (0.2242)\tAccu 0.9688 (0.9407)\t\n",
            "Epoch: [39][30/44]\tLoss 0.1041 (0.2202)\tAccu 0.9375 (0.9406)\t\n",
            "Epoch: [39][31/44]\tLoss 0.0587 (0.2150)\tAccu 1.0000 (0.9425)\t\n",
            "Epoch: [39][32/44]\tLoss 0.0697 (0.2105)\tAccu 0.9688 (0.9434)\t\n",
            "Epoch: [39][33/44]\tLoss 0.1459 (0.2085)\tAccu 0.9688 (0.9441)\t\n",
            "Epoch: [39][34/44]\tLoss 0.2912 (0.2109)\tAccu 0.9062 (0.9430)\t\n",
            "Epoch: [39][35/44]\tLoss 0.2973 (0.2134)\tAccu 0.9062 (0.9420)\t\n",
            "Epoch: [39][36/44]\tLoss 0.1927 (0.2128)\tAccu 0.9375 (0.9418)\t\n",
            "Epoch: [39][37/44]\tLoss 0.0928 (0.2096)\tAccu 0.9688 (0.9426)\t\n",
            "Epoch: [39][38/44]\tLoss 0.1050 (0.2068)\tAccu 0.9688 (0.9433)\t\n",
            "Epoch: [39][39/44]\tLoss 0.0761 (0.2035)\tAccu 1.0000 (0.9447)\t\n",
            "Epoch: [39][40/44]\tLoss 0.0364 (0.1993)\tAccu 1.0000 (0.9461)\t\n",
            "Epoch: [39][41/44]\tLoss 0.0232 (0.1950)\tAccu 1.0000 (0.9474)\t\n",
            "Epoch: [39][42/44]\tLoss 0.0483 (0.1915)\tAccu 1.0000 (0.9487)\t\n",
            "Epoch: [39][43/44]\tLoss 0.1071 (0.1896)\tAccu 0.9688 (0.9491)\t\n",
            "Epoch: [39][44/44]\tLoss 0.0102 (0.1865)\tAccu 1.0000 (0.9503)\t\n",
            "Accu 0.1950\t\n",
            "Epoch: [40][1/44]\tLoss 0.0401 (0.0401)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [40][2/44]\tLoss 0.6823 (0.3612)\tAccu 0.8438 (0.9219)\t\n",
            "Epoch: [40][3/44]\tLoss 0.0730 (0.2651)\tAccu 1.0000 (0.9479)\t\n",
            "Epoch: [40][4/44]\tLoss 0.4505 (0.3115)\tAccu 0.8750 (0.9297)\t\n",
            "Epoch: [40][5/44]\tLoss 0.0504 (0.2592)\tAccu 0.9688 (0.9375)\t\n",
            "Epoch: [40][6/44]\tLoss 0.1368 (0.2388)\tAccu 0.9375 (0.9375)\t\n",
            "Epoch: [40][7/44]\tLoss 0.0266 (0.2085)\tAccu 1.0000 (0.9464)\t\n",
            "Epoch: [40][8/44]\tLoss 0.0668 (0.1908)\tAccu 0.9688 (0.9492)\t\n",
            "Epoch: [40][9/44]\tLoss 0.0907 (0.1797)\tAccu 1.0000 (0.9549)\t\n",
            "Epoch: [40][10/44]\tLoss 0.1247 (0.1742)\tAccu 0.9688 (0.9563)\t\n",
            "Epoch: [40][11/44]\tLoss 0.0345 (0.1615)\tAccu 1.0000 (0.9602)\t\n",
            "Epoch: [40][12/44]\tLoss 0.0877 (0.1553)\tAccu 0.9688 (0.9609)\t\n",
            "Epoch: [40][13/44]\tLoss 0.1015 (0.1512)\tAccu 0.9688 (0.9615)\t\n",
            "Epoch: [40][14/44]\tLoss 0.2230 (0.1563)\tAccu 0.9375 (0.9598)\t\n",
            "Epoch: [40][15/44]\tLoss 0.6268 (0.1877)\tAccu 0.8438 (0.9521)\t\n",
            "Epoch: [40][16/44]\tLoss 0.1092 (0.1828)\tAccu 0.9375 (0.9512)\t\n",
            "Epoch: [40][17/44]\tLoss 0.0813 (0.1768)\tAccu 0.9688 (0.9522)\t\n",
            "Epoch: [40][18/44]\tLoss 0.1830 (0.1772)\tAccu 0.9688 (0.9531)\t\n",
            "Epoch: [40][19/44]\tLoss 0.0921 (0.1727)\tAccu 1.0000 (0.9556)\t\n",
            "Epoch: [40][20/44]\tLoss 0.0833 (0.1682)\tAccu 1.0000 (0.9578)\t\n",
            "Epoch: [40][21/44]\tLoss 0.1008 (0.1650)\tAccu 1.0000 (0.9598)\t\n",
            "Epoch: [40][22/44]\tLoss 0.1481 (0.1642)\tAccu 0.9688 (0.9602)\t\n",
            "Epoch: [40][23/44]\tLoss 0.0359 (0.1587)\tAccu 1.0000 (0.9620)\t\n",
            "Epoch: [40][24/44]\tLoss 0.1125 (0.1567)\tAccu 0.9688 (0.9622)\t\n",
            "Epoch: [40][25/44]\tLoss 0.0354 (0.1519)\tAccu 1.0000 (0.9637)\t\n",
            "Epoch: [40][26/44]\tLoss 0.1992 (0.1537)\tAccu 0.9688 (0.9639)\t\n",
            "Epoch: [40][27/44]\tLoss 0.1428 (0.1533)\tAccu 0.9688 (0.9641)\t\n",
            "Epoch: [40][28/44]\tLoss 0.0533 (0.1497)\tAccu 0.9688 (0.9643)\t\n",
            "Epoch: [40][29/44]\tLoss 0.0459 (0.1461)\tAccu 1.0000 (0.9655)\t\n",
            "Epoch: [40][30/44]\tLoss 0.0475 (0.1429)\tAccu 1.0000 (0.9667)\t\n",
            "Epoch: [40][31/44]\tLoss 0.4277 (0.1520)\tAccu 0.9062 (0.9647)\t\n",
            "Epoch: [40][32/44]\tLoss 0.3390 (0.1579)\tAccu 0.9375 (0.9639)\t\n",
            "Epoch: [40][33/44]\tLoss 0.3319 (0.1632)\tAccu 0.9375 (0.9631)\t\n",
            "Epoch: [40][34/44]\tLoss 0.0157 (0.1588)\tAccu 1.0000 (0.9642)\t\n",
            "Epoch: [40][35/44]\tLoss 0.0815 (0.1566)\tAccu 1.0000 (0.9652)\t\n",
            "Epoch: [40][36/44]\tLoss 0.0532 (0.1537)\tAccu 0.9688 (0.9653)\t\n",
            "Epoch: [40][37/44]\tLoss 0.0908 (0.1520)\tAccu 0.9688 (0.9654)\t\n",
            "Epoch: [40][38/44]\tLoss 0.0721 (0.1499)\tAccu 1.0000 (0.9663)\t\n",
            "Epoch: [40][39/44]\tLoss 0.0809 (0.1482)\tAccu 1.0000 (0.9671)\t\n",
            "Epoch: [40][40/44]\tLoss 0.1116 (0.1472)\tAccu 0.9688 (0.9672)\t\n",
            "Epoch: [40][41/44]\tLoss 0.0982 (0.1461)\tAccu 0.9688 (0.9672)\t\n",
            "Epoch: [40][42/44]\tLoss 0.0421 (0.1436)\tAccu 1.0000 (0.9680)\t\n",
            "Epoch: [40][43/44]\tLoss 0.0769 (0.1420)\tAccu 1.0000 (0.9688)\t\n",
            "Epoch: [40][44/44]\tLoss 0.1001 (0.1413)\tAccu 0.9167 (0.9676)\t\n",
            "Accu 0.2050\t\n",
            "Epoch: [41][1/44]\tLoss 0.1599 (0.1599)\tAccu 0.9375 (0.9375)\t\n",
            "Epoch: [41][2/44]\tLoss 0.0251 (0.0925)\tAccu 1.0000 (0.9688)\t\n",
            "Epoch: [41][3/44]\tLoss 0.0479 (0.0776)\tAccu 1.0000 (0.9792)\t\n",
            "Epoch: [41][4/44]\tLoss 0.1253 (0.0896)\tAccu 0.9688 (0.9766)\t\n",
            "Epoch: [41][5/44]\tLoss 0.2085 (0.1134)\tAccu 0.9375 (0.9688)\t\n",
            "Epoch: [41][6/44]\tLoss 0.1131 (0.1133)\tAccu 0.9688 (0.9688)\t\n",
            "Epoch: [41][7/44]\tLoss 0.0560 (0.1051)\tAccu 1.0000 (0.9732)\t\n",
            "Epoch: [41][8/44]\tLoss 0.3823 (0.1398)\tAccu 0.9375 (0.9688)\t\n",
            "Epoch: [41][9/44]\tLoss 0.1970 (0.1461)\tAccu 0.9688 (0.9688)\t\n",
            "Epoch: [41][10/44]\tLoss 0.2159 (0.1531)\tAccu 0.9688 (0.9688)\t\n",
            "Epoch: [41][11/44]\tLoss 0.0285 (0.1418)\tAccu 1.0000 (0.9716)\t\n",
            "Epoch: [41][12/44]\tLoss 0.5737 (0.1778)\tAccu 0.9375 (0.9688)\t\n",
            "Epoch: [41][13/44]\tLoss 0.0524 (0.1681)\tAccu 1.0000 (0.9712)\t\n",
            "Epoch: [41][14/44]\tLoss 0.1480 (0.1667)\tAccu 0.9375 (0.9688)\t\n",
            "Epoch: [41][15/44]\tLoss 0.1562 (0.1660)\tAccu 0.9688 (0.9688)\t\n",
            "Epoch: [41][16/44]\tLoss 0.0246 (0.1572)\tAccu 1.0000 (0.9707)\t\n",
            "Epoch: [41][17/44]\tLoss 0.1608 (0.1574)\tAccu 0.9688 (0.9706)\t\n",
            "Epoch: [41][18/44]\tLoss 0.0504 (0.1514)\tAccu 1.0000 (0.9722)\t\n",
            "Epoch: [41][19/44]\tLoss 0.0730 (0.1473)\tAccu 1.0000 (0.9737)\t\n",
            "Epoch: [41][20/44]\tLoss 0.1475 (0.1473)\tAccu 0.9688 (0.9734)\t\n",
            "Epoch: [41][21/44]\tLoss 0.3587 (0.1574)\tAccu 0.9688 (0.9732)\t\n",
            "Epoch: [41][22/44]\tLoss 0.3516 (0.1662)\tAccu 0.9062 (0.9702)\t\n",
            "Epoch: [41][23/44]\tLoss 0.1021 (0.1634)\tAccu 0.9688 (0.9701)\t\n",
            "Epoch: [41][24/44]\tLoss 0.0514 (0.1588)\tAccu 1.0000 (0.9714)\t\n",
            "Epoch: [41][25/44]\tLoss 0.0565 (0.1547)\tAccu 0.9688 (0.9712)\t\n",
            "Epoch: [41][26/44]\tLoss 0.0804 (0.1518)\tAccu 0.9688 (0.9712)\t\n",
            "Epoch: [41][27/44]\tLoss 0.0649 (0.1486)\tAccu 1.0000 (0.9722)\t\n",
            "Epoch: [41][28/44]\tLoss 0.1287 (0.1479)\tAccu 0.9375 (0.9710)\t\n",
            "Epoch: [41][29/44]\tLoss 0.0529 (0.1446)\tAccu 0.9688 (0.9709)\t\n",
            "Epoch: [41][30/44]\tLoss 0.1496 (0.1448)\tAccu 0.9375 (0.9698)\t\n",
            "Epoch: [41][31/44]\tLoss 0.0376 (0.1413)\tAccu 1.0000 (0.9708)\t\n",
            "Epoch: [41][32/44]\tLoss 0.0829 (0.1395)\tAccu 0.9688 (0.9707)\t\n",
            "Epoch: [41][33/44]\tLoss 0.0514 (0.1368)\tAccu 1.0000 (0.9716)\t\n",
            "Epoch: [41][34/44]\tLoss 0.1511 (0.1372)\tAccu 0.9688 (0.9715)\t\n",
            "Epoch: [41][35/44]\tLoss 0.1048 (0.1363)\tAccu 1.0000 (0.9723)\t\n",
            "Epoch: [41][36/44]\tLoss 0.0252 (0.1332)\tAccu 1.0000 (0.9731)\t\n",
            "Epoch: [41][37/44]\tLoss 0.0315 (0.1305)\tAccu 1.0000 (0.9738)\t\n",
            "Epoch: [41][38/44]\tLoss 0.1337 (0.1306)\tAccu 0.9688 (0.9737)\t\n",
            "Epoch: [41][39/44]\tLoss 0.0311 (0.1280)\tAccu 1.0000 (0.9744)\t\n",
            "Epoch: [41][40/44]\tLoss 0.0274 (0.1255)\tAccu 1.0000 (0.9750)\t\n",
            "Epoch: [41][41/44]\tLoss 0.0507 (0.1237)\tAccu 1.0000 (0.9756)\t\n",
            "Epoch: [41][42/44]\tLoss 0.0482 (0.1219)\tAccu 1.0000 (0.9762)\t\n",
            "Epoch: [41][43/44]\tLoss 0.0680 (0.1206)\tAccu 0.9688 (0.9760)\t\n",
            "Epoch: [41][44/44]\tLoss 0.0046 (0.1186)\tAccu 1.0000 (0.9766)\t\n",
            "Accu 0.2025\t\n",
            "Epoch: [42][1/44]\tLoss 0.0406 (0.0406)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [42][2/44]\tLoss 0.0728 (0.0567)\tAccu 0.9688 (0.9844)\t\n",
            "Epoch: [42][3/44]\tLoss 0.0676 (0.0603)\tAccu 0.9688 (0.9792)\t\n",
            "Epoch: [42][4/44]\tLoss 0.0637 (0.0612)\tAccu 1.0000 (0.9844)\t\n",
            "Epoch: [42][5/44]\tLoss 0.0223 (0.0534)\tAccu 1.0000 (0.9875)\t\n",
            "Epoch: [42][6/44]\tLoss 0.1219 (0.0648)\tAccu 0.9688 (0.9844)\t\n",
            "Epoch: [42][7/44]\tLoss 0.0272 (0.0594)\tAccu 1.0000 (0.9866)\t\n",
            "Epoch: [42][8/44]\tLoss 0.0487 (0.0581)\tAccu 1.0000 (0.9883)\t\n",
            "Epoch: [42][9/44]\tLoss 0.0664 (0.0590)\tAccu 0.9688 (0.9861)\t\n",
            "Epoch: [42][10/44]\tLoss 0.1274 (0.0659)\tAccu 0.9688 (0.9844)\t\n",
            "Epoch: [42][11/44]\tLoss 0.0534 (0.0647)\tAccu 1.0000 (0.9858)\t\n",
            "Epoch: [42][12/44]\tLoss 0.0598 (0.0643)\tAccu 0.9688 (0.9844)\t\n",
            "Epoch: [42][13/44]\tLoss 0.0332 (0.0619)\tAccu 1.0000 (0.9856)\t\n",
            "Epoch: [42][14/44]\tLoss 0.0048 (0.0578)\tAccu 1.0000 (0.9866)\t\n",
            "Epoch: [42][15/44]\tLoss 0.0305 (0.0560)\tAccu 1.0000 (0.9875)\t\n",
            "Epoch: [42][16/44]\tLoss 0.0250 (0.0541)\tAccu 1.0000 (0.9883)\t\n",
            "Epoch: [42][17/44]\tLoss 0.0171 (0.0519)\tAccu 1.0000 (0.9890)\t\n",
            "Epoch: [42][18/44]\tLoss 0.0150 (0.0499)\tAccu 1.0000 (0.9896)\t\n",
            "Epoch: [42][19/44]\tLoss 0.0592 (0.0504)\tAccu 1.0000 (0.9901)\t\n",
            "Epoch: [42][20/44]\tLoss 0.0709 (0.0514)\tAccu 1.0000 (0.9906)\t\n",
            "Epoch: [42][21/44]\tLoss 0.0622 (0.0519)\tAccu 1.0000 (0.9911)\t\n",
            "Epoch: [42][22/44]\tLoss 0.0094 (0.0500)\tAccu 1.0000 (0.9915)\t\n",
            "Epoch: [42][23/44]\tLoss 0.1350 (0.0537)\tAccu 0.9375 (0.9891)\t\n",
            "Epoch: [42][24/44]\tLoss 0.0391 (0.0531)\tAccu 1.0000 (0.9896)\t\n",
            "Epoch: [42][25/44]\tLoss 0.0410 (0.0526)\tAccu 0.9688 (0.9888)\t\n",
            "Epoch: [42][26/44]\tLoss 0.0370 (0.0520)\tAccu 1.0000 (0.9892)\t\n",
            "Epoch: [42][27/44]\tLoss 0.0107 (0.0504)\tAccu 1.0000 (0.9896)\t\n",
            "Epoch: [42][28/44]\tLoss 0.0062 (0.0489)\tAccu 1.0000 (0.9900)\t\n",
            "Epoch: [42][29/44]\tLoss 0.0346 (0.0484)\tAccu 1.0000 (0.9903)\t\n",
            "Epoch: [42][30/44]\tLoss 0.0112 (0.0471)\tAccu 1.0000 (0.9906)\t\n",
            "Epoch: [42][31/44]\tLoss 0.0911 (0.0486)\tAccu 1.0000 (0.9909)\t\n",
            "Epoch: [42][32/44]\tLoss 0.0592 (0.0489)\tAccu 1.0000 (0.9912)\t\n",
            "Epoch: [42][33/44]\tLoss 0.0443 (0.0487)\tAccu 0.9688 (0.9905)\t\n",
            "Epoch: [42][34/44]\tLoss 0.0433 (0.0486)\tAccu 1.0000 (0.9908)\t\n",
            "Epoch: [42][35/44]\tLoss 0.2063 (0.0531)\tAccu 0.9688 (0.9902)\t\n",
            "Epoch: [42][36/44]\tLoss 0.0138 (0.0520)\tAccu 1.0000 (0.9905)\t\n",
            "Epoch: [42][37/44]\tLoss 0.0170 (0.0511)\tAccu 1.0000 (0.9907)\t\n",
            "Epoch: [42][38/44]\tLoss 0.0227 (0.0503)\tAccu 1.0000 (0.9910)\t\n",
            "Epoch: [42][39/44]\tLoss 0.0236 (0.0496)\tAccu 1.0000 (0.9912)\t\n",
            "Epoch: [42][40/44]\tLoss 0.0196 (0.0489)\tAccu 1.0000 (0.9914)\t\n",
            "Epoch: [42][41/44]\tLoss 0.0143 (0.0480)\tAccu 1.0000 (0.9916)\t\n",
            "Epoch: [42][42/44]\tLoss 0.0055 (0.0470)\tAccu 1.0000 (0.9918)\t\n",
            "Epoch: [42][43/44]\tLoss 0.0139 (0.0462)\tAccu 1.0000 (0.9920)\t\n",
            "Epoch: [42][44/44]\tLoss 0.0042 (0.0455)\tAccu 1.0000 (0.9922)\t\n",
            "Accu 0.2100\t\n",
            "Epoch: [43][1/44]\tLoss 0.0103 (0.0103)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [43][2/44]\tLoss 0.0570 (0.0337)\tAccu 0.9688 (0.9844)\t\n",
            "Epoch: [43][3/44]\tLoss 0.0128 (0.0267)\tAccu 1.0000 (0.9896)\t\n",
            "Epoch: [43][4/44]\tLoss 0.0313 (0.0279)\tAccu 1.0000 (0.9922)\t\n",
            "Epoch: [43][5/44]\tLoss 0.0125 (0.0248)\tAccu 1.0000 (0.9938)\t\n",
            "Epoch: [43][6/44]\tLoss 0.0566 (0.0301)\tAccu 1.0000 (0.9948)\t\n",
            "Epoch: [43][7/44]\tLoss 0.0316 (0.0303)\tAccu 1.0000 (0.9955)\t\n",
            "Epoch: [43][8/44]\tLoss 0.0248 (0.0296)\tAccu 1.0000 (0.9961)\t\n",
            "Epoch: [43][9/44]\tLoss 0.0132 (0.0278)\tAccu 1.0000 (0.9965)\t\n",
            "Epoch: [43][10/44]\tLoss 0.0353 (0.0286)\tAccu 1.0000 (0.9969)\t\n",
            "Epoch: [43][11/44]\tLoss 0.0118 (0.0270)\tAccu 1.0000 (0.9972)\t\n",
            "Epoch: [43][12/44]\tLoss 0.0084 (0.0255)\tAccu 1.0000 (0.9974)\t\n",
            "Epoch: [43][13/44]\tLoss 0.0049 (0.0239)\tAccu 1.0000 (0.9976)\t\n",
            "Epoch: [43][14/44]\tLoss 0.0535 (0.0260)\tAccu 0.9688 (0.9955)\t\n",
            "Epoch: [43][15/44]\tLoss 0.0298 (0.0263)\tAccu 1.0000 (0.9958)\t\n",
            "Epoch: [43][16/44]\tLoss 0.0082 (0.0251)\tAccu 1.0000 (0.9961)\t\n",
            "Epoch: [43][17/44]\tLoss 0.0097 (0.0242)\tAccu 1.0000 (0.9963)\t\n",
            "Epoch: [43][18/44]\tLoss 0.0127 (0.0236)\tAccu 1.0000 (0.9965)\t\n",
            "Epoch: [43][19/44]\tLoss 0.0168 (0.0232)\tAccu 1.0000 (0.9967)\t\n",
            "Epoch: [43][20/44]\tLoss 0.0140 (0.0228)\tAccu 1.0000 (0.9969)\t\n",
            "Epoch: [43][21/44]\tLoss 0.0073 (0.0220)\tAccu 1.0000 (0.9970)\t\n",
            "Epoch: [43][22/44]\tLoss 0.0061 (0.0213)\tAccu 1.0000 (0.9972)\t\n",
            "Epoch: [43][23/44]\tLoss 0.0088 (0.0208)\tAccu 1.0000 (0.9973)\t\n",
            "Epoch: [43][24/44]\tLoss 0.0054 (0.0201)\tAccu 1.0000 (0.9974)\t\n",
            "Epoch: [43][25/44]\tLoss 0.0051 (0.0195)\tAccu 1.0000 (0.9975)\t\n",
            "Epoch: [43][26/44]\tLoss 0.0072 (0.0190)\tAccu 1.0000 (0.9976)\t\n",
            "Epoch: [43][27/44]\tLoss 0.0087 (0.0187)\tAccu 1.0000 (0.9977)\t\n",
            "Epoch: [43][28/44]\tLoss 0.0046 (0.0182)\tAccu 1.0000 (0.9978)\t\n",
            "Epoch: [43][29/44]\tLoss 0.0076 (0.0178)\tAccu 1.0000 (0.9978)\t\n",
            "Epoch: [43][30/44]\tLoss 0.0050 (0.0174)\tAccu 1.0000 (0.9979)\t\n",
            "Epoch: [43][31/44]\tLoss 0.0037 (0.0169)\tAccu 1.0000 (0.9980)\t\n",
            "Epoch: [43][32/44]\tLoss 0.0087 (0.0167)\tAccu 1.0000 (0.9980)\t\n",
            "Epoch: [43][33/44]\tLoss 0.0157 (0.0166)\tAccu 1.0000 (0.9981)\t\n",
            "Epoch: [43][34/44]\tLoss 0.0070 (0.0164)\tAccu 1.0000 (0.9982)\t\n",
            "Epoch: [43][35/44]\tLoss 0.0468 (0.0172)\tAccu 1.0000 (0.9982)\t\n",
            "Epoch: [43][36/44]\tLoss 0.0059 (0.0169)\tAccu 1.0000 (0.9983)\t\n",
            "Epoch: [43][37/44]\tLoss 0.0038 (0.0166)\tAccu 1.0000 (0.9983)\t\n",
            "Epoch: [43][38/44]\tLoss 0.0065 (0.0163)\tAccu 1.0000 (0.9984)\t\n",
            "Epoch: [43][39/44]\tLoss 0.0083 (0.0161)\tAccu 1.0000 (0.9984)\t\n",
            "Epoch: [43][40/44]\tLoss 0.0098 (0.0159)\tAccu 1.0000 (0.9984)\t\n",
            "Epoch: [43][41/44]\tLoss 0.0053 (0.0157)\tAccu 1.0000 (0.9985)\t\n",
            "Epoch: [43][42/44]\tLoss 0.0046 (0.0154)\tAccu 1.0000 (0.9985)\t\n",
            "Epoch: [43][43/44]\tLoss 0.0218 (0.0156)\tAccu 1.0000 (0.9985)\t\n",
            "Epoch: [43][44/44]\tLoss 0.0029 (0.0153)\tAccu 1.0000 (0.9986)\t\n",
            "Accu 0.2225\t\n",
            "Epoch: [44][1/44]\tLoss 0.0134 (0.0134)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [44][2/44]\tLoss 0.0175 (0.0155)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [44][3/44]\tLoss 0.0080 (0.0130)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [44][4/44]\tLoss 0.0078 (0.0117)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [44][5/44]\tLoss 0.0049 (0.0103)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [44][6/44]\tLoss 0.0367 (0.0147)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [44][7/44]\tLoss 0.0069 (0.0136)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [44][8/44]\tLoss 0.0043 (0.0125)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [44][9/44]\tLoss 0.0047 (0.0116)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [44][10/44]\tLoss 0.0190 (0.0123)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [44][11/44]\tLoss 0.0054 (0.0117)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [44][12/44]\tLoss 0.0032 (0.0110)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [44][13/44]\tLoss 0.0041 (0.0105)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [44][14/44]\tLoss 0.0015 (0.0098)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [44][15/44]\tLoss 0.0185 (0.0104)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [44][16/44]\tLoss 0.0065 (0.0102)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [44][17/44]\tLoss 0.0051 (0.0099)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [44][18/44]\tLoss 0.0047 (0.0096)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [44][19/44]\tLoss 0.0103 (0.0096)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [44][20/44]\tLoss 0.0072 (0.0095)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [44][21/44]\tLoss 0.0030 (0.0092)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [44][22/44]\tLoss 0.0096 (0.0092)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [44][23/44]\tLoss 0.0071 (0.0091)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [44][24/44]\tLoss 0.0097 (0.0091)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [44][25/44]\tLoss 0.0051 (0.0090)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [44][26/44]\tLoss 0.0034 (0.0088)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [44][27/44]\tLoss 0.0038 (0.0086)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [44][28/44]\tLoss 0.0044 (0.0084)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [44][29/44]\tLoss 0.0022 (0.0082)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [44][30/44]\tLoss 0.0018 (0.0080)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [44][31/44]\tLoss 0.0027 (0.0078)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [44][32/44]\tLoss 0.0068 (0.0078)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [44][33/44]\tLoss 0.0047 (0.0077)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [44][34/44]\tLoss 0.0088 (0.0077)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [44][35/44]\tLoss 0.0390 (0.0086)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [44][36/44]\tLoss 0.0023 (0.0084)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [44][37/44]\tLoss 0.0022 (0.0083)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [44][38/44]\tLoss 0.0037 (0.0082)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [44][39/44]\tLoss 0.0040 (0.0080)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [44][40/44]\tLoss 0.0045 (0.0080)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [44][41/44]\tLoss 0.0026 (0.0078)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [44][42/44]\tLoss 0.0019 (0.0077)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [44][43/44]\tLoss 0.0032 (0.0076)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [44][44/44]\tLoss 0.0013 (0.0075)\tAccu 1.0000 (1.0000)\t\n",
            "Accu 0.2225\t\n",
            "Epoch: [45][1/44]\tLoss 0.0045 (0.0045)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [45][2/44]\tLoss 0.0023 (0.0034)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [45][3/44]\tLoss 0.0052 (0.0040)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [45][4/44]\tLoss 0.0050 (0.0042)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [45][5/44]\tLoss 0.0056 (0.0045)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [45][6/44]\tLoss 0.0200 (0.0071)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [45][7/44]\tLoss 0.0039 (0.0067)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [45][8/44]\tLoss 0.0017 (0.0060)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [45][9/44]\tLoss 0.0044 (0.0059)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [45][10/44]\tLoss 0.0064 (0.0059)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [45][11/44]\tLoss 0.0020 (0.0056)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [45][12/44]\tLoss 0.0014 (0.0052)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [45][13/44]\tLoss 0.0016 (0.0049)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [45][14/44]\tLoss 0.0018 (0.0047)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [45][15/44]\tLoss 0.0089 (0.0050)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [45][16/44]\tLoss 0.0016 (0.0048)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [45][17/44]\tLoss 0.0054 (0.0048)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [45][18/44]\tLoss 0.0023 (0.0047)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [45][19/44]\tLoss 0.0037 (0.0046)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [45][20/44]\tLoss 0.0049 (0.0046)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [45][21/44]\tLoss 0.0020 (0.0045)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [45][22/44]\tLoss 0.0013 (0.0044)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [45][23/44]\tLoss 0.0009 (0.0042)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [45][24/44]\tLoss 0.0020 (0.0041)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [45][25/44]\tLoss 0.0015 (0.0040)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [45][26/44]\tLoss 0.0026 (0.0040)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [45][27/44]\tLoss 0.0022 (0.0039)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [45][28/44]\tLoss 0.0016 (0.0038)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [45][29/44]\tLoss 0.0019 (0.0037)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [45][30/44]\tLoss 0.0016 (0.0037)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [45][31/44]\tLoss 0.0017 (0.0036)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [45][32/44]\tLoss 0.0022 (0.0036)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [45][33/44]\tLoss 0.0031 (0.0036)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [45][34/44]\tLoss 0.0028 (0.0035)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [45][35/44]\tLoss 0.0238 (0.0041)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [45][36/44]\tLoss 0.0018 (0.0040)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [45][37/44]\tLoss 0.0015 (0.0040)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [45][38/44]\tLoss 0.0020 (0.0039)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [45][39/44]\tLoss 0.0013 (0.0039)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [45][40/44]\tLoss 0.0029 (0.0038)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [45][41/44]\tLoss 0.0030 (0.0038)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [45][42/44]\tLoss 0.0007 (0.0037)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [45][43/44]\tLoss 0.0023 (0.0037)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [45][44/44]\tLoss 0.0007 (0.0037)\tAccu 1.0000 (1.0000)\t\n",
            "Accu 0.2225\t\n",
            "Epoch: [46][1/44]\tLoss 0.0032 (0.0032)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [46][2/44]\tLoss 0.0012 (0.0022)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [46][3/44]\tLoss 0.0036 (0.0027)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [46][4/44]\tLoss 0.0043 (0.0031)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [46][5/44]\tLoss 0.0019 (0.0029)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [46][6/44]\tLoss 0.0085 (0.0038)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [46][7/44]\tLoss 0.0027 (0.0036)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [46][8/44]\tLoss 0.0016 (0.0034)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [46][9/44]\tLoss 0.0033 (0.0034)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [46][10/44]\tLoss 0.0045 (0.0035)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [46][11/44]\tLoss 0.0016 (0.0033)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [46][12/44]\tLoss 0.0010 (0.0031)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [46][13/44]\tLoss 0.0012 (0.0030)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [46][14/44]\tLoss 0.0008 (0.0028)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [46][15/44]\tLoss 0.0065 (0.0031)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [46][16/44]\tLoss 0.0013 (0.0029)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [46][17/44]\tLoss 0.0013 (0.0028)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [46][18/44]\tLoss 0.0016 (0.0028)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [46][19/44]\tLoss 0.0030 (0.0028)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [46][20/44]\tLoss 0.0037 (0.0028)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [46][21/44]\tLoss 0.0016 (0.0028)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [46][22/44]\tLoss 0.0014 (0.0027)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [46][23/44]\tLoss 0.0006 (0.0026)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [46][24/44]\tLoss 0.0018 (0.0026)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [46][25/44]\tLoss 0.0012 (0.0025)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [46][26/44]\tLoss 0.0023 (0.0025)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [46][27/44]\tLoss 0.0019 (0.0025)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [46][28/44]\tLoss 0.0011 (0.0025)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [46][29/44]\tLoss 0.0014 (0.0024)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [46][30/44]\tLoss 0.0008 (0.0024)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [46][31/44]\tLoss 0.0014 (0.0023)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [46][32/44]\tLoss 0.0016 (0.0023)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [46][33/44]\tLoss 0.0020 (0.0023)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [46][34/44]\tLoss 0.0023 (0.0023)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [46][35/44]\tLoss 0.0171 (0.0027)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [46][36/44]\tLoss 0.0016 (0.0027)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [46][37/44]\tLoss 0.0012 (0.0027)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [46][38/44]\tLoss 0.0016 (0.0026)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [46][39/44]\tLoss 0.0011 (0.0026)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [46][40/44]\tLoss 0.0022 (0.0026)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [46][41/44]\tLoss 0.0018 (0.0026)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [46][42/44]\tLoss 0.0007 (0.0025)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [46][43/44]\tLoss 0.0017 (0.0025)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [46][44/44]\tLoss 0.0006 (0.0025)\tAccu 1.0000 (1.0000)\t\n",
            "Accu 0.2175\t\n",
            "Epoch: [47][1/44]\tLoss 0.0022 (0.0022)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [47][2/44]\tLoss 0.0012 (0.0017)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [47][3/44]\tLoss 0.0032 (0.0022)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [47][4/44]\tLoss 0.0031 (0.0024)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [47][5/44]\tLoss 0.0017 (0.0023)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [47][6/44]\tLoss 0.0059 (0.0029)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [47][7/44]\tLoss 0.0023 (0.0028)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [47][8/44]\tLoss 0.0013 (0.0026)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [47][9/44]\tLoss 0.0027 (0.0026)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [47][10/44]\tLoss 0.0027 (0.0026)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [47][11/44]\tLoss 0.0013 (0.0025)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [47][12/44]\tLoss 0.0009 (0.0024)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [47][13/44]\tLoss 0.0011 (0.0023)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [47][14/44]\tLoss 0.0008 (0.0022)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [47][15/44]\tLoss 0.0047 (0.0023)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [47][16/44]\tLoss 0.0012 (0.0023)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [47][17/44]\tLoss 0.0012 (0.0022)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [47][18/44]\tLoss 0.0013 (0.0022)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [47][19/44]\tLoss 0.0025 (0.0022)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [47][20/44]\tLoss 0.0030 (0.0022)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [47][21/44]\tLoss 0.0014 (0.0022)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [47][22/44]\tLoss 0.0012 (0.0021)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [47][23/44]\tLoss 0.0006 (0.0021)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [47][24/44]\tLoss 0.0015 (0.0020)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [47][25/44]\tLoss 0.0010 (0.0020)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [47][26/44]\tLoss 0.0020 (0.0020)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [47][27/44]\tLoss 0.0016 (0.0020)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [47][28/44]\tLoss 0.0011 (0.0020)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [47][29/44]\tLoss 0.0013 (0.0019)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [47][30/44]\tLoss 0.0007 (0.0019)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [47][31/44]\tLoss 0.0013 (0.0019)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [47][32/44]\tLoss 0.0015 (0.0019)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [47][33/44]\tLoss 0.0018 (0.0019)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [47][34/44]\tLoss 0.0020 (0.0019)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [47][35/44]\tLoss 0.0131 (0.0022)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [47][36/44]\tLoss 0.0015 (0.0022)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [47][37/44]\tLoss 0.0011 (0.0021)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [47][38/44]\tLoss 0.0015 (0.0021)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [47][39/44]\tLoss 0.0009 (0.0021)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [47][40/44]\tLoss 0.0021 (0.0021)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [47][41/44]\tLoss 0.0016 (0.0021)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [47][42/44]\tLoss 0.0007 (0.0020)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [47][43/44]\tLoss 0.0015 (0.0020)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [47][44/44]\tLoss 0.0005 (0.0020)\tAccu 1.0000 (1.0000)\t\n",
            "Accu 0.2200\t\n",
            "Epoch: [48][1/44]\tLoss 0.0020 (0.0020)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [48][2/44]\tLoss 0.0011 (0.0015)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [48][3/44]\tLoss 0.0029 (0.0020)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [48][4/44]\tLoss 0.0026 (0.0021)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [48][5/44]\tLoss 0.0016 (0.0020)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [48][6/44]\tLoss 0.0047 (0.0025)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [48][7/44]\tLoss 0.0021 (0.0024)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [48][8/44]\tLoss 0.0013 (0.0023)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [48][9/44]\tLoss 0.0022 (0.0023)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [48][10/44]\tLoss 0.0024 (0.0023)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [48][11/44]\tLoss 0.0012 (0.0022)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [48][12/44]\tLoss 0.0008 (0.0021)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [48][13/44]\tLoss 0.0010 (0.0020)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [48][14/44]\tLoss 0.0007 (0.0019)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [48][15/44]\tLoss 0.0039 (0.0020)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [48][16/44]\tLoss 0.0011 (0.0020)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [48][17/44]\tLoss 0.0011 (0.0019)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [48][18/44]\tLoss 0.0012 (0.0019)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [48][19/44]\tLoss 0.0022 (0.0019)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [48][20/44]\tLoss 0.0026 (0.0019)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [48][21/44]\tLoss 0.0013 (0.0019)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [48][22/44]\tLoss 0.0011 (0.0019)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [48][23/44]\tLoss 0.0005 (0.0018)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [48][24/44]\tLoss 0.0013 (0.0018)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [48][25/44]\tLoss 0.0010 (0.0018)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [48][26/44]\tLoss 0.0018 (0.0018)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [48][27/44]\tLoss 0.0014 (0.0017)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [48][28/44]\tLoss 0.0010 (0.0017)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [48][29/44]\tLoss 0.0012 (0.0017)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [48][30/44]\tLoss 0.0006 (0.0017)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [48][31/44]\tLoss 0.0013 (0.0016)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [48][32/44]\tLoss 0.0012 (0.0016)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [48][33/44]\tLoss 0.0015 (0.0016)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [48][34/44]\tLoss 0.0018 (0.0016)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [48][35/44]\tLoss 0.0111 (0.0019)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [48][36/44]\tLoss 0.0013 (0.0019)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [48][37/44]\tLoss 0.0010 (0.0019)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [48][38/44]\tLoss 0.0013 (0.0019)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [48][39/44]\tLoss 0.0008 (0.0018)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [48][40/44]\tLoss 0.0018 (0.0018)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [48][41/44]\tLoss 0.0013 (0.0018)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [48][42/44]\tLoss 0.0006 (0.0018)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [48][43/44]\tLoss 0.0014 (0.0018)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [48][44/44]\tLoss 0.0004 (0.0018)\tAccu 1.0000 (1.0000)\t\n",
            "Accu 0.2225\t\n",
            "Epoch: [49][1/44]\tLoss 0.0018 (0.0018)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [49][2/44]\tLoss 0.0010 (0.0014)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [49][3/44]\tLoss 0.0026 (0.0018)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [49][4/44]\tLoss 0.0022 (0.0019)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [49][5/44]\tLoss 0.0014 (0.0018)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [49][6/44]\tLoss 0.0038 (0.0021)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [49][7/44]\tLoss 0.0019 (0.0021)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [49][8/44]\tLoss 0.0011 (0.0020)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [49][9/44]\tLoss 0.0020 (0.0020)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [49][10/44]\tLoss 0.0021 (0.0020)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [49][11/44]\tLoss 0.0011 (0.0019)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [49][12/44]\tLoss 0.0007 (0.0018)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [49][13/44]\tLoss 0.0010 (0.0017)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [49][14/44]\tLoss 0.0006 (0.0017)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [49][15/44]\tLoss 0.0032 (0.0018)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [49][16/44]\tLoss 0.0009 (0.0017)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [49][17/44]\tLoss 0.0010 (0.0017)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [49][18/44]\tLoss 0.0011 (0.0016)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [49][19/44]\tLoss 0.0020 (0.0017)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [49][20/44]\tLoss 0.0023 (0.0017)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [49][21/44]\tLoss 0.0012 (0.0017)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [49][22/44]\tLoss 0.0011 (0.0016)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [49][23/44]\tLoss 0.0005 (0.0016)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [49][24/44]\tLoss 0.0012 (0.0016)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [49][25/44]\tLoss 0.0009 (0.0015)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [49][26/44]\tLoss 0.0016 (0.0015)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [49][27/44]\tLoss 0.0013 (0.0015)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [49][28/44]\tLoss 0.0009 (0.0015)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [49][29/44]\tLoss 0.0010 (0.0015)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [49][30/44]\tLoss 0.0006 (0.0015)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [49][31/44]\tLoss 0.0012 (0.0015)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [49][32/44]\tLoss 0.0011 (0.0014)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [49][33/44]\tLoss 0.0014 (0.0014)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [49][34/44]\tLoss 0.0017 (0.0015)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [49][35/44]\tLoss 0.0092 (0.0017)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [49][36/44]\tLoss 0.0013 (0.0017)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [49][37/44]\tLoss 0.0009 (0.0016)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [49][38/44]\tLoss 0.0012 (0.0016)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [49][39/44]\tLoss 0.0008 (0.0016)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [49][40/44]\tLoss 0.0017 (0.0016)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [49][41/44]\tLoss 0.0012 (0.0016)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [49][42/44]\tLoss 0.0006 (0.0016)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [49][43/44]\tLoss 0.0012 (0.0016)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [49][44/44]\tLoss 0.0004 (0.0016)\tAccu 1.0000 (1.0000)\t\n",
            "Accu 0.2250\t\n",
            "Epoch: [50][1/44]\tLoss 0.0016 (0.0016)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [50][2/44]\tLoss 0.0009 (0.0013)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [50][3/44]\tLoss 0.0023 (0.0016)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [50][4/44]\tLoss 0.0019 (0.0017)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [50][5/44]\tLoss 0.0013 (0.0016)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [50][6/44]\tLoss 0.0032 (0.0019)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [50][7/44]\tLoss 0.0017 (0.0019)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [50][8/44]\tLoss 0.0010 (0.0018)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [50][9/44]\tLoss 0.0017 (0.0018)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [50][10/44]\tLoss 0.0019 (0.0018)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [50][11/44]\tLoss 0.0010 (0.0017)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [50][12/44]\tLoss 0.0007 (0.0016)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [50][13/44]\tLoss 0.0009 (0.0016)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [50][14/44]\tLoss 0.0006 (0.0015)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [50][15/44]\tLoss 0.0028 (0.0016)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [50][16/44]\tLoss 0.0009 (0.0015)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [50][17/44]\tLoss 0.0009 (0.0015)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [50][18/44]\tLoss 0.0010 (0.0015)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [50][19/44]\tLoss 0.0017 (0.0015)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [50][20/44]\tLoss 0.0021 (0.0015)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [50][21/44]\tLoss 0.0011 (0.0015)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [50][22/44]\tLoss 0.0010 (0.0015)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [50][23/44]\tLoss 0.0005 (0.0014)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [50][24/44]\tLoss 0.0011 (0.0014)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [50][25/44]\tLoss 0.0008 (0.0014)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [50][26/44]\tLoss 0.0015 (0.0014)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [50][27/44]\tLoss 0.0012 (0.0014)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [50][28/44]\tLoss 0.0009 (0.0014)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [50][29/44]\tLoss 0.0010 (0.0014)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [50][30/44]\tLoss 0.0005 (0.0013)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [50][31/44]\tLoss 0.0011 (0.0013)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [50][32/44]\tLoss 0.0010 (0.0013)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [50][33/44]\tLoss 0.0012 (0.0013)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [50][34/44]\tLoss 0.0016 (0.0013)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [50][35/44]\tLoss 0.0080 (0.0015)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [50][36/44]\tLoss 0.0012 (0.0015)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [50][37/44]\tLoss 0.0009 (0.0015)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [50][38/44]\tLoss 0.0012 (0.0015)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [50][39/44]\tLoss 0.0007 (0.0015)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [50][40/44]\tLoss 0.0015 (0.0015)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [50][41/44]\tLoss 0.0010 (0.0014)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [50][42/44]\tLoss 0.0006 (0.0014)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [50][43/44]\tLoss 0.0011 (0.0014)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [50][44/44]\tLoss 0.0004 (0.0014)\tAccu 1.0000 (1.0000)\t\n",
            "Accu 0.2250\t\n",
            "Best Acc: 0.2525\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgy6kbE0MmlH",
        "colab_type": "text"
      },
      "source": [
        "#### Weight Visualization [8 pts]\n",
        "\n",
        "For the best convolutional model you obtained, extract the final weights from the first convolutional layer. Visualize each filter of the first convolutional layer as an image. Show these images in a grid."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBBFQMFBMmlH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 854
        },
        "outputId": "fed1e783-a0dc-4408-d721-53821c8b86be"
      },
      "source": [
        "import numpy as np\n",
        "from mpl_toolkits.axes_grid1 import ImageGrid\n",
        "# write your code in this cell to visualize filters of the first convolutional layer \n",
        "def visualizeWeights():\n",
        "  model = torch.load(r'/content/drive/My Drive/Colab Notebooks/CS464 - HW3/best_model_cnn.pth')\n",
        "  weights = model.conv1.weight.data\n",
        "\n",
        "  weights = weights.cpu().numpy()\n",
        "  weights = weights.reshape(32, 3, 9)\n",
        "  for i in range(len(weights[0, 0])):\n",
        "    min_weights= np.min(weights[:, :, i])\n",
        "    weights[:, :, i] -=  min_weights\n",
        "    max_weights= np.max(weights[:, :, i])\n",
        "    weights[:, :, i] /= max_weights- weights[:, :, i]\n",
        "\n",
        "  \n",
        "  weights = weights.reshape(32, 3, 3, 3)\n",
        "  weights = torch.from_numpy(weights)\n",
        "  fig = plt.figure(figsize=(16., 16.))\n",
        "  grid = ImageGrid(fig, 120, nrows_ncols=(4, 8))\n",
        "\n",
        "  for ax, im in zip(grid, weights):\n",
        "    im = np.swapaxes(im, 0, 2)\n",
        "    im = np.swapaxes(im, 0, 1)\n",
        "    ax.imshow(im)\n",
        "\n",
        "  plt.show()  \n",
        "\n",
        "visualizeWeights()"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: RuntimeWarning: divide by zero encountered in true_divide\n",
            "  \n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbkAAADpCAYAAAC5rUlrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1152x1152 with 64 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfRgU3GmMmlK",
        "colab_type": "text"
      },
      "source": [
        "### 2.3 Transfer Learning [20 pts]\n",
        "\n",
        "The trained weights of a network can be used as a starting point for the weights of a different neural network to solve another similar problem. This approach is called <b>\"Transfer Learning\"</b>. <br>\n",
        "\n",
        "For this model, your data loader is the same as Question 2.2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dA3bTeCZMmlR",
        "colab_type": "text"
      },
      "source": [
        "#### Model\n",
        "\n",
        "You will to use Inception_V3 convolutional neural network which is one of the well-known CNN models. You <b>DO NOT</b> need to implement your own  architecture. Torchvision has also a model set which contains commonly used CNN models including Inception_V3 (Szegedy et al., 2016). You will use pretrained network weights as a starting point. These weights will come from the result of the training with Imagenet dataset. These will be loaded automatically when you set the \"pretrained\" parameter as \"True\" (check the hints in the code). Otherwise, weights will be randomly initialized."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cThe_zoRMmla",
        "colab_type": "text"
      },
      "source": [
        "#### Train & Validation  [15 pts]\n",
        "\n",
        "In this case, use ImageNet pretrained Inception_V3 model on the Animal dataset. At the end of each epoch, you should evaluate your network with validation split. Modify your classifier layer by adding fully connected layers with proper activation function. Print training loss, training accuracy, validation loss and validation accuracy values for each epoch as an output of the cell below. Report the best validation accuracy score. Then, report test accuacy for your best model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weQ3PHEuMmla",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9a38c47a-2e1f-4a45-cd7b-c3cfefe53537"
      },
      "source": [
        "import torchvision\n",
        "\n",
        "# write your code in this cell to train your network using transfer learning approach\n",
        "\n",
        "# HINTS:\n",
        "# inception_v3 = torchvision.models.inception_v3(pretrained=True) will return an Inception_V3 model instance with ImageNet pretrained network weights.\n",
        "# Decide whether if you'd like to freeze those weights or not. \n",
        "# You need to make some changes in the classifier layer to get a proper network for your problem.\n",
        "\n",
        "\n",
        "\n",
        "max_epoch = 20\n",
        "train_batch = 32\n",
        "test_batch = 32\n",
        "\n",
        "\n",
        "learning_rate = 0.001\n",
        "\n",
        "use_gpu = torch.cuda.is_available()\n",
        "print(\"Use GPU:\", use_gpu)\n",
        "\n",
        "\n",
        "def main(): # you are free to change parameters\n",
        "\n",
        "    # Create train dataset loader\n",
        "    # Create validation dataset loader\n",
        "    # Create test dataset loader\n",
        "    # initialize your GENet neural network\n",
        "    # define your loss function\n",
        "    train_dataset, val_dataset, test_dataset = get_dataset(ROOT)\n",
        "    trainloader = DataLoader(train_dataset, batch_size=train_batch)\n",
        "    testloader = DataLoader(test_dataset, batch_size=test_batch)\n",
        "    val_loader = DataLoader(val_dataset)\n",
        "\n",
        "    model = torchvision.models.inception_v3(pretrained=True)\n",
        "    feature_amount = model.fc.in_features\n",
        "    model.fc = torch.nn.Linear(feature_amount, 10)\n",
        "    model.aux_logits = False\n",
        "    model = model.cuda()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-04) # you can play with momentum and weight_decay parameters as well\n",
        "    criterion = nn.CrossEntropyLoss().cuda()\n",
        "    # start training\n",
        "    # for each epoch calculate validation performance\n",
        "    # save best model according to validation performance\n",
        "    best_acc = 0\n",
        "    best_path = '/content/drive/My Drive/Colab Notebooks/CS464 - HW3/best_model_transfer_learning.pth'\n",
        "    for epoch in range(max_epoch):\n",
        "       train(epoch, model, criterion, optimizer, trainloader)\n",
        "       acc = test(model, val_loader)\n",
        "       if acc > best_acc:\n",
        "          torch.save(model, best_path)\n",
        "          best_acc = acc\n",
        "    print(\"Best Acc:\", best_acc)\n",
        "''' Train your network for a one epoch '''\n",
        "def train(epoch, model, criterion, optimizer, loader): # you are free to change parameters\n",
        "    model.train()\n",
        "    losses = AverageMeter()\n",
        "    accuracies = AverageMeter()\n",
        "\n",
        "    for batch_idx, (data, labels) in enumerate(loader):\n",
        "        # TODO:\n",
        "        # Implement training code for a one iteration\n",
        "        y_pred = model(data.float().cuda())\n",
        "        loss = criterion(y_pred, labels.long().cuda())\n",
        "        losses.update(loss.item(), data.size(0))\n",
        "        accuracy = find_accuracy(y_pred, labels)\n",
        "        # accuracy = pf1(y_pred, labels)\n",
        "        accuracies.update(accuracy)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # print('Epoch: [{0}][{1}/{2}]\\t'\n",
        "        #       'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "        #       'Data {data_time.val:.4f} ({data_time.avg:.4f})\\t'\n",
        "        #       'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "        #       'Accu {acc.val:.4f} ({acc.avg:.4f})\\t'.format(\n",
        "        #        epoch + 1, batch_idx + 1, len(trainloader), \n",
        "        #        batch_time=batch_time,\n",
        "        #        data_time=data_time, \n",
        "        #        loss=losses,\n",
        "        #        acc=accuracies))\n",
        "        print('Epoch: [{0}][{1}/{2}]\\t'\n",
        "              'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "              'Accu {acc.val:.4f} ({acc.avg:.4f})\\t'.format(\n",
        "               epoch + 1, batch_idx + 1, len(loader), \n",
        "               loss=losses,\n",
        "               acc=accuracies))\n",
        "\n",
        "\n",
        "''' Test&Validate your network '''\n",
        "def test(model, loader): # you are free to change parameters\n",
        "\n",
        "    model.eval()\n",
        "    # losses = AverageMeter()\n",
        "    accuracies = AverageMeter()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, labels) in enumerate(loader):\n",
        "            # TODO:\n",
        "            # Implement test code\n",
        "            # y_pred = model(data.long().conda())\n",
        "            y_pred = model(data.float().cuda())\n",
        "            # loss = criterion(y_pred, labels.long().cuda())\n",
        "            # losses.update(loss.item(), data.size(0))\n",
        "            accuracy = find_accuracy(y_pred, labels)\n",
        "            # accuracy = pf1(y_pred, labels)\n",
        "            accuracies.update(accuracy)\n",
        "        # print('Time {batch_time.avg:.3f}\\t'\n",
        "        #       'Accu {acc.avg:.4f}\\t'.format(\n",
        "        #        batch_time=batch_time, \n",
        "        #        acc=accuracies))\n",
        "        print('Accu {acc.avg:.4f}\\t'.format(\n",
        "               acc=accuracies))\n",
        "    return accuracies.avg\n",
        "\n",
        "main()\n"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Use GPU: True\n",
            "torch.Size([3, 100, 100])\n",
            "torch.Size([2000, 3, 100, 100])\n",
            "Epoch: [1][1/44]\tLoss 2.3768 (2.3768)\tAccu 0.0938 (0.0938)\t\n",
            "Epoch: [1][2/44]\tLoss 2.3776 (2.3772)\tAccu 0.0312 (0.0625)\t\n",
            "Epoch: [1][3/44]\tLoss 2.3442 (2.3662)\tAccu 0.0625 (0.0625)\t\n",
            "Epoch: [1][4/44]\tLoss 2.3354 (2.3585)\tAccu 0.0000 (0.0469)\t\n",
            "Epoch: [1][5/44]\tLoss 2.3195 (2.3507)\tAccu 0.0938 (0.0563)\t\n",
            "Epoch: [1][6/44]\tLoss 2.2967 (2.3417)\tAccu 0.1250 (0.0677)\t\n",
            "Epoch: [1][7/44]\tLoss 2.3084 (2.3369)\tAccu 0.1562 (0.0804)\t\n",
            "Epoch: [1][8/44]\tLoss 2.3477 (2.3383)\tAccu 0.0938 (0.0820)\t\n",
            "Epoch: [1][9/44]\tLoss 2.3618 (2.3409)\tAccu 0.0625 (0.0799)\t\n",
            "Epoch: [1][10/44]\tLoss 2.3911 (2.3459)\tAccu 0.1250 (0.0844)\t\n",
            "Epoch: [1][11/44]\tLoss 2.2920 (2.3410)\tAccu 0.1875 (0.0938)\t\n",
            "Epoch: [1][12/44]\tLoss 2.2928 (2.3370)\tAccu 0.0938 (0.0938)\t\n",
            "Epoch: [1][13/44]\tLoss 2.3173 (2.3355)\tAccu 0.0938 (0.0938)\t\n",
            "Epoch: [1][14/44]\tLoss 2.2653 (2.3305)\tAccu 0.0938 (0.0938)\t\n",
            "Epoch: [1][15/44]\tLoss 2.2955 (2.3282)\tAccu 0.2188 (0.1021)\t\n",
            "Epoch: [1][16/44]\tLoss 2.4802 (2.3377)\tAccu 0.1250 (0.1035)\t\n",
            "Epoch: [1][17/44]\tLoss 2.2403 (2.3319)\tAccu 0.1562 (0.1066)\t\n",
            "Epoch: [1][18/44]\tLoss 2.3197 (2.3313)\tAccu 0.1562 (0.1094)\t\n",
            "Epoch: [1][19/44]\tLoss 2.3420 (2.3318)\tAccu 0.0625 (0.1069)\t\n",
            "Epoch: [1][20/44]\tLoss 2.2886 (2.3297)\tAccu 0.0000 (0.1016)\t\n",
            "Epoch: [1][21/44]\tLoss 2.3188 (2.3291)\tAccu 0.0312 (0.0982)\t\n",
            "Epoch: [1][22/44]\tLoss 2.2788 (2.3269)\tAccu 0.0938 (0.0980)\t\n",
            "Epoch: [1][23/44]\tLoss 2.3145 (2.3263)\tAccu 0.0938 (0.0978)\t\n",
            "Epoch: [1][24/44]\tLoss 2.3417 (2.3270)\tAccu 0.0625 (0.0964)\t\n",
            "Epoch: [1][25/44]\tLoss 2.3121 (2.3264)\tAccu 0.1875 (0.1000)\t\n",
            "Epoch: [1][26/44]\tLoss 2.2602 (2.3238)\tAccu 0.1250 (0.1010)\t\n",
            "Epoch: [1][27/44]\tLoss 2.3459 (2.3246)\tAccu 0.0938 (0.1007)\t\n",
            "Epoch: [1][28/44]\tLoss 2.2464 (2.3218)\tAccu 0.1562 (0.1027)\t\n",
            "Epoch: [1][29/44]\tLoss 2.4340 (2.3257)\tAccu 0.0000 (0.0991)\t\n",
            "Epoch: [1][30/44]\tLoss 2.3656 (2.3270)\tAccu 0.0938 (0.0990)\t\n",
            "Epoch: [1][31/44]\tLoss 2.3150 (2.3267)\tAccu 0.1562 (0.1008)\t\n",
            "Epoch: [1][32/44]\tLoss 2.2942 (2.3256)\tAccu 0.0625 (0.0996)\t\n",
            "Epoch: [1][33/44]\tLoss 2.4034 (2.3280)\tAccu 0.1562 (0.1013)\t\n",
            "Epoch: [1][34/44]\tLoss 2.2007 (2.3243)\tAccu 0.2188 (0.1048)\t\n",
            "Epoch: [1][35/44]\tLoss 2.2510 (2.3222)\tAccu 0.1250 (0.1054)\t\n",
            "Epoch: [1][36/44]\tLoss 2.2917 (2.3213)\tAccu 0.1250 (0.1059)\t\n",
            "Epoch: [1][37/44]\tLoss 2.2740 (2.3200)\tAccu 0.1562 (0.1073)\t\n",
            "Epoch: [1][38/44]\tLoss 2.3801 (2.3216)\tAccu 0.1250 (0.1077)\t\n",
            "Epoch: [1][39/44]\tLoss 2.2138 (2.3188)\tAccu 0.2188 (0.1106)\t\n",
            "Epoch: [1][40/44]\tLoss 2.2873 (2.3181)\tAccu 0.0312 (0.1086)\t\n",
            "Epoch: [1][41/44]\tLoss 2.2787 (2.3171)\tAccu 0.1562 (0.1098)\t\n",
            "Epoch: [1][42/44]\tLoss 2.2499 (2.3155)\tAccu 0.0938 (0.1094)\t\n",
            "Epoch: [1][43/44]\tLoss 2.3275 (2.3158)\tAccu 0.0938 (0.1090)\t\n",
            "Epoch: [1][44/44]\tLoss 2.2541 (2.3147)\tAccu 0.2083 (0.1113)\t\n",
            "Accu 0.1625\t\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Inception3. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BasicConv2d. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Conv2d. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BatchNorm2d. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type InceptionA. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type InceptionB. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type InceptionC. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type InceptionAux. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type InceptionD. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type InceptionE. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: [2][1/44]\tLoss 2.2697 (2.2697)\tAccu 0.1562 (0.1562)\t\n",
            "Epoch: [2][2/44]\tLoss 2.3225 (2.2961)\tAccu 0.2188 (0.1875)\t\n",
            "Epoch: [2][3/44]\tLoss 2.2629 (2.2850)\tAccu 0.0938 (0.1562)\t\n",
            "Epoch: [2][4/44]\tLoss 2.2182 (2.2683)\tAccu 0.1562 (0.1562)\t\n",
            "Epoch: [2][5/44]\tLoss 2.2867 (2.2720)\tAccu 0.1875 (0.1625)\t\n",
            "Epoch: [2][6/44]\tLoss 2.3387 (2.2831)\tAccu 0.1250 (0.1562)\t\n",
            "Epoch: [2][7/44]\tLoss 2.4044 (2.3004)\tAccu 0.0625 (0.1429)\t\n",
            "Epoch: [2][8/44]\tLoss 2.2032 (2.2883)\tAccu 0.1250 (0.1406)\t\n",
            "Epoch: [2][9/44]\tLoss 2.3000 (2.2896)\tAccu 0.0938 (0.1354)\t\n",
            "Epoch: [2][10/44]\tLoss 2.2901 (2.2896)\tAccu 0.1250 (0.1344)\t\n",
            "Epoch: [2][11/44]\tLoss 2.3376 (2.2940)\tAccu 0.1250 (0.1335)\t\n",
            "Epoch: [2][12/44]\tLoss 2.2535 (2.2906)\tAccu 0.1250 (0.1328)\t\n",
            "Epoch: [2][13/44]\tLoss 2.2660 (2.2887)\tAccu 0.1562 (0.1346)\t\n",
            "Epoch: [2][14/44]\tLoss 2.2000 (2.2824)\tAccu 0.2812 (0.1451)\t\n",
            "Epoch: [2][15/44]\tLoss 2.2055 (2.2773)\tAccu 0.2500 (0.1521)\t\n",
            "Epoch: [2][16/44]\tLoss 2.2898 (2.2780)\tAccu 0.1875 (0.1543)\t\n",
            "Epoch: [2][17/44]\tLoss 2.3203 (2.2805)\tAccu 0.0938 (0.1507)\t\n",
            "Epoch: [2][18/44]\tLoss 2.1670 (2.2742)\tAccu 0.2500 (0.1562)\t\n",
            "Epoch: [2][19/44]\tLoss 2.1853 (2.2695)\tAccu 0.2188 (0.1595)\t\n",
            "Epoch: [2][20/44]\tLoss 2.1825 (2.2652)\tAccu 0.2812 (0.1656)\t\n",
            "Epoch: [2][21/44]\tLoss 2.2380 (2.2639)\tAccu 0.1875 (0.1667)\t\n",
            "Epoch: [2][22/44]\tLoss 2.2359 (2.2626)\tAccu 0.2500 (0.1705)\t\n",
            "Epoch: [2][23/44]\tLoss 2.2425 (2.2617)\tAccu 0.2500 (0.1739)\t\n",
            "Epoch: [2][24/44]\tLoss 2.2508 (2.2613)\tAccu 0.2188 (0.1758)\t\n",
            "Epoch: [2][25/44]\tLoss 2.2970 (2.2627)\tAccu 0.1250 (0.1737)\t\n",
            "Epoch: [2][26/44]\tLoss 2.1987 (2.2603)\tAccu 0.2500 (0.1767)\t\n",
            "Epoch: [2][27/44]\tLoss 2.2216 (2.2588)\tAccu 0.2188 (0.1782)\t\n",
            "Epoch: [2][28/44]\tLoss 2.2799 (2.2596)\tAccu 0.2812 (0.1819)\t\n",
            "Epoch: [2][29/44]\tLoss 2.2721 (2.2600)\tAccu 0.1875 (0.1821)\t\n",
            "Epoch: [2][30/44]\tLoss 2.2587 (2.2600)\tAccu 0.1250 (0.1802)\t\n",
            "Epoch: [2][31/44]\tLoss 2.1859 (2.2576)\tAccu 0.2500 (0.1825)\t\n",
            "Epoch: [2][32/44]\tLoss 2.0950 (2.2525)\tAccu 0.2500 (0.1846)\t\n",
            "Epoch: [2][33/44]\tLoss 2.1436 (2.2492)\tAccu 0.3438 (0.1894)\t\n",
            "Epoch: [2][34/44]\tLoss 2.1513 (2.2463)\tAccu 0.2500 (0.1912)\t\n",
            "Epoch: [2][35/44]\tLoss 2.2798 (2.2473)\tAccu 0.1562 (0.1902)\t\n",
            "Epoch: [2][36/44]\tLoss 2.2217 (2.2466)\tAccu 0.2500 (0.1918)\t\n",
            "Epoch: [2][37/44]\tLoss 2.1297 (2.2434)\tAccu 0.2500 (0.1934)\t\n",
            "Epoch: [2][38/44]\tLoss 2.2279 (2.2430)\tAccu 0.1875 (0.1933)\t\n",
            "Epoch: [2][39/44]\tLoss 2.1174 (2.2398)\tAccu 0.3438 (0.1971)\t\n",
            "Epoch: [2][40/44]\tLoss 2.0839 (2.2359)\tAccu 0.3438 (0.2008)\t\n",
            "Epoch: [2][41/44]\tLoss 2.1460 (2.2337)\tAccu 0.2500 (0.2020)\t\n",
            "Epoch: [2][42/44]\tLoss 2.0305 (2.2289)\tAccu 0.3438 (0.2054)\t\n",
            "Epoch: [2][43/44]\tLoss 2.1366 (2.2267)\tAccu 0.2500 (0.2064)\t\n",
            "Epoch: [2][44/44]\tLoss 2.1683 (2.2257)\tAccu 0.1250 (0.2045)\t\n",
            "Accu 0.1800\t\n",
            "Epoch: [3][1/44]\tLoss 2.2009 (2.2009)\tAccu 0.1875 (0.1875)\t\n",
            "Epoch: [3][2/44]\tLoss 2.1256 (2.1632)\tAccu 0.2812 (0.2344)\t\n",
            "Epoch: [3][3/44]\tLoss 2.1903 (2.1722)\tAccu 0.2188 (0.2292)\t\n",
            "Epoch: [3][4/44]\tLoss 1.9959 (2.1282)\tAccu 0.3750 (0.2656)\t\n",
            "Epoch: [3][5/44]\tLoss 2.1204 (2.1266)\tAccu 0.4375 (0.3000)\t\n",
            "Epoch: [3][6/44]\tLoss 2.2470 (2.1467)\tAccu 0.2188 (0.2865)\t\n",
            "Epoch: [3][7/44]\tLoss 2.2702 (2.1643)\tAccu 0.1875 (0.2723)\t\n",
            "Epoch: [3][8/44]\tLoss 2.1809 (2.1664)\tAccu 0.2188 (0.2656)\t\n",
            "Epoch: [3][9/44]\tLoss 2.0891 (2.1578)\tAccu 0.3438 (0.2743)\t\n",
            "Epoch: [3][10/44]\tLoss 2.1308 (2.1551)\tAccu 0.2188 (0.2687)\t\n",
            "Epoch: [3][11/44]\tLoss 2.0247 (2.1432)\tAccu 0.3438 (0.2756)\t\n",
            "Epoch: [3][12/44]\tLoss 2.2687 (2.1537)\tAccu 0.2812 (0.2760)\t\n",
            "Epoch: [3][13/44]\tLoss 1.9740 (2.1399)\tAccu 0.5000 (0.2933)\t\n",
            "Epoch: [3][14/44]\tLoss 2.1075 (2.1376)\tAccu 0.1875 (0.2857)\t\n",
            "Epoch: [3][15/44]\tLoss 2.0538 (2.1320)\tAccu 0.3125 (0.2875)\t\n",
            "Epoch: [3][16/44]\tLoss 2.0829 (2.1289)\tAccu 0.4688 (0.2988)\t\n",
            "Epoch: [3][17/44]\tLoss 2.0565 (2.1247)\tAccu 0.3438 (0.3015)\t\n",
            "Epoch: [3][18/44]\tLoss 2.0237 (2.1190)\tAccu 0.2188 (0.2969)\t\n",
            "Epoch: [3][19/44]\tLoss 2.0120 (2.1134)\tAccu 0.4688 (0.3059)\t\n",
            "Epoch: [3][20/44]\tLoss 1.9328 (2.1044)\tAccu 0.4062 (0.3109)\t\n",
            "Epoch: [3][21/44]\tLoss 2.1270 (2.1055)\tAccu 0.1875 (0.3051)\t\n",
            "Epoch: [3][22/44]\tLoss 2.1181 (2.1060)\tAccu 0.2812 (0.3040)\t\n",
            "Epoch: [3][23/44]\tLoss 2.0306 (2.1028)\tAccu 0.3438 (0.3057)\t\n",
            "Epoch: [3][24/44]\tLoss 2.0364 (2.1000)\tAccu 0.2500 (0.3034)\t\n",
            "Epoch: [3][25/44]\tLoss 2.0987 (2.0999)\tAccu 0.2812 (0.3025)\t\n",
            "Epoch: [3][26/44]\tLoss 1.9930 (2.0958)\tAccu 0.5000 (0.3101)\t\n",
            "Epoch: [3][27/44]\tLoss 1.9544 (2.0906)\tAccu 0.2500 (0.3079)\t\n",
            "Epoch: [3][28/44]\tLoss 1.9461 (2.0854)\tAccu 0.2188 (0.3047)\t\n",
            "Epoch: [3][29/44]\tLoss 1.9709 (2.0815)\tAccu 0.4375 (0.3093)\t\n",
            "Epoch: [3][30/44]\tLoss 1.9981 (2.0787)\tAccu 0.2500 (0.3073)\t\n",
            "Epoch: [3][31/44]\tLoss 1.9039 (2.0731)\tAccu 0.3750 (0.3095)\t\n",
            "Epoch: [3][32/44]\tLoss 1.9219 (2.0683)\tAccu 0.3125 (0.3096)\t\n",
            "Epoch: [3][33/44]\tLoss 1.9713 (2.0654)\tAccu 0.4688 (0.3144)\t\n",
            "Epoch: [3][34/44]\tLoss 2.0037 (2.0636)\tAccu 0.2812 (0.3134)\t\n",
            "Epoch: [3][35/44]\tLoss 2.0921 (2.0644)\tAccu 0.3125 (0.3134)\t\n",
            "Epoch: [3][36/44]\tLoss 2.1385 (2.0665)\tAccu 0.1875 (0.3099)\t\n",
            "Epoch: [3][37/44]\tLoss 1.9345 (2.0629)\tAccu 0.4688 (0.3142)\t\n",
            "Epoch: [3][38/44]\tLoss 2.0204 (2.0618)\tAccu 0.3125 (0.3141)\t\n",
            "Epoch: [3][39/44]\tLoss 1.9072 (2.0578)\tAccu 0.3750 (0.3157)\t\n",
            "Epoch: [3][40/44]\tLoss 1.8114 (2.0516)\tAccu 0.4688 (0.3195)\t\n",
            "Epoch: [3][41/44]\tLoss 1.8113 (2.0458)\tAccu 0.4375 (0.3224)\t\n",
            "Epoch: [3][42/44]\tLoss 1.7726 (2.0393)\tAccu 0.5312 (0.3274)\t\n",
            "Epoch: [3][43/44]\tLoss 1.9074 (2.0362)\tAccu 0.4062 (0.3292)\t\n",
            "Epoch: [3][44/44]\tLoss 1.9136 (2.0341)\tAccu 0.4167 (0.3312)\t\n",
            "Accu 0.2625\t\n",
            "Epoch: [4][1/44]\tLoss 1.8622 (1.8622)\tAccu 0.4375 (0.4375)\t\n",
            "Epoch: [4][2/44]\tLoss 1.8062 (1.8342)\tAccu 0.4062 (0.4219)\t\n",
            "Epoch: [4][3/44]\tLoss 1.8610 (1.8432)\tAccu 0.4062 (0.4167)\t\n",
            "Epoch: [4][4/44]\tLoss 1.7947 (1.8310)\tAccu 0.4375 (0.4219)\t\n",
            "Epoch: [4][5/44]\tLoss 1.7975 (1.8243)\tAccu 0.4062 (0.4188)\t\n",
            "Epoch: [4][6/44]\tLoss 1.8633 (1.8308)\tAccu 0.4062 (0.4167)\t\n",
            "Epoch: [4][7/44]\tLoss 1.8593 (1.8349)\tAccu 0.4062 (0.4152)\t\n",
            "Epoch: [4][8/44]\tLoss 1.6683 (1.8141)\tAccu 0.5000 (0.4258)\t\n",
            "Epoch: [4][9/44]\tLoss 1.8161 (1.8143)\tAccu 0.5000 (0.4340)\t\n",
            "Epoch: [4][10/44]\tLoss 1.8077 (1.8136)\tAccu 0.5000 (0.4406)\t\n",
            "Epoch: [4][11/44]\tLoss 1.7212 (1.8052)\tAccu 0.5312 (0.4489)\t\n",
            "Epoch: [4][12/44]\tLoss 1.8632 (1.8100)\tAccu 0.5000 (0.4531)\t\n",
            "Epoch: [4][13/44]\tLoss 1.7104 (1.8024)\tAccu 0.4062 (0.4495)\t\n",
            "Epoch: [4][14/44]\tLoss 1.8890 (1.8086)\tAccu 0.2812 (0.4375)\t\n",
            "Epoch: [4][15/44]\tLoss 1.9363 (1.8171)\tAccu 0.2188 (0.4229)\t\n",
            "Epoch: [4][16/44]\tLoss 1.8585 (1.8197)\tAccu 0.5000 (0.4277)\t\n",
            "Epoch: [4][17/44]\tLoss 1.8369 (1.8207)\tAccu 0.4375 (0.4283)\t\n",
            "Epoch: [4][18/44]\tLoss 1.8665 (1.8232)\tAccu 0.3438 (0.4236)\t\n",
            "Epoch: [4][19/44]\tLoss 1.6885 (1.8161)\tAccu 0.4062 (0.4227)\t\n",
            "Epoch: [4][20/44]\tLoss 1.6533 (1.8080)\tAccu 0.4688 (0.4250)\t\n",
            "Epoch: [4][21/44]\tLoss 1.7901 (1.8071)\tAccu 0.4375 (0.4256)\t\n",
            "Epoch: [4][22/44]\tLoss 1.6957 (1.8021)\tAccu 0.4688 (0.4276)\t\n",
            "Epoch: [4][23/44]\tLoss 1.6800 (1.7968)\tAccu 0.5312 (0.4321)\t\n",
            "Epoch: [4][24/44]\tLoss 1.5803 (1.7878)\tAccu 0.5938 (0.4388)\t\n",
            "Epoch: [4][25/44]\tLoss 1.8166 (1.7889)\tAccu 0.5000 (0.4412)\t\n",
            "Epoch: [4][26/44]\tLoss 1.5684 (1.7804)\tAccu 0.5938 (0.4471)\t\n",
            "Epoch: [4][27/44]\tLoss 1.5958 (1.7736)\tAccu 0.4688 (0.4479)\t\n",
            "Epoch: [4][28/44]\tLoss 1.7458 (1.7726)\tAccu 0.5000 (0.4498)\t\n",
            "Epoch: [4][29/44]\tLoss 1.6134 (1.7671)\tAccu 0.5000 (0.4515)\t\n",
            "Epoch: [4][30/44]\tLoss 1.6824 (1.7643)\tAccu 0.5000 (0.4531)\t\n",
            "Epoch: [4][31/44]\tLoss 1.5725 (1.7581)\tAccu 0.5625 (0.4567)\t\n",
            "Epoch: [4][32/44]\tLoss 1.4920 (1.7498)\tAccu 0.6562 (0.4629)\t\n",
            "Epoch: [4][33/44]\tLoss 1.6991 (1.7482)\tAccu 0.6250 (0.4678)\t\n",
            "Epoch: [4][34/44]\tLoss 1.5827 (1.7434)\tAccu 0.4375 (0.4669)\t\n",
            "Epoch: [4][35/44]\tLoss 1.6837 (1.7417)\tAccu 0.4688 (0.4670)\t\n",
            "Epoch: [4][36/44]\tLoss 1.7545 (1.7420)\tAccu 0.4375 (0.4661)\t\n",
            "Epoch: [4][37/44]\tLoss 1.5737 (1.7375)\tAccu 0.6250 (0.4704)\t\n",
            "Epoch: [4][38/44]\tLoss 1.6570 (1.7354)\tAccu 0.5000 (0.4712)\t\n",
            "Epoch: [4][39/44]\tLoss 1.6076 (1.7321)\tAccu 0.6562 (0.4760)\t\n",
            "Epoch: [4][40/44]\tLoss 1.4376 (1.7247)\tAccu 0.5312 (0.4773)\t\n",
            "Epoch: [4][41/44]\tLoss 1.5215 (1.7198)\tAccu 0.5312 (0.4787)\t\n",
            "Epoch: [4][42/44]\tLoss 1.4557 (1.7135)\tAccu 0.6250 (0.4821)\t\n",
            "Epoch: [4][43/44]\tLoss 1.5532 (1.7097)\tAccu 0.5938 (0.4847)\t\n",
            "Epoch: [4][44/44]\tLoss 1.5435 (1.7069)\tAccu 0.6250 (0.4879)\t\n",
            "Accu 0.3275\t\n",
            "Epoch: [5][1/44]\tLoss 1.5528 (1.5528)\tAccu 0.5000 (0.5000)\t\n",
            "Epoch: [5][2/44]\tLoss 1.6014 (1.5771)\tAccu 0.5000 (0.5000)\t\n",
            "Epoch: [5][3/44]\tLoss 1.5116 (1.5552)\tAccu 0.5625 (0.5208)\t\n",
            "Epoch: [5][4/44]\tLoss 1.3828 (1.5121)\tAccu 0.6875 (0.5625)\t\n",
            "Epoch: [5][5/44]\tLoss 1.4287 (1.4954)\tAccu 0.7188 (0.5938)\t\n",
            "Epoch: [5][6/44]\tLoss 1.6008 (1.5130)\tAccu 0.5312 (0.5833)\t\n",
            "Epoch: [5][7/44]\tLoss 1.5616 (1.5200)\tAccu 0.5000 (0.5714)\t\n",
            "Epoch: [5][8/44]\tLoss 1.2387 (1.4848)\tAccu 0.7500 (0.5938)\t\n",
            "Epoch: [5][9/44]\tLoss 1.3328 (1.4679)\tAccu 0.6562 (0.6007)\t\n",
            "Epoch: [5][10/44]\tLoss 1.4164 (1.4628)\tAccu 0.7188 (0.6125)\t\n",
            "Epoch: [5][11/44]\tLoss 1.2905 (1.4471)\tAccu 0.7500 (0.6250)\t\n",
            "Epoch: [5][12/44]\tLoss 1.4738 (1.4493)\tAccu 0.6250 (0.6250)\t\n",
            "Epoch: [5][13/44]\tLoss 1.2945 (1.4374)\tAccu 0.6250 (0.6250)\t\n",
            "Epoch: [5][14/44]\tLoss 1.4077 (1.4353)\tAccu 0.5312 (0.6183)\t\n",
            "Epoch: [5][15/44]\tLoss 1.4915 (1.4390)\tAccu 0.5625 (0.6146)\t\n",
            "Epoch: [5][16/44]\tLoss 1.4064 (1.4370)\tAccu 0.5000 (0.6074)\t\n",
            "Epoch: [5][17/44]\tLoss 1.4280 (1.4365)\tAccu 0.4688 (0.5993)\t\n",
            "Epoch: [5][18/44]\tLoss 1.5488 (1.4427)\tAccu 0.5312 (0.5955)\t\n",
            "Epoch: [5][19/44]\tLoss 1.3003 (1.4352)\tAccu 0.7188 (0.6020)\t\n",
            "Epoch: [5][20/44]\tLoss 1.2346 (1.4252)\tAccu 0.6562 (0.6047)\t\n",
            "Epoch: [5][21/44]\tLoss 1.4562 (1.4267)\tAccu 0.5938 (0.6042)\t\n",
            "Epoch: [5][22/44]\tLoss 1.2413 (1.4182)\tAccu 0.5938 (0.6037)\t\n",
            "Epoch: [5][23/44]\tLoss 1.3354 (1.4146)\tAccu 0.5000 (0.5992)\t\n",
            "Epoch: [5][24/44]\tLoss 1.3534 (1.4121)\tAccu 0.7188 (0.6042)\t\n",
            "Epoch: [5][25/44]\tLoss 1.5099 (1.4160)\tAccu 0.5000 (0.6000)\t\n",
            "Epoch: [5][26/44]\tLoss 1.1168 (1.4045)\tAccu 0.7188 (0.6046)\t\n",
            "Epoch: [5][27/44]\tLoss 1.2609 (1.3992)\tAccu 0.6250 (0.6053)\t\n",
            "Epoch: [5][28/44]\tLoss 1.3913 (1.3989)\tAccu 0.5312 (0.6027)\t\n",
            "Epoch: [5][29/44]\tLoss 1.3354 (1.3967)\tAccu 0.6562 (0.6045)\t\n",
            "Epoch: [5][30/44]\tLoss 1.3451 (1.3950)\tAccu 0.5312 (0.6021)\t\n",
            "Epoch: [5][31/44]\tLoss 1.3371 (1.3931)\tAccu 0.5938 (0.6018)\t\n",
            "Epoch: [5][32/44]\tLoss 1.3234 (1.3909)\tAccu 0.5625 (0.6006)\t\n",
            "Epoch: [5][33/44]\tLoss 1.2313 (1.3861)\tAccu 0.5938 (0.6004)\t\n",
            "Epoch: [5][34/44]\tLoss 1.4642 (1.3884)\tAccu 0.5000 (0.5974)\t\n",
            "Epoch: [5][35/44]\tLoss 1.3404 (1.3870)\tAccu 0.5625 (0.5964)\t\n",
            "Epoch: [5][36/44]\tLoss 1.4880 (1.3898)\tAccu 0.4375 (0.5920)\t\n",
            "Epoch: [5][37/44]\tLoss 1.0953 (1.3819)\tAccu 0.7500 (0.5963)\t\n",
            "Epoch: [5][38/44]\tLoss 1.1652 (1.3762)\tAccu 0.6875 (0.5987)\t\n",
            "Epoch: [5][39/44]\tLoss 1.1142 (1.3694)\tAccu 0.8125 (0.6042)\t\n",
            "Epoch: [5][40/44]\tLoss 0.9334 (1.3585)\tAccu 0.8438 (0.6102)\t\n",
            "Epoch: [5][41/44]\tLoss 1.0535 (1.3511)\tAccu 0.7188 (0.6128)\t\n",
            "Epoch: [5][42/44]\tLoss 0.9767 (1.3422)\tAccu 0.7500 (0.6161)\t\n",
            "Epoch: [5][43/44]\tLoss 1.1496 (1.3377)\tAccu 0.6875 (0.6177)\t\n",
            "Epoch: [5][44/44]\tLoss 1.0356 (1.3325)\tAccu 0.7917 (0.6217)\t\n",
            "Accu 0.3450\t\n",
            "Epoch: [6][1/44]\tLoss 1.0933 (1.0933)\tAccu 0.7500 (0.7500)\t\n",
            "Epoch: [6][2/44]\tLoss 0.9587 (1.0260)\tAccu 0.7500 (0.7500)\t\n",
            "Epoch: [6][3/44]\tLoss 1.2254 (1.0925)\tAccu 0.5625 (0.6875)\t\n",
            "Epoch: [6][4/44]\tLoss 0.9723 (1.0624)\tAccu 0.6875 (0.6875)\t\n",
            "Epoch: [6][5/44]\tLoss 0.9777 (1.0455)\tAccu 0.8125 (0.7125)\t\n",
            "Epoch: [6][6/44]\tLoss 1.1495 (1.0628)\tAccu 0.6875 (0.7083)\t\n",
            "Epoch: [6][7/44]\tLoss 1.1596 (1.0766)\tAccu 0.7188 (0.7098)\t\n",
            "Epoch: [6][8/44]\tLoss 0.8929 (1.0537)\tAccu 0.7812 (0.7188)\t\n",
            "Epoch: [6][9/44]\tLoss 1.0207 (1.0500)\tAccu 0.7500 (0.7222)\t\n",
            "Epoch: [6][10/44]\tLoss 1.0429 (1.0493)\tAccu 0.7812 (0.7281)\t\n",
            "Epoch: [6][11/44]\tLoss 0.7369 (1.0209)\tAccu 0.9062 (0.7443)\t\n",
            "Epoch: [6][12/44]\tLoss 0.9868 (1.0180)\tAccu 0.7500 (0.7448)\t\n",
            "Epoch: [6][13/44]\tLoss 0.8533 (1.0054)\tAccu 0.7812 (0.7476)\t\n",
            "Epoch: [6][14/44]\tLoss 0.9400 (1.0007)\tAccu 0.7812 (0.7500)\t\n",
            "Epoch: [6][15/44]\tLoss 1.1688 (1.0119)\tAccu 0.7188 (0.7479)\t\n",
            "Epoch: [6][16/44]\tLoss 0.9082 (1.0054)\tAccu 0.8125 (0.7520)\t\n",
            "Epoch: [6][17/44]\tLoss 1.1500 (1.0139)\tAccu 0.6875 (0.7482)\t\n",
            "Epoch: [6][18/44]\tLoss 1.0930 (1.0183)\tAccu 0.6875 (0.7448)\t\n",
            "Epoch: [6][19/44]\tLoss 0.8146 (1.0076)\tAccu 0.8750 (0.7516)\t\n",
            "Epoch: [6][20/44]\tLoss 0.7791 (0.9962)\tAccu 0.7500 (0.7516)\t\n",
            "Epoch: [6][21/44]\tLoss 0.9956 (0.9962)\tAccu 0.6875 (0.7485)\t\n",
            "Epoch: [6][22/44]\tLoss 1.0057 (0.9966)\tAccu 0.6875 (0.7457)\t\n",
            "Epoch: [6][23/44]\tLoss 1.0502 (0.9989)\tAccu 0.6875 (0.7432)\t\n",
            "Epoch: [6][24/44]\tLoss 1.0857 (1.0025)\tAccu 0.7188 (0.7422)\t\n",
            "Epoch: [6][25/44]\tLoss 1.1667 (1.0091)\tAccu 0.6875 (0.7400)\t\n",
            "Epoch: [6][26/44]\tLoss 0.7782 (1.0002)\tAccu 0.7812 (0.7416)\t\n",
            "Epoch: [6][27/44]\tLoss 0.8344 (0.9941)\tAccu 0.7812 (0.7431)\t\n",
            "Epoch: [6][28/44]\tLoss 0.8498 (0.9889)\tAccu 0.8750 (0.7478)\t\n",
            "Epoch: [6][29/44]\tLoss 0.8442 (0.9839)\tAccu 0.7188 (0.7468)\t\n",
            "Epoch: [6][30/44]\tLoss 0.8831 (0.9806)\tAccu 0.7188 (0.7458)\t\n",
            "Epoch: [6][31/44]\tLoss 0.8777 (0.9773)\tAccu 0.7812 (0.7470)\t\n",
            "Epoch: [6][32/44]\tLoss 0.8554 (0.9735)\tAccu 0.7812 (0.7480)\t\n",
            "Epoch: [6][33/44]\tLoss 0.8622 (0.9701)\tAccu 0.7812 (0.7491)\t\n",
            "Epoch: [6][34/44]\tLoss 1.0513 (0.9725)\tAccu 0.6875 (0.7472)\t\n",
            "Epoch: [6][35/44]\tLoss 0.9654 (0.9723)\tAccu 0.6875 (0.7455)\t\n",
            "Epoch: [6][36/44]\tLoss 1.1330 (0.9767)\tAccu 0.6562 (0.7431)\t\n",
            "Epoch: [6][37/44]\tLoss 0.7747 (0.9713)\tAccu 0.7812 (0.7441)\t\n",
            "Epoch: [6][38/44]\tLoss 0.7889 (0.9665)\tAccu 0.8125 (0.7459)\t\n",
            "Epoch: [6][39/44]\tLoss 0.6519 (0.9584)\tAccu 0.9375 (0.7508)\t\n",
            "Epoch: [6][40/44]\tLoss 0.5630 (0.9485)\tAccu 0.9062 (0.7547)\t\n",
            "Epoch: [6][41/44]\tLoss 0.6778 (0.9419)\tAccu 0.8438 (0.7569)\t\n",
            "Epoch: [6][42/44]\tLoss 0.6344 (0.9346)\tAccu 0.9062 (0.7604)\t\n",
            "Epoch: [6][43/44]\tLoss 0.8585 (0.9328)\tAccu 0.7812 (0.7609)\t\n",
            "Epoch: [6][44/44]\tLoss 0.6405 (0.9278)\tAccu 0.8750 (0.7635)\t\n",
            "Accu 0.3775\t\n",
            "Epoch: [7][1/44]\tLoss 0.6524 (0.6524)\tAccu 0.9062 (0.9062)\t\n",
            "Epoch: [7][2/44]\tLoss 0.6735 (0.6629)\tAccu 0.9062 (0.9062)\t\n",
            "Epoch: [7][3/44]\tLoss 0.8120 (0.7126)\tAccu 0.6875 (0.8333)\t\n",
            "Epoch: [7][4/44]\tLoss 0.7165 (0.7136)\tAccu 0.7812 (0.8203)\t\n",
            "Epoch: [7][5/44]\tLoss 0.6687 (0.7046)\tAccu 0.9062 (0.8375)\t\n",
            "Epoch: [7][6/44]\tLoss 0.7887 (0.7186)\tAccu 0.7812 (0.8281)\t\n",
            "Epoch: [7][7/44]\tLoss 0.7068 (0.7169)\tAccu 0.8438 (0.8304)\t\n",
            "Epoch: [7][8/44]\tLoss 0.5676 (0.6983)\tAccu 0.8438 (0.8320)\t\n",
            "Epoch: [7][9/44]\tLoss 0.6052 (0.6879)\tAccu 0.7812 (0.8264)\t\n",
            "Epoch: [7][10/44]\tLoss 0.7063 (0.6898)\tAccu 0.8438 (0.8281)\t\n",
            "Epoch: [7][11/44]\tLoss 0.4254 (0.6657)\tAccu 0.9688 (0.8409)\t\n",
            "Epoch: [7][12/44]\tLoss 0.7682 (0.6743)\tAccu 0.7500 (0.8333)\t\n",
            "Epoch: [7][13/44]\tLoss 0.5483 (0.6646)\tAccu 0.8750 (0.8365)\t\n",
            "Epoch: [7][14/44]\tLoss 0.5434 (0.6559)\tAccu 0.8438 (0.8371)\t\n",
            "Epoch: [7][15/44]\tLoss 0.5993 (0.6522)\tAccu 0.9062 (0.8417)\t\n",
            "Epoch: [7][16/44]\tLoss 0.5346 (0.6448)\tAccu 0.9062 (0.8457)\t\n",
            "Epoch: [7][17/44]\tLoss 0.7889 (0.6533)\tAccu 0.7500 (0.8401)\t\n",
            "Epoch: [7][18/44]\tLoss 0.8223 (0.6627)\tAccu 0.7500 (0.8351)\t\n",
            "Epoch: [7][19/44]\tLoss 0.4339 (0.6506)\tAccu 0.9062 (0.8388)\t\n",
            "Epoch: [7][20/44]\tLoss 0.5348 (0.6448)\tAccu 0.8438 (0.8391)\t\n",
            "Epoch: [7][21/44]\tLoss 0.6612 (0.6456)\tAccu 0.8438 (0.8393)\t\n",
            "Epoch: [7][22/44]\tLoss 0.8169 (0.6534)\tAccu 0.7500 (0.8352)\t\n",
            "Epoch: [7][23/44]\tLoss 0.7659 (0.6583)\tAccu 0.7500 (0.8315)\t\n",
            "Epoch: [7][24/44]\tLoss 0.7248 (0.6611)\tAccu 0.7812 (0.8294)\t\n",
            "Epoch: [7][25/44]\tLoss 0.8740 (0.6696)\tAccu 0.7812 (0.8275)\t\n",
            "Epoch: [7][26/44]\tLoss 0.7014 (0.6708)\tAccu 0.8438 (0.8281)\t\n",
            "Epoch: [7][27/44]\tLoss 0.5291 (0.6656)\tAccu 0.8750 (0.8299)\t\n",
            "Epoch: [7][28/44]\tLoss 0.5662 (0.6620)\tAccu 0.9062 (0.8326)\t\n",
            "Epoch: [7][29/44]\tLoss 0.5394 (0.6578)\tAccu 0.8750 (0.8341)\t\n",
            "Epoch: [7][30/44]\tLoss 0.5092 (0.6528)\tAccu 0.8750 (0.8354)\t\n",
            "Epoch: [7][31/44]\tLoss 0.7318 (0.6554)\tAccu 0.7812 (0.8337)\t\n",
            "Epoch: [7][32/44]\tLoss 0.4675 (0.6495)\tAccu 0.9688 (0.8379)\t\n",
            "Epoch: [7][33/44]\tLoss 0.6584 (0.6498)\tAccu 0.7188 (0.8343)\t\n",
            "Epoch: [7][34/44]\tLoss 0.6810 (0.6507)\tAccu 0.7812 (0.8327)\t\n",
            "Epoch: [7][35/44]\tLoss 0.6799 (0.6515)\tAccu 0.8125 (0.8321)\t\n",
            "Epoch: [7][36/44]\tLoss 0.7073 (0.6531)\tAccu 0.6875 (0.8281)\t\n",
            "Epoch: [7][37/44]\tLoss 0.4836 (0.6485)\tAccu 0.9062 (0.8302)\t\n",
            "Epoch: [7][38/44]\tLoss 0.3632 (0.6410)\tAccu 0.9688 (0.8339)\t\n",
            "Epoch: [7][39/44]\tLoss 0.2994 (0.6322)\tAccu 0.9688 (0.8373)\t\n",
            "Epoch: [7][40/44]\tLoss 0.3521 (0.6252)\tAccu 0.9688 (0.8406)\t\n",
            "Epoch: [7][41/44]\tLoss 0.3313 (0.6181)\tAccu 0.9062 (0.8422)\t\n",
            "Epoch: [7][42/44]\tLoss 0.3190 (0.6109)\tAccu 0.9688 (0.8452)\t\n",
            "Epoch: [7][43/44]\tLoss 0.5014 (0.6084)\tAccu 0.9062 (0.8467)\t\n",
            "Epoch: [7][44/44]\tLoss 0.4678 (0.6060)\tAccu 0.9167 (0.8482)\t\n",
            "Accu 0.4100\t\n",
            "Epoch: [8][1/44]\tLoss 0.4173 (0.4173)\tAccu 0.9375 (0.9375)\t\n",
            "Epoch: [8][2/44]\tLoss 0.4659 (0.4416)\tAccu 0.9062 (0.9219)\t\n",
            "Epoch: [8][3/44]\tLoss 0.5855 (0.4896)\tAccu 0.7812 (0.8750)\t\n",
            "Epoch: [8][4/44]\tLoss 0.4027 (0.4678)\tAccu 0.9062 (0.8828)\t\n",
            "Epoch: [8][5/44]\tLoss 0.4559 (0.4655)\tAccu 0.9375 (0.8938)\t\n",
            "Epoch: [8][6/44]\tLoss 0.6044 (0.4886)\tAccu 0.8750 (0.8906)\t\n",
            "Epoch: [8][7/44]\tLoss 0.6084 (0.5057)\tAccu 0.9062 (0.8929)\t\n",
            "Epoch: [8][8/44]\tLoss 0.5692 (0.5137)\tAccu 0.8438 (0.8867)\t\n",
            "Epoch: [8][9/44]\tLoss 0.4109 (0.5022)\tAccu 0.9062 (0.8889)\t\n",
            "Epoch: [8][10/44]\tLoss 0.3790 (0.4899)\tAccu 0.9375 (0.8938)\t\n",
            "Epoch: [8][11/44]\tLoss 0.3926 (0.4811)\tAccu 0.9062 (0.8949)\t\n",
            "Epoch: [8][12/44]\tLoss 0.5666 (0.4882)\tAccu 0.9062 (0.8958)\t\n",
            "Epoch: [8][13/44]\tLoss 0.4050 (0.4818)\tAccu 0.8750 (0.8942)\t\n",
            "Epoch: [8][14/44]\tLoss 0.3892 (0.4752)\tAccu 0.9688 (0.8996)\t\n",
            "Epoch: [8][15/44]\tLoss 0.4030 (0.4704)\tAccu 0.9375 (0.9021)\t\n",
            "Epoch: [8][16/44]\tLoss 0.2920 (0.4592)\tAccu 1.0000 (0.9082)\t\n",
            "Epoch: [8][17/44]\tLoss 0.4205 (0.4569)\tAccu 0.9688 (0.9118)\t\n",
            "Epoch: [8][18/44]\tLoss 0.4429 (0.4562)\tAccu 0.8750 (0.9097)\t\n",
            "Epoch: [8][19/44]\tLoss 0.2140 (0.4434)\tAccu 1.0000 (0.9145)\t\n",
            "Epoch: [8][20/44]\tLoss 0.4251 (0.4425)\tAccu 0.8750 (0.9125)\t\n",
            "Epoch: [8][21/44]\tLoss 0.4442 (0.4426)\tAccu 0.9062 (0.9122)\t\n",
            "Epoch: [8][22/44]\tLoss 0.4790 (0.4442)\tAccu 0.8750 (0.9105)\t\n",
            "Epoch: [8][23/44]\tLoss 0.4088 (0.4427)\tAccu 0.9062 (0.9103)\t\n",
            "Epoch: [8][24/44]\tLoss 0.5076 (0.4454)\tAccu 0.8750 (0.9089)\t\n",
            "Epoch: [8][25/44]\tLoss 0.5664 (0.4502)\tAccu 0.8125 (0.9050)\t\n",
            "Epoch: [8][26/44]\tLoss 0.2548 (0.4427)\tAccu 0.9688 (0.9075)\t\n",
            "Epoch: [8][27/44]\tLoss 0.3488 (0.4392)\tAccu 0.9688 (0.9097)\t\n",
            "Epoch: [8][28/44]\tLoss 0.3623 (0.4365)\tAccu 0.9062 (0.9096)\t\n",
            "Epoch: [8][29/44]\tLoss 0.3272 (0.4327)\tAccu 0.9375 (0.9106)\t\n",
            "Epoch: [8][30/44]\tLoss 0.3662 (0.4305)\tAccu 0.9062 (0.9104)\t\n",
            "Epoch: [8][31/44]\tLoss 0.4846 (0.4323)\tAccu 0.9062 (0.9103)\t\n",
            "Epoch: [8][32/44]\tLoss 0.4576 (0.4330)\tAccu 0.8750 (0.9092)\t\n",
            "Epoch: [8][33/44]\tLoss 0.4682 (0.4341)\tAccu 0.8125 (0.9062)\t\n",
            "Epoch: [8][34/44]\tLoss 0.3621 (0.4320)\tAccu 0.9375 (0.9072)\t\n",
            "Epoch: [8][35/44]\tLoss 0.3844 (0.4306)\tAccu 0.9375 (0.9080)\t\n",
            "Epoch: [8][36/44]\tLoss 0.4700 (0.4317)\tAccu 0.8438 (0.9062)\t\n",
            "Epoch: [8][37/44]\tLoss 0.2198 (0.4260)\tAccu 0.9688 (0.9079)\t\n",
            "Epoch: [8][38/44]\tLoss 0.2382 (0.4211)\tAccu 1.0000 (0.9104)\t\n",
            "Epoch: [8][39/44]\tLoss 0.1863 (0.4150)\tAccu 0.9375 (0.9111)\t\n",
            "Epoch: [8][40/44]\tLoss 0.2168 (0.4101)\tAccu 0.9688 (0.9125)\t\n",
            "Epoch: [8][41/44]\tLoss 0.2387 (0.4059)\tAccu 0.9375 (0.9131)\t\n",
            "Epoch: [8][42/44]\tLoss 0.1752 (0.4004)\tAccu 1.0000 (0.9152)\t\n",
            "Epoch: [8][43/44]\tLoss 0.3489 (0.3992)\tAccu 0.9062 (0.9150)\t\n",
            "Epoch: [8][44/44]\tLoss 0.4351 (0.3998)\tAccu 0.9167 (0.9150)\t\n",
            "Accu 0.3925\t\n",
            "Epoch: [9][1/44]\tLoss 0.2530 (0.2530)\tAccu 0.9688 (0.9688)\t\n",
            "Epoch: [9][2/44]\tLoss 0.2138 (0.2334)\tAccu 0.9688 (0.9688)\t\n",
            "Epoch: [9][3/44]\tLoss 0.2764 (0.2477)\tAccu 0.9688 (0.9688)\t\n",
            "Epoch: [9][4/44]\tLoss 0.2681 (0.2528)\tAccu 0.9688 (0.9688)\t\n",
            "Epoch: [9][5/44]\tLoss 0.2652 (0.2553)\tAccu 0.9375 (0.9625)\t\n",
            "Epoch: [9][6/44]\tLoss 0.2800 (0.2594)\tAccu 0.9375 (0.9583)\t\n",
            "Epoch: [9][7/44]\tLoss 0.3292 (0.2694)\tAccu 0.9062 (0.9509)\t\n",
            "Epoch: [9][8/44]\tLoss 0.3825 (0.2835)\tAccu 0.9375 (0.9492)\t\n",
            "Epoch: [9][9/44]\tLoss 0.2468 (0.2794)\tAccu 0.9375 (0.9479)\t\n",
            "Epoch: [9][10/44]\tLoss 0.3542 (0.2869)\tAccu 0.9375 (0.9469)\t\n",
            "Epoch: [9][11/44]\tLoss 0.1507 (0.2745)\tAccu 0.9688 (0.9489)\t\n",
            "Epoch: [9][12/44]\tLoss 0.3090 (0.2774)\tAccu 0.9375 (0.9479)\t\n",
            "Epoch: [9][13/44]\tLoss 0.2078 (0.2720)\tAccu 0.9688 (0.9495)\t\n",
            "Epoch: [9][14/44]\tLoss 0.1707 (0.2648)\tAccu 0.9688 (0.9509)\t\n",
            "Epoch: [9][15/44]\tLoss 0.1392 (0.2564)\tAccu 1.0000 (0.9542)\t\n",
            "Epoch: [9][16/44]\tLoss 0.1373 (0.2490)\tAccu 1.0000 (0.9570)\t\n",
            "Epoch: [9][17/44]\tLoss 0.3075 (0.2524)\tAccu 0.8750 (0.9522)\t\n",
            "Epoch: [9][18/44]\tLoss 0.2181 (0.2505)\tAccu 0.9688 (0.9531)\t\n",
            "Epoch: [9][19/44]\tLoss 0.1427 (0.2448)\tAccu 0.9688 (0.9539)\t\n",
            "Epoch: [9][20/44]\tLoss 0.3484 (0.2500)\tAccu 0.9062 (0.9516)\t\n",
            "Epoch: [9][21/44]\tLoss 0.3992 (0.2571)\tAccu 0.9062 (0.9494)\t\n",
            "Epoch: [9][22/44]\tLoss 0.2704 (0.2577)\tAccu 0.9688 (0.9503)\t\n",
            "Epoch: [9][23/44]\tLoss 0.2621 (0.2579)\tAccu 1.0000 (0.9524)\t\n",
            "Epoch: [9][24/44]\tLoss 0.3185 (0.2604)\tAccu 0.9062 (0.9505)\t\n",
            "Epoch: [9][25/44]\tLoss 0.2820 (0.2613)\tAccu 0.9688 (0.9513)\t\n",
            "Epoch: [9][26/44]\tLoss 0.1609 (0.2574)\tAccu 0.9375 (0.9507)\t\n",
            "Epoch: [9][27/44]\tLoss 0.1692 (0.2542)\tAccu 1.0000 (0.9525)\t\n",
            "Epoch: [9][28/44]\tLoss 0.3689 (0.2583)\tAccu 0.8750 (0.9498)\t\n",
            "Epoch: [9][29/44]\tLoss 0.1911 (0.2560)\tAccu 0.9375 (0.9494)\t\n",
            "Epoch: [9][30/44]\tLoss 0.3322 (0.2585)\tAccu 0.9375 (0.9490)\t\n",
            "Epoch: [9][31/44]\tLoss 0.2404 (0.2579)\tAccu 0.9375 (0.9486)\t\n",
            "Epoch: [9][32/44]\tLoss 0.2445 (0.2575)\tAccu 1.0000 (0.9502)\t\n",
            "Epoch: [9][33/44]\tLoss 0.2679 (0.2578)\tAccu 0.9375 (0.9498)\t\n",
            "Epoch: [9][34/44]\tLoss 0.2142 (0.2565)\tAccu 0.9688 (0.9504)\t\n",
            "Epoch: [9][35/44]\tLoss 0.2045 (0.2550)\tAccu 1.0000 (0.9518)\t\n",
            "Epoch: [9][36/44]\tLoss 0.2672 (0.2554)\tAccu 0.9688 (0.9523)\t\n",
            "Epoch: [9][37/44]\tLoss 0.1302 (0.2520)\tAccu 1.0000 (0.9535)\t\n",
            "Epoch: [9][38/44]\tLoss 0.1622 (0.2496)\tAccu 1.0000 (0.9548)\t\n",
            "Epoch: [9][39/44]\tLoss 0.1095 (0.2460)\tAccu 0.9688 (0.9551)\t\n",
            "Epoch: [9][40/44]\tLoss 0.0898 (0.2421)\tAccu 1.0000 (0.9563)\t\n",
            "Epoch: [9][41/44]\tLoss 0.1464 (0.2398)\tAccu 0.9688 (0.9566)\t\n",
            "Epoch: [9][42/44]\tLoss 0.1051 (0.2366)\tAccu 1.0000 (0.9576)\t\n",
            "Epoch: [9][43/44]\tLoss 0.1675 (0.2350)\tAccu 0.9688 (0.9578)\t\n",
            "Epoch: [9][44/44]\tLoss 0.2259 (0.2348)\tAccu 0.9583 (0.9579)\t\n",
            "Accu 0.3900\t\n",
            "Epoch: [10][1/44]\tLoss 0.1594 (0.1594)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [10][2/44]\tLoss 0.1265 (0.1429)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [10][3/44]\tLoss 0.0994 (0.1284)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [10][4/44]\tLoss 0.1783 (0.1409)\tAccu 0.9688 (0.9922)\t\n",
            "Epoch: [10][5/44]\tLoss 0.1568 (0.1441)\tAccu 0.9688 (0.9875)\t\n",
            "Epoch: [10][6/44]\tLoss 0.2099 (0.1550)\tAccu 0.9688 (0.9844)\t\n",
            "Epoch: [10][7/44]\tLoss 0.1256 (0.1508)\tAccu 0.9688 (0.9821)\t\n",
            "Epoch: [10][8/44]\tLoss 0.1552 (0.1514)\tAccu 1.0000 (0.9844)\t\n",
            "Epoch: [10][9/44]\tLoss 0.1207 (0.1480)\tAccu 0.9688 (0.9826)\t\n",
            "Epoch: [10][10/44]\tLoss 0.1399 (0.1471)\tAccu 1.0000 (0.9844)\t\n",
            "Epoch: [10][11/44]\tLoss 0.0941 (0.1423)\tAccu 0.9688 (0.9830)\t\n",
            "Epoch: [10][12/44]\tLoss 0.1664 (0.1443)\tAccu 1.0000 (0.9844)\t\n",
            "Epoch: [10][13/44]\tLoss 0.0834 (0.1396)\tAccu 1.0000 (0.9856)\t\n",
            "Epoch: [10][14/44]\tLoss 0.2020 (0.1441)\tAccu 0.9375 (0.9821)\t\n",
            "Epoch: [10][15/44]\tLoss 0.1131 (0.1420)\tAccu 0.9688 (0.9812)\t\n",
            "Epoch: [10][16/44]\tLoss 0.0478 (0.1361)\tAccu 1.0000 (0.9824)\t\n",
            "Epoch: [10][17/44]\tLoss 0.1283 (0.1357)\tAccu 0.9688 (0.9816)\t\n",
            "Epoch: [10][18/44]\tLoss 0.1209 (0.1349)\tAccu 1.0000 (0.9826)\t\n",
            "Epoch: [10][19/44]\tLoss 0.0841 (0.1322)\tAccu 1.0000 (0.9836)\t\n",
            "Epoch: [10][20/44]\tLoss 0.1272 (0.1319)\tAccu 1.0000 (0.9844)\t\n",
            "Epoch: [10][21/44]\tLoss 0.2430 (0.1372)\tAccu 0.9375 (0.9821)\t\n",
            "Epoch: [10][22/44]\tLoss 0.0914 (0.1351)\tAccu 1.0000 (0.9830)\t\n",
            "Epoch: [10][23/44]\tLoss 0.1251 (0.1347)\tAccu 1.0000 (0.9837)\t\n",
            "Epoch: [10][24/44]\tLoss 0.2023 (0.1375)\tAccu 0.9375 (0.9818)\t\n",
            "Epoch: [10][25/44]\tLoss 0.1036 (0.1362)\tAccu 1.0000 (0.9825)\t\n",
            "Epoch: [10][26/44]\tLoss 0.1238 (0.1357)\tAccu 0.9688 (0.9820)\t\n",
            "Epoch: [10][27/44]\tLoss 0.0933 (0.1341)\tAccu 0.9688 (0.9815)\t\n",
            "Epoch: [10][28/44]\tLoss 0.1362 (0.1342)\tAccu 0.9688 (0.9810)\t\n",
            "Epoch: [10][29/44]\tLoss 0.1064 (0.1332)\tAccu 1.0000 (0.9817)\t\n",
            "Epoch: [10][30/44]\tLoss 0.1819 (0.1349)\tAccu 0.9688 (0.9812)\t\n",
            "Epoch: [10][31/44]\tLoss 0.1465 (0.1352)\tAccu 0.9375 (0.9798)\t\n",
            "Epoch: [10][32/44]\tLoss 0.0984 (0.1341)\tAccu 1.0000 (0.9805)\t\n",
            "Epoch: [10][33/44]\tLoss 0.1231 (0.1338)\tAccu 0.9375 (0.9792)\t\n",
            "Epoch: [10][34/44]\tLoss 0.0715 (0.1319)\tAccu 1.0000 (0.9798)\t\n",
            "Epoch: [10][35/44]\tLoss 0.1234 (0.1317)\tAccu 0.9688 (0.9795)\t\n",
            "Epoch: [10][36/44]\tLoss 0.1564 (0.1324)\tAccu 1.0000 (0.9800)\t\n",
            "Epoch: [10][37/44]\tLoss 0.0698 (0.1307)\tAccu 1.0000 (0.9806)\t\n",
            "Epoch: [10][38/44]\tLoss 0.0536 (0.1286)\tAccu 1.0000 (0.9811)\t\n",
            "Epoch: [10][39/44]\tLoss 0.0535 (0.1267)\tAccu 1.0000 (0.9816)\t\n",
            "Epoch: [10][40/44]\tLoss 0.0579 (0.1250)\tAccu 1.0000 (0.9820)\t\n",
            "Epoch: [10][41/44]\tLoss 0.0796 (0.1239)\tAccu 0.9688 (0.9817)\t\n",
            "Epoch: [10][42/44]\tLoss 0.0467 (0.1221)\tAccu 1.0000 (0.9821)\t\n",
            "Epoch: [10][43/44]\tLoss 0.1737 (0.1233)\tAccu 0.9688 (0.9818)\t\n",
            "Epoch: [10][44/44]\tLoss 0.1242 (0.1233)\tAccu 0.9583 (0.9813)\t\n",
            "Accu 0.4175\t\n",
            "Epoch: [11][1/44]\tLoss 0.0495 (0.0495)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [11][2/44]\tLoss 0.0704 (0.0600)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [11][3/44]\tLoss 0.0620 (0.0606)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [11][4/44]\tLoss 0.0633 (0.0613)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [11][5/44]\tLoss 0.1184 (0.0727)\tAccu 0.9688 (0.9938)\t\n",
            "Epoch: [11][6/44]\tLoss 0.0720 (0.0726)\tAccu 1.0000 (0.9948)\t\n",
            "Epoch: [11][7/44]\tLoss 0.0931 (0.0755)\tAccu 1.0000 (0.9955)\t\n",
            "Epoch: [11][8/44]\tLoss 0.1179 (0.0808)\tAccu 1.0000 (0.9961)\t\n",
            "Epoch: [11][9/44]\tLoss 0.1175 (0.0849)\tAccu 0.9375 (0.9896)\t\n",
            "Epoch: [11][10/44]\tLoss 0.0648 (0.0829)\tAccu 1.0000 (0.9906)\t\n",
            "Epoch: [11][11/44]\tLoss 0.0360 (0.0786)\tAccu 1.0000 (0.9915)\t\n",
            "Epoch: [11][12/44]\tLoss 0.0553 (0.0767)\tAccu 1.0000 (0.9922)\t\n",
            "Epoch: [11][13/44]\tLoss 0.0393 (0.0738)\tAccu 1.0000 (0.9928)\t\n",
            "Epoch: [11][14/44]\tLoss 0.0559 (0.0725)\tAccu 1.0000 (0.9933)\t\n",
            "Epoch: [11][15/44]\tLoss 0.0536 (0.0713)\tAccu 1.0000 (0.9938)\t\n",
            "Epoch: [11][16/44]\tLoss 0.0381 (0.0692)\tAccu 1.0000 (0.9941)\t\n",
            "Epoch: [11][17/44]\tLoss 0.1062 (0.0714)\tAccu 0.9688 (0.9926)\t\n",
            "Epoch: [11][18/44]\tLoss 0.0571 (0.0706)\tAccu 1.0000 (0.9931)\t\n",
            "Epoch: [11][19/44]\tLoss 0.0298 (0.0684)\tAccu 1.0000 (0.9934)\t\n",
            "Epoch: [11][20/44]\tLoss 0.0908 (0.0695)\tAccu 1.0000 (0.9938)\t\n",
            "Epoch: [11][21/44]\tLoss 0.1521 (0.0735)\tAccu 0.9688 (0.9926)\t\n",
            "Epoch: [11][22/44]\tLoss 0.0805 (0.0738)\tAccu 0.9688 (0.9915)\t\n",
            "Epoch: [11][23/44]\tLoss 0.0460 (0.0726)\tAccu 1.0000 (0.9918)\t\n",
            "Epoch: [11][24/44]\tLoss 0.1344 (0.0752)\tAccu 0.9375 (0.9896)\t\n",
            "Epoch: [11][25/44]\tLoss 0.0763 (0.0752)\tAccu 1.0000 (0.9900)\t\n",
            "Epoch: [11][26/44]\tLoss 0.0258 (0.0733)\tAccu 1.0000 (0.9904)\t\n",
            "Epoch: [11][27/44]\tLoss 0.0446 (0.0722)\tAccu 1.0000 (0.9907)\t\n",
            "Epoch: [11][28/44]\tLoss 0.0818 (0.0726)\tAccu 1.0000 (0.9911)\t\n",
            "Epoch: [11][29/44]\tLoss 0.0668 (0.0724)\tAccu 1.0000 (0.9914)\t\n",
            "Epoch: [11][30/44]\tLoss 0.0847 (0.0728)\tAccu 0.9688 (0.9906)\t\n",
            "Epoch: [11][31/44]\tLoss 0.0655 (0.0726)\tAccu 1.0000 (0.9909)\t\n",
            "Epoch: [11][32/44]\tLoss 0.0718 (0.0725)\tAccu 1.0000 (0.9912)\t\n",
            "Epoch: [11][33/44]\tLoss 0.0806 (0.0728)\tAccu 0.9688 (0.9905)\t\n",
            "Epoch: [11][34/44]\tLoss 0.0354 (0.0717)\tAccu 1.0000 (0.9908)\t\n",
            "Epoch: [11][35/44]\tLoss 0.0658 (0.0715)\tAccu 1.0000 (0.9911)\t\n",
            "Epoch: [11][36/44]\tLoss 0.1032 (0.0724)\tAccu 1.0000 (0.9913)\t\n",
            "Epoch: [11][37/44]\tLoss 0.0500 (0.0718)\tAccu 1.0000 (0.9916)\t\n",
            "Epoch: [11][38/44]\tLoss 0.0391 (0.0709)\tAccu 1.0000 (0.9918)\t\n",
            "Epoch: [11][39/44]\tLoss 0.0185 (0.0696)\tAccu 1.0000 (0.9920)\t\n",
            "Epoch: [11][40/44]\tLoss 0.0288 (0.0686)\tAccu 1.0000 (0.9922)\t\n",
            "Epoch: [11][41/44]\tLoss 0.0445 (0.0680)\tAccu 1.0000 (0.9924)\t\n",
            "Epoch: [11][42/44]\tLoss 0.0197 (0.0668)\tAccu 1.0000 (0.9926)\t\n",
            "Epoch: [11][43/44]\tLoss 0.0792 (0.0671)\tAccu 1.0000 (0.9927)\t\n",
            "Epoch: [11][44/44]\tLoss 0.0632 (0.0671)\tAccu 1.0000 (0.9929)\t\n",
            "Accu 0.4275\t\n",
            "Epoch: [12][1/44]\tLoss 0.0307 (0.0307)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [12][2/44]\tLoss 0.0393 (0.0350)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [12][3/44]\tLoss 0.0249 (0.0317)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [12][4/44]\tLoss 0.0181 (0.0283)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [12][5/44]\tLoss 0.0734 (0.0373)\tAccu 0.9688 (0.9938)\t\n",
            "Epoch: [12][6/44]\tLoss 0.0304 (0.0362)\tAccu 1.0000 (0.9948)\t\n",
            "Epoch: [12][7/44]\tLoss 0.0388 (0.0365)\tAccu 1.0000 (0.9955)\t\n",
            "Epoch: [12][8/44]\tLoss 0.0355 (0.0364)\tAccu 1.0000 (0.9961)\t\n",
            "Epoch: [12][9/44]\tLoss 0.0611 (0.0391)\tAccu 1.0000 (0.9965)\t\n",
            "Epoch: [12][10/44]\tLoss 0.0572 (0.0409)\tAccu 1.0000 (0.9969)\t\n",
            "Epoch: [12][11/44]\tLoss 0.0178 (0.0388)\tAccu 1.0000 (0.9972)\t\n",
            "Epoch: [12][12/44]\tLoss 0.0322 (0.0383)\tAccu 1.0000 (0.9974)\t\n",
            "Epoch: [12][13/44]\tLoss 0.0117 (0.0362)\tAccu 1.0000 (0.9976)\t\n",
            "Epoch: [12][14/44]\tLoss 0.0393 (0.0365)\tAccu 1.0000 (0.9978)\t\n",
            "Epoch: [12][15/44]\tLoss 0.0277 (0.0359)\tAccu 1.0000 (0.9979)\t\n",
            "Epoch: [12][16/44]\tLoss 0.0249 (0.0352)\tAccu 1.0000 (0.9980)\t\n",
            "Epoch: [12][17/44]\tLoss 0.0799 (0.0378)\tAccu 0.9688 (0.9963)\t\n",
            "Epoch: [12][18/44]\tLoss 0.0363 (0.0377)\tAccu 1.0000 (0.9965)\t\n",
            "Epoch: [12][19/44]\tLoss 0.0227 (0.0370)\tAccu 1.0000 (0.9967)\t\n",
            "Epoch: [12][20/44]\tLoss 0.0422 (0.0372)\tAccu 1.0000 (0.9969)\t\n",
            "Epoch: [12][21/44]\tLoss 0.1030 (0.0403)\tAccu 0.9688 (0.9955)\t\n",
            "Epoch: [12][22/44]\tLoss 0.0379 (0.0402)\tAccu 1.0000 (0.9957)\t\n",
            "Epoch: [12][23/44]\tLoss 0.0339 (0.0400)\tAccu 1.0000 (0.9959)\t\n",
            "Epoch: [12][24/44]\tLoss 0.1075 (0.0428)\tAccu 0.9688 (0.9948)\t\n",
            "Epoch: [12][25/44]\tLoss 0.0396 (0.0427)\tAccu 1.0000 (0.9950)\t\n",
            "Epoch: [12][26/44]\tLoss 0.0215 (0.0418)\tAccu 1.0000 (0.9952)\t\n",
            "Epoch: [12][27/44]\tLoss 0.0282 (0.0413)\tAccu 1.0000 (0.9954)\t\n",
            "Epoch: [12][28/44]\tLoss 0.0311 (0.0410)\tAccu 1.0000 (0.9955)\t\n",
            "Epoch: [12][29/44]\tLoss 0.0275 (0.0405)\tAccu 1.0000 (0.9957)\t\n",
            "Epoch: [12][30/44]\tLoss 0.0317 (0.0402)\tAccu 1.0000 (0.9958)\t\n",
            "Epoch: [12][31/44]\tLoss 0.0453 (0.0404)\tAccu 1.0000 (0.9960)\t\n",
            "Epoch: [12][32/44]\tLoss 0.0392 (0.0403)\tAccu 1.0000 (0.9961)\t\n",
            "Epoch: [12][33/44]\tLoss 0.0365 (0.0402)\tAccu 1.0000 (0.9962)\t\n",
            "Epoch: [12][34/44]\tLoss 0.0221 (0.0397)\tAccu 1.0000 (0.9963)\t\n",
            "Epoch: [12][35/44]\tLoss 0.0261 (0.0393)\tAccu 1.0000 (0.9964)\t\n",
            "Epoch: [12][36/44]\tLoss 0.0475 (0.0395)\tAccu 1.0000 (0.9965)\t\n",
            "Epoch: [12][37/44]\tLoss 0.0260 (0.0392)\tAccu 1.0000 (0.9966)\t\n",
            "Epoch: [12][38/44]\tLoss 0.0234 (0.0388)\tAccu 1.0000 (0.9967)\t\n",
            "Epoch: [12][39/44]\tLoss 0.0272 (0.0385)\tAccu 1.0000 (0.9968)\t\n",
            "Epoch: [12][40/44]\tLoss 0.0158 (0.0379)\tAccu 1.0000 (0.9969)\t\n",
            "Epoch: [12][41/44]\tLoss 0.0434 (0.0380)\tAccu 0.9688 (0.9962)\t\n",
            "Epoch: [12][42/44]\tLoss 0.0192 (0.0376)\tAccu 1.0000 (0.9963)\t\n",
            "Epoch: [12][43/44]\tLoss 0.0350 (0.0375)\tAccu 1.0000 (0.9964)\t\n",
            "Epoch: [12][44/44]\tLoss 0.0364 (0.0375)\tAccu 1.0000 (0.9964)\t\n",
            "Accu 0.4225\t\n",
            "Epoch: [13][1/44]\tLoss 0.0236 (0.0236)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [13][2/44]\tLoss 0.0247 (0.0242)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [13][3/44]\tLoss 0.0179 (0.0221)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [13][4/44]\tLoss 0.0254 (0.0229)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [13][5/44]\tLoss 0.0750 (0.0333)\tAccu 0.9688 (0.9938)\t\n",
            "Epoch: [13][6/44]\tLoss 0.0218 (0.0314)\tAccu 1.0000 (0.9948)\t\n",
            "Epoch: [13][7/44]\tLoss 0.0187 (0.0296)\tAccu 1.0000 (0.9955)\t\n",
            "Epoch: [13][8/44]\tLoss 0.0273 (0.0293)\tAccu 1.0000 (0.9961)\t\n",
            "Epoch: [13][9/44]\tLoss 0.0479 (0.0314)\tAccu 1.0000 (0.9965)\t\n",
            "Epoch: [13][10/44]\tLoss 0.0450 (0.0327)\tAccu 1.0000 (0.9969)\t\n",
            "Epoch: [13][11/44]\tLoss 0.0155 (0.0312)\tAccu 1.0000 (0.9972)\t\n",
            "Epoch: [13][12/44]\tLoss 0.0184 (0.0301)\tAccu 1.0000 (0.9974)\t\n",
            "Epoch: [13][13/44]\tLoss 0.0234 (0.0296)\tAccu 1.0000 (0.9976)\t\n",
            "Epoch: [13][14/44]\tLoss 0.0318 (0.0297)\tAccu 1.0000 (0.9978)\t\n",
            "Epoch: [13][15/44]\tLoss 0.0209 (0.0291)\tAccu 1.0000 (0.9979)\t\n",
            "Epoch: [13][16/44]\tLoss 0.0208 (0.0286)\tAccu 1.0000 (0.9980)\t\n",
            "Epoch: [13][17/44]\tLoss 0.0664 (0.0309)\tAccu 0.9688 (0.9963)\t\n",
            "Epoch: [13][18/44]\tLoss 0.0225 (0.0304)\tAccu 1.0000 (0.9965)\t\n",
            "Epoch: [13][19/44]\tLoss 0.0179 (0.0297)\tAccu 1.0000 (0.9967)\t\n",
            "Epoch: [13][20/44]\tLoss 0.0320 (0.0298)\tAccu 1.0000 (0.9969)\t\n",
            "Epoch: [13][21/44]\tLoss 0.0875 (0.0326)\tAccu 0.9688 (0.9955)\t\n",
            "Epoch: [13][22/44]\tLoss 0.0343 (0.0327)\tAccu 1.0000 (0.9957)\t\n",
            "Epoch: [13][23/44]\tLoss 0.0202 (0.0321)\tAccu 1.0000 (0.9959)\t\n",
            "Epoch: [13][24/44]\tLoss 0.0724 (0.0338)\tAccu 0.9688 (0.9948)\t\n",
            "Epoch: [13][25/44]\tLoss 0.0284 (0.0336)\tAccu 1.0000 (0.9950)\t\n",
            "Epoch: [13][26/44]\tLoss 0.0139 (0.0328)\tAccu 1.0000 (0.9952)\t\n",
            "Epoch: [13][27/44]\tLoss 0.0149 (0.0322)\tAccu 1.0000 (0.9954)\t\n",
            "Epoch: [13][28/44]\tLoss 0.0209 (0.0318)\tAccu 1.0000 (0.9955)\t\n",
            "Epoch: [13][29/44]\tLoss 0.0159 (0.0312)\tAccu 1.0000 (0.9957)\t\n",
            "Epoch: [13][30/44]\tLoss 0.0189 (0.0308)\tAccu 1.0000 (0.9958)\t\n",
            "Epoch: [13][31/44]\tLoss 0.0255 (0.0306)\tAccu 1.0000 (0.9960)\t\n",
            "Epoch: [13][32/44]\tLoss 0.0340 (0.0307)\tAccu 1.0000 (0.9961)\t\n",
            "Epoch: [13][33/44]\tLoss 0.0166 (0.0303)\tAccu 1.0000 (0.9962)\t\n",
            "Epoch: [13][34/44]\tLoss 0.0172 (0.0299)\tAccu 1.0000 (0.9963)\t\n",
            "Epoch: [13][35/44]\tLoss 0.0119 (0.0294)\tAccu 1.0000 (0.9964)\t\n",
            "Epoch: [13][36/44]\tLoss 0.0322 (0.0295)\tAccu 1.0000 (0.9965)\t\n",
            "Epoch: [13][37/44]\tLoss 0.0212 (0.0293)\tAccu 1.0000 (0.9966)\t\n",
            "Epoch: [13][38/44]\tLoss 0.0169 (0.0289)\tAccu 1.0000 (0.9967)\t\n",
            "Epoch: [13][39/44]\tLoss 0.0197 (0.0287)\tAccu 1.0000 (0.9968)\t\n",
            "Epoch: [13][40/44]\tLoss 0.0130 (0.0283)\tAccu 1.0000 (0.9969)\t\n",
            "Epoch: [13][41/44]\tLoss 0.0210 (0.0281)\tAccu 1.0000 (0.9970)\t\n",
            "Epoch: [13][42/44]\tLoss 0.0135 (0.0278)\tAccu 1.0000 (0.9970)\t\n",
            "Epoch: [13][43/44]\tLoss 0.0226 (0.0277)\tAccu 1.0000 (0.9971)\t\n",
            "Epoch: [13][44/44]\tLoss 0.0159 (0.0275)\tAccu 1.0000 (0.9972)\t\n",
            "Accu 0.4400\t\n",
            "Epoch: [14][1/44]\tLoss 0.0173 (0.0173)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [14][2/44]\tLoss 0.0191 (0.0182)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [14][3/44]\tLoss 0.0128 (0.0164)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [14][4/44]\tLoss 0.0163 (0.0164)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [14][5/44]\tLoss 0.0358 (0.0203)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [14][6/44]\tLoss 0.0168 (0.0197)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [14][7/44]\tLoss 0.0150 (0.0190)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [14][8/44]\tLoss 0.0222 (0.0194)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [14][9/44]\tLoss 0.0466 (0.0224)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [14][10/44]\tLoss 0.0176 (0.0220)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [14][11/44]\tLoss 0.0122 (0.0211)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [14][12/44]\tLoss 0.0251 (0.0214)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [14][13/44]\tLoss 0.0111 (0.0206)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [14][14/44]\tLoss 0.0171 (0.0204)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [14][15/44]\tLoss 0.0127 (0.0199)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [14][16/44]\tLoss 0.0107 (0.0193)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [14][17/44]\tLoss 0.0664 (0.0221)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [14][18/44]\tLoss 0.0183 (0.0218)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [14][19/44]\tLoss 0.0136 (0.0214)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [14][20/44]\tLoss 0.0178 (0.0212)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [14][21/44]\tLoss 0.0830 (0.0242)\tAccu 0.9688 (0.9985)\t\n",
            "Epoch: [14][22/44]\tLoss 0.0178 (0.0239)\tAccu 1.0000 (0.9986)\t\n",
            "Epoch: [14][23/44]\tLoss 0.0169 (0.0236)\tAccu 1.0000 (0.9986)\t\n",
            "Epoch: [14][24/44]\tLoss 0.0746 (0.0257)\tAccu 0.9688 (0.9974)\t\n",
            "Epoch: [14][25/44]\tLoss 0.0217 (0.0255)\tAccu 1.0000 (0.9975)\t\n",
            "Epoch: [14][26/44]\tLoss 0.0152 (0.0251)\tAccu 1.0000 (0.9976)\t\n",
            "Epoch: [14][27/44]\tLoss 0.0132 (0.0247)\tAccu 1.0000 (0.9977)\t\n",
            "Epoch: [14][28/44]\tLoss 0.0126 (0.0243)\tAccu 1.0000 (0.9978)\t\n",
            "Epoch: [14][29/44]\tLoss 0.0193 (0.0241)\tAccu 1.0000 (0.9978)\t\n",
            "Epoch: [14][30/44]\tLoss 0.0147 (0.0238)\tAccu 1.0000 (0.9979)\t\n",
            "Epoch: [14][31/44]\tLoss 0.0192 (0.0236)\tAccu 1.0000 (0.9980)\t\n",
            "Epoch: [14][32/44]\tLoss 0.0325 (0.0239)\tAccu 1.0000 (0.9980)\t\n",
            "Epoch: [14][33/44]\tLoss 0.0114 (0.0235)\tAccu 1.0000 (0.9981)\t\n",
            "Epoch: [14][34/44]\tLoss 0.0150 (0.0233)\tAccu 1.0000 (0.9982)\t\n",
            "Epoch: [14][35/44]\tLoss 0.0158 (0.0231)\tAccu 1.0000 (0.9982)\t\n",
            "Epoch: [14][36/44]\tLoss 0.0310 (0.0233)\tAccu 1.0000 (0.9983)\t\n",
            "Epoch: [14][37/44]\tLoss 0.0123 (0.0230)\tAccu 1.0000 (0.9983)\t\n",
            "Epoch: [14][38/44]\tLoss 0.0104 (0.0227)\tAccu 1.0000 (0.9984)\t\n",
            "Epoch: [14][39/44]\tLoss 0.0105 (0.0224)\tAccu 1.0000 (0.9984)\t\n",
            "Epoch: [14][40/44]\tLoss 0.0100 (0.0220)\tAccu 1.0000 (0.9984)\t\n",
            "Epoch: [14][41/44]\tLoss 0.0109 (0.0218)\tAccu 1.0000 (0.9985)\t\n",
            "Epoch: [14][42/44]\tLoss 0.0074 (0.0214)\tAccu 1.0000 (0.9985)\t\n",
            "Epoch: [14][43/44]\tLoss 0.0210 (0.0214)\tAccu 1.0000 (0.9985)\t\n",
            "Epoch: [14][44/44]\tLoss 0.0108 (0.0212)\tAccu 1.0000 (0.9986)\t\n",
            "Accu 0.4250\t\n",
            "Epoch: [15][1/44]\tLoss 0.0106 (0.0106)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [15][2/44]\tLoss 0.0150 (0.0128)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [15][3/44]\tLoss 0.0095 (0.0117)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [15][4/44]\tLoss 0.0106 (0.0114)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [15][5/44]\tLoss 0.0196 (0.0131)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [15][6/44]\tLoss 0.0122 (0.0129)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [15][7/44]\tLoss 0.0147 (0.0132)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [15][8/44]\tLoss 0.0140 (0.0133)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [15][9/44]\tLoss 0.0523 (0.0176)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [15][10/44]\tLoss 0.0120 (0.0171)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [15][11/44]\tLoss 0.0077 (0.0162)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [15][12/44]\tLoss 0.0169 (0.0163)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [15][13/44]\tLoss 0.0088 (0.0157)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [15][14/44]\tLoss 0.0191 (0.0159)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [15][15/44]\tLoss 0.0132 (0.0157)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [15][16/44]\tLoss 0.0103 (0.0154)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [15][17/44]\tLoss 0.0511 (0.0175)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [15][18/44]\tLoss 0.0146 (0.0173)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [15][19/44]\tLoss 0.0093 (0.0169)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [15][20/44]\tLoss 0.0187 (0.0170)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [15][21/44]\tLoss 0.0789 (0.0200)\tAccu 0.9688 (0.9985)\t\n",
            "Epoch: [15][22/44]\tLoss 0.0141 (0.0197)\tAccu 1.0000 (0.9986)\t\n",
            "Epoch: [15][23/44]\tLoss 0.0126 (0.0194)\tAccu 1.0000 (0.9986)\t\n",
            "Epoch: [15][24/44]\tLoss 0.0606 (0.0211)\tAccu 0.9688 (0.9974)\t\n",
            "Epoch: [15][25/44]\tLoss 0.0141 (0.0208)\tAccu 1.0000 (0.9975)\t\n",
            "Epoch: [15][26/44]\tLoss 0.0169 (0.0207)\tAccu 1.0000 (0.9976)\t\n",
            "Epoch: [15][27/44]\tLoss 0.0145 (0.0204)\tAccu 1.0000 (0.9977)\t\n",
            "Epoch: [15][28/44]\tLoss 0.0111 (0.0201)\tAccu 1.0000 (0.9978)\t\n",
            "Epoch: [15][29/44]\tLoss 0.0094 (0.0197)\tAccu 1.0000 (0.9978)\t\n",
            "Epoch: [15][30/44]\tLoss 0.0112 (0.0195)\tAccu 1.0000 (0.9979)\t\n",
            "Epoch: [15][31/44]\tLoss 0.0139 (0.0193)\tAccu 1.0000 (0.9980)\t\n",
            "Epoch: [15][32/44]\tLoss 0.0305 (0.0196)\tAccu 1.0000 (0.9980)\t\n",
            "Epoch: [15][33/44]\tLoss 0.0115 (0.0194)\tAccu 1.0000 (0.9981)\t\n",
            "Epoch: [15][34/44]\tLoss 0.0126 (0.0192)\tAccu 1.0000 (0.9982)\t\n",
            "Epoch: [15][35/44]\tLoss 0.0121 (0.0190)\tAccu 1.0000 (0.9982)\t\n",
            "Epoch: [15][36/44]\tLoss 0.0205 (0.0190)\tAccu 1.0000 (0.9983)\t\n",
            "Epoch: [15][37/44]\tLoss 0.0096 (0.0188)\tAccu 1.0000 (0.9983)\t\n",
            "Epoch: [15][38/44]\tLoss 0.0114 (0.0186)\tAccu 1.0000 (0.9984)\t\n",
            "Epoch: [15][39/44]\tLoss 0.0079 (0.0183)\tAccu 1.0000 (0.9984)\t\n",
            "Epoch: [15][40/44]\tLoss 0.0149 (0.0182)\tAccu 1.0000 (0.9984)\t\n",
            "Epoch: [15][41/44]\tLoss 0.0157 (0.0181)\tAccu 1.0000 (0.9985)\t\n",
            "Epoch: [15][42/44]\tLoss 0.0074 (0.0179)\tAccu 1.0000 (0.9985)\t\n",
            "Epoch: [15][43/44]\tLoss 0.0151 (0.0178)\tAccu 1.0000 (0.9985)\t\n",
            "Epoch: [15][44/44]\tLoss 0.0085 (0.0177)\tAccu 1.0000 (0.9986)\t\n",
            "Accu 0.4300\t\n",
            "Epoch: [16][1/44]\tLoss 0.0081 (0.0081)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [16][2/44]\tLoss 0.0109 (0.0095)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [16][3/44]\tLoss 0.0067 (0.0086)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [16][4/44]\tLoss 0.0083 (0.0085)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [16][5/44]\tLoss 0.0162 (0.0101)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [16][6/44]\tLoss 0.0120 (0.0104)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [16][7/44]\tLoss 0.0123 (0.0107)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [16][8/44]\tLoss 0.0097 (0.0105)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [16][9/44]\tLoss 0.0353 (0.0133)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [16][10/44]\tLoss 0.0140 (0.0134)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [16][11/44]\tLoss 0.0057 (0.0127)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [16][12/44]\tLoss 0.0144 (0.0128)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [16][13/44]\tLoss 0.0092 (0.0125)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [16][14/44]\tLoss 0.0115 (0.0124)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [16][15/44]\tLoss 0.0087 (0.0122)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [16][16/44]\tLoss 0.0071 (0.0119)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [16][17/44]\tLoss 0.0474 (0.0140)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [16][18/44]\tLoss 0.0096 (0.0137)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [16][19/44]\tLoss 0.0074 (0.0134)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [16][20/44]\tLoss 0.0153 (0.0135)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [16][21/44]\tLoss 0.0717 (0.0163)\tAccu 0.9688 (0.9985)\t\n",
            "Epoch: [16][22/44]\tLoss 0.0138 (0.0161)\tAccu 1.0000 (0.9986)\t\n",
            "Epoch: [16][23/44]\tLoss 0.0109 (0.0159)\tAccu 1.0000 (0.9986)\t\n",
            "Epoch: [16][24/44]\tLoss 0.0546 (0.0175)\tAccu 0.9688 (0.9974)\t\n",
            "Epoch: [16][25/44]\tLoss 0.0167 (0.0175)\tAccu 1.0000 (0.9975)\t\n",
            "Epoch: [16][26/44]\tLoss 0.0088 (0.0172)\tAccu 1.0000 (0.9976)\t\n",
            "Epoch: [16][27/44]\tLoss 0.0081 (0.0168)\tAccu 1.0000 (0.9977)\t\n",
            "Epoch: [16][28/44]\tLoss 0.0092 (0.0166)\tAccu 1.0000 (0.9978)\t\n",
            "Epoch: [16][29/44]\tLoss 0.0099 (0.0163)\tAccu 1.0000 (0.9978)\t\n",
            "Epoch: [16][30/44]\tLoss 0.0109 (0.0161)\tAccu 1.0000 (0.9979)\t\n",
            "Epoch: [16][31/44]\tLoss 0.0131 (0.0160)\tAccu 1.0000 (0.9980)\t\n",
            "Epoch: [16][32/44]\tLoss 0.0196 (0.0162)\tAccu 1.0000 (0.9980)\t\n",
            "Epoch: [16][33/44]\tLoss 0.0123 (0.0160)\tAccu 1.0000 (0.9981)\t\n",
            "Epoch: [16][34/44]\tLoss 0.0162 (0.0160)\tAccu 1.0000 (0.9982)\t\n",
            "Epoch: [16][35/44]\tLoss 0.0116 (0.0159)\tAccu 1.0000 (0.9982)\t\n",
            "Epoch: [16][36/44]\tLoss 0.0122 (0.0158)\tAccu 1.0000 (0.9983)\t\n",
            "Epoch: [16][37/44]\tLoss 0.0084 (0.0156)\tAccu 1.0000 (0.9983)\t\n",
            "Epoch: [16][38/44]\tLoss 0.0064 (0.0154)\tAccu 1.0000 (0.9984)\t\n",
            "Epoch: [16][39/44]\tLoss 0.0072 (0.0152)\tAccu 1.0000 (0.9984)\t\n",
            "Epoch: [16][40/44]\tLoss 0.0105 (0.0150)\tAccu 1.0000 (0.9984)\t\n",
            "Epoch: [16][41/44]\tLoss 0.0082 (0.0149)\tAccu 1.0000 (0.9985)\t\n",
            "Epoch: [16][42/44]\tLoss 0.0060 (0.0147)\tAccu 1.0000 (0.9985)\t\n",
            "Epoch: [16][43/44]\tLoss 0.0141 (0.0147)\tAccu 1.0000 (0.9985)\t\n",
            "Epoch: [16][44/44]\tLoss 0.0077 (0.0145)\tAccu 1.0000 (0.9986)\t\n",
            "Accu 0.4250\t\n",
            "Epoch: [17][1/44]\tLoss 0.0081 (0.0081)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [17][2/44]\tLoss 0.0115 (0.0098)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [17][3/44]\tLoss 0.0063 (0.0086)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [17][4/44]\tLoss 0.0084 (0.0086)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [17][5/44]\tLoss 0.0106 (0.0090)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [17][6/44]\tLoss 0.0098 (0.0091)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [17][7/44]\tLoss 0.0087 (0.0090)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [17][8/44]\tLoss 0.0122 (0.0094)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [17][9/44]\tLoss 0.0357 (0.0124)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [17][10/44]\tLoss 0.0123 (0.0123)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [17][11/44]\tLoss 0.0080 (0.0120)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [17][12/44]\tLoss 0.0104 (0.0118)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [17][13/44]\tLoss 0.0056 (0.0113)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [17][14/44]\tLoss 0.0104 (0.0113)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [17][15/44]\tLoss 0.0069 (0.0110)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [17][16/44]\tLoss 0.0052 (0.0106)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [17][17/44]\tLoss 0.0454 (0.0127)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [17][18/44]\tLoss 0.0107 (0.0126)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [17][19/44]\tLoss 0.0083 (0.0123)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [17][20/44]\tLoss 0.0100 (0.0122)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [17][21/44]\tLoss 0.0736 (0.0151)\tAccu 0.9688 (0.9985)\t\n",
            "Epoch: [17][22/44]\tLoss 0.0133 (0.0151)\tAccu 1.0000 (0.9986)\t\n",
            "Epoch: [17][23/44]\tLoss 0.0148 (0.0150)\tAccu 1.0000 (0.9986)\t\n",
            "Epoch: [17][24/44]\tLoss 0.0426 (0.0162)\tAccu 1.0000 (0.9987)\t\n",
            "Epoch: [17][25/44]\tLoss 0.0116 (0.0160)\tAccu 1.0000 (0.9988)\t\n",
            "Epoch: [17][26/44]\tLoss 0.0065 (0.0156)\tAccu 1.0000 (0.9988)\t\n",
            "Epoch: [17][27/44]\tLoss 0.0073 (0.0153)\tAccu 1.0000 (0.9988)\t\n",
            "Epoch: [17][28/44]\tLoss 0.0071 (0.0150)\tAccu 1.0000 (0.9989)\t\n",
            "Epoch: [17][29/44]\tLoss 0.0095 (0.0148)\tAccu 1.0000 (0.9989)\t\n",
            "Epoch: [17][30/44]\tLoss 0.0133 (0.0148)\tAccu 1.0000 (0.9990)\t\n",
            "Epoch: [17][31/44]\tLoss 0.0086 (0.0146)\tAccu 1.0000 (0.9990)\t\n",
            "Epoch: [17][32/44]\tLoss 0.0129 (0.0145)\tAccu 1.0000 (0.9990)\t\n",
            "Epoch: [17][33/44]\tLoss 0.0080 (0.0143)\tAccu 1.0000 (0.9991)\t\n",
            "Epoch: [17][34/44]\tLoss 0.0101 (0.0142)\tAccu 1.0000 (0.9991)\t\n",
            "Epoch: [17][35/44]\tLoss 0.0097 (0.0141)\tAccu 1.0000 (0.9991)\t\n",
            "Epoch: [17][36/44]\tLoss 0.0112 (0.0140)\tAccu 1.0000 (0.9991)\t\n",
            "Epoch: [17][37/44]\tLoss 0.0087 (0.0139)\tAccu 1.0000 (0.9992)\t\n",
            "Epoch: [17][38/44]\tLoss 0.0061 (0.0137)\tAccu 1.0000 (0.9992)\t\n",
            "Epoch: [17][39/44]\tLoss 0.0048 (0.0134)\tAccu 1.0000 (0.9992)\t\n",
            "Epoch: [17][40/44]\tLoss 0.0060 (0.0132)\tAccu 1.0000 (0.9992)\t\n",
            "Epoch: [17][41/44]\tLoss 0.0070 (0.0131)\tAccu 1.0000 (0.9992)\t\n",
            "Epoch: [17][42/44]\tLoss 0.0047 (0.0129)\tAccu 1.0000 (0.9993)\t\n",
            "Epoch: [17][43/44]\tLoss 0.0105 (0.0128)\tAccu 1.0000 (0.9993)\t\n",
            "Epoch: [17][44/44]\tLoss 0.0070 (0.0127)\tAccu 1.0000 (0.9993)\t\n",
            "Accu 0.4350\t\n",
            "Epoch: [18][1/44]\tLoss 0.0087 (0.0087)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [18][2/44]\tLoss 0.0077 (0.0082)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [18][3/44]\tLoss 0.0091 (0.0085)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [18][4/44]\tLoss 0.0097 (0.0088)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [18][5/44]\tLoss 0.0134 (0.0097)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [18][6/44]\tLoss 0.0059 (0.0091)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [18][7/44]\tLoss 0.0082 (0.0090)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [18][8/44]\tLoss 0.0106 (0.0092)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [18][9/44]\tLoss 0.0203 (0.0104)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [18][10/44]\tLoss 0.0105 (0.0104)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [18][11/44]\tLoss 0.0042 (0.0098)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [18][12/44]\tLoss 0.0086 (0.0097)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [18][13/44]\tLoss 0.0066 (0.0095)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [18][14/44]\tLoss 0.0076 (0.0094)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [18][15/44]\tLoss 0.0062 (0.0092)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [18][16/44]\tLoss 0.0072 (0.0090)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [18][17/44]\tLoss 0.0371 (0.0107)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [18][18/44]\tLoss 0.0072 (0.0105)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [18][19/44]\tLoss 0.0081 (0.0104)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [18][20/44]\tLoss 0.0085 (0.0103)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [18][21/44]\tLoss 0.0655 (0.0129)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [18][22/44]\tLoss 0.0108 (0.0128)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [18][23/44]\tLoss 0.0078 (0.0126)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [18][24/44]\tLoss 0.0393 (0.0137)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [18][25/44]\tLoss 0.0131 (0.0137)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [18][26/44]\tLoss 0.0065 (0.0134)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [18][27/44]\tLoss 0.0080 (0.0132)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [18][28/44]\tLoss 0.0098 (0.0131)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [18][29/44]\tLoss 0.0077 (0.0129)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [18][30/44]\tLoss 0.0078 (0.0127)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [18][31/44]\tLoss 0.0098 (0.0126)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [18][32/44]\tLoss 0.0094 (0.0125)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [18][33/44]\tLoss 0.0081 (0.0124)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [18][34/44]\tLoss 0.0074 (0.0123)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [18][35/44]\tLoss 0.0095 (0.0122)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [18][36/44]\tLoss 0.0107 (0.0121)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [18][37/44]\tLoss 0.0061 (0.0120)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [18][38/44]\tLoss 0.0061 (0.0118)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [18][39/44]\tLoss 0.0045 (0.0116)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [18][40/44]\tLoss 0.0050 (0.0115)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [18][41/44]\tLoss 0.0101 (0.0114)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [18][42/44]\tLoss 0.0048 (0.0113)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [18][43/44]\tLoss 0.0105 (0.0113)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [18][44/44]\tLoss 0.0104 (0.0112)\tAccu 1.0000 (1.0000)\t\n",
            "Accu 0.4300\t\n",
            "Epoch: [19][1/44]\tLoss 0.0055 (0.0055)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [19][2/44]\tLoss 0.0080 (0.0068)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [19][3/44]\tLoss 0.0085 (0.0073)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [19][4/44]\tLoss 0.0095 (0.0079)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [19][5/44]\tLoss 0.0075 (0.0078)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [19][6/44]\tLoss 0.0065 (0.0076)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [19][7/44]\tLoss 0.0068 (0.0075)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [19][8/44]\tLoss 0.0072 (0.0074)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [19][9/44]\tLoss 0.0271 (0.0096)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [19][10/44]\tLoss 0.0063 (0.0093)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [19][11/44]\tLoss 0.0057 (0.0090)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [19][12/44]\tLoss 0.0074 (0.0088)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [19][13/44]\tLoss 0.0050 (0.0085)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [19][14/44]\tLoss 0.0106 (0.0087)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [19][15/44]\tLoss 0.0071 (0.0086)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [19][16/44]\tLoss 0.0082 (0.0086)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [19][17/44]\tLoss 0.0373 (0.0103)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [19][18/44]\tLoss 0.0095 (0.0102)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [19][19/44]\tLoss 0.0072 (0.0101)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [19][20/44]\tLoss 0.0105 (0.0101)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [19][21/44]\tLoss 0.0686 (0.0129)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [19][22/44]\tLoss 0.0096 (0.0127)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [19][23/44]\tLoss 0.0100 (0.0126)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [19][24/44]\tLoss 0.0390 (0.0137)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [19][25/44]\tLoss 0.0106 (0.0136)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [19][26/44]\tLoss 0.0056 (0.0133)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [19][27/44]\tLoss 0.0071 (0.0130)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [19][28/44]\tLoss 0.0072 (0.0128)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [19][29/44]\tLoss 0.0065 (0.0126)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [19][30/44]\tLoss 0.0059 (0.0124)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [19][31/44]\tLoss 0.0092 (0.0123)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [19][32/44]\tLoss 0.0086 (0.0122)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [19][33/44]\tLoss 0.0067 (0.0120)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [19][34/44]\tLoss 0.0090 (0.0119)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [19][35/44]\tLoss 0.0078 (0.0118)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [19][36/44]\tLoss 0.0107 (0.0118)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [19][37/44]\tLoss 0.0061 (0.0116)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [19][38/44]\tLoss 0.0091 (0.0116)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [19][39/44]\tLoss 0.0055 (0.0114)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [19][40/44]\tLoss 0.0040 (0.0112)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [19][41/44]\tLoss 0.0053 (0.0111)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [19][42/44]\tLoss 0.0039 (0.0109)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [19][43/44]\tLoss 0.0059 (0.0108)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [19][44/44]\tLoss 0.0056 (0.0107)\tAccu 1.0000 (1.0000)\t\n",
            "Accu 0.4350\t\n",
            "Epoch: [20][1/44]\tLoss 0.0054 (0.0054)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [20][2/44]\tLoss 0.0074 (0.0064)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [20][3/44]\tLoss 0.0054 (0.0060)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [20][4/44]\tLoss 0.0072 (0.0063)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [20][5/44]\tLoss 0.0053 (0.0061)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [20][6/44]\tLoss 0.0067 (0.0062)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [20][7/44]\tLoss 0.0075 (0.0064)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [20][8/44]\tLoss 0.0065 (0.0064)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [20][9/44]\tLoss 0.0189 (0.0078)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [20][10/44]\tLoss 0.0071 (0.0077)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [20][11/44]\tLoss 0.0048 (0.0075)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [20][12/44]\tLoss 0.0069 (0.0074)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [20][13/44]\tLoss 0.0060 (0.0073)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [20][14/44]\tLoss 0.0076 (0.0073)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [20][15/44]\tLoss 0.0064 (0.0073)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [20][16/44]\tLoss 0.0073 (0.0073)\tAccu 1.0000 (1.0000)\t\n",
            "Epoch: [20][17/44]\tLoss 0.0548 (0.0101)\tAccu 0.9688 (0.9982)\t\n",
            "Epoch: [20][18/44]\tLoss 0.0070 (0.0099)\tAccu 1.0000 (0.9983)\t\n",
            "Epoch: [20][19/44]\tLoss 0.0067 (0.0097)\tAccu 1.0000 (0.9984)\t\n",
            "Epoch: [20][20/44]\tLoss 0.0095 (0.0097)\tAccu 1.0000 (0.9984)\t\n",
            "Epoch: [20][21/44]\tLoss 0.0639 (0.0123)\tAccu 1.0000 (0.9985)\t\n",
            "Epoch: [20][22/44]\tLoss 0.0093 (0.0122)\tAccu 1.0000 (0.9986)\t\n",
            "Epoch: [20][23/44]\tLoss 0.0077 (0.0120)\tAccu 1.0000 (0.9986)\t\n",
            "Epoch: [20][24/44]\tLoss 0.0199 (0.0123)\tAccu 1.0000 (0.9987)\t\n",
            "Epoch: [20][25/44]\tLoss 0.0121 (0.0123)\tAccu 1.0000 (0.9988)\t\n",
            "Epoch: [20][26/44]\tLoss 0.0053 (0.0120)\tAccu 1.0000 (0.9988)\t\n",
            "Epoch: [20][27/44]\tLoss 0.0066 (0.0118)\tAccu 1.0000 (0.9988)\t\n",
            "Epoch: [20][28/44]\tLoss 0.0078 (0.0117)\tAccu 1.0000 (0.9989)\t\n",
            "Epoch: [20][29/44]\tLoss 0.0062 (0.0115)\tAccu 1.0000 (0.9989)\t\n",
            "Epoch: [20][30/44]\tLoss 0.0056 (0.0113)\tAccu 1.0000 (0.9990)\t\n",
            "Epoch: [20][31/44]\tLoss 0.0083 (0.0112)\tAccu 1.0000 (0.9990)\t\n",
            "Epoch: [20][32/44]\tLoss 0.0148 (0.0113)\tAccu 1.0000 (0.9990)\t\n",
            "Epoch: [20][33/44]\tLoss 0.0058 (0.0111)\tAccu 1.0000 (0.9991)\t\n",
            "Epoch: [20][34/44]\tLoss 0.0115 (0.0112)\tAccu 1.0000 (0.9991)\t\n",
            "Epoch: [20][35/44]\tLoss 0.0055 (0.0110)\tAccu 1.0000 (0.9991)\t\n",
            "Epoch: [20][36/44]\tLoss 0.0073 (0.0109)\tAccu 1.0000 (0.9991)\t\n",
            "Epoch: [20][37/44]\tLoss 0.0055 (0.0107)\tAccu 1.0000 (0.9992)\t\n",
            "Epoch: [20][38/44]\tLoss 0.0053 (0.0106)\tAccu 1.0000 (0.9992)\t\n",
            "Epoch: [20][39/44]\tLoss 0.0052 (0.0105)\tAccu 1.0000 (0.9992)\t\n",
            "Epoch: [20][40/44]\tLoss 0.0042 (0.0103)\tAccu 1.0000 (0.9992)\t\n",
            "Epoch: [20][41/44]\tLoss 0.0050 (0.0102)\tAccu 1.0000 (0.9992)\t\n",
            "Epoch: [20][42/44]\tLoss 0.0039 (0.0100)\tAccu 1.0000 (0.9993)\t\n",
            "Epoch: [20][43/44]\tLoss 0.0086 (0.0100)\tAccu 1.0000 (0.9993)\t\n",
            "Epoch: [20][44/44]\tLoss 0.0047 (0.0099)\tAccu 1.0000 (0.9993)\t\n",
            "Accu 0.4325\t\n",
            "Best Acc: 0.44\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dq8fOnTaMmle",
        "colab_type": "text"
      },
      "source": [
        "#### Kernel Output Visualization [5 pts]\n",
        "\n",
        "You have trained Inception_V3 which is pretrained with ImageNet dataset. For this network, extract the final weights from the first convolutional layer. Visualize each filter of the first convolutional layer as an image. Merge each image obtained from the corresponding kernel in a squared grid format. Explain what these outputs mean explicitly. Compare your plot with the obtained from 3.3.2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDVTBrUMMmle",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# write your code in this cell to visualize output of the each filter at the first conv layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCDpcMJTHD0h",
        "colab_type": "text"
      },
      "source": [
        "### 2.4 Interpretation [10 pts.]\n",
        "\n",
        "Explicitly discuss the results that you have obtained in Question 2. Among MLP, CNN (trained from scratch) and transfer learning, which one do you think is better? What are the weaknesses and strengths of each method?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7q8jg7nfFUe-",
        "colab_type": "text"
      },
      "source": [
        "##References\n",
        "\n",
        "Alessio, Corrado. “Animals-10.” Kaggle, 4 Oct. 2018, https://www.kaggle.com/alessiocorrado99/animals10.<br>\n",
        "\n",
        "Szegedy, Christian, et al. \"Rethinking the inception architecture for computer vision.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.<br>\n"
      ]
    }
  ]
}